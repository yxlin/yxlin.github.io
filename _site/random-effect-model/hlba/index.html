<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>HLBA Model | Cognitive Models</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="HLBA Model" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this tutorial, I used the DE-MCMC sampler (Turner et al., 2013) to fit a hierarchical LBA model. This particular design has a stimulus (S) and a frequency (F) factor." />
<meta property="og:description" content="In this tutorial, I used the DE-MCMC sampler (Turner et al., 2013) to fit a hierarchical LBA model. This particular design has a stimulus (S) and a frequency (F) factor." />
<link rel="canonical" href="http://localhost:4000/random-effect-model/hlba/" />
<meta property="og:url" content="http://localhost:4000/random-effect-model/hlba/" />
<meta property="og:site_name" content="Cognitive Models" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-03T06:11:35+00:00" />
<script type="application/ld+json">
{"datePublished":"2020-10-03T06:11:35+00:00","description":"In this tutorial, I used the DE-MCMC sampler (Turner et al., 2013) to fit a hierarchical LBA model. This particular design has a stimulus (S) and a frequency (F) factor.","@type":"Article","url":"http://localhost:4000/random-effect-model/hlba/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/siteicon.png"}},"headline":"HLBA Model","dateModified":"2020-10-03T06:11:35+00:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Cognitive Models" />

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,400italic,700,700italic|Open+Sans:400,400italic,600,600italic,700,700italic|Inconsolata:400,700">
		<link rel="stylesheet" href="/css/main.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">

		
	</head>

	<body>
	  <header>
	    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
	    
			<h1>
				<a href="/"><img src="/images/emblem.svg" width="40" height="40" alt="Cognitive Models logo"></a>
				Cognitive Models
				<button type="button" class="open-nav" id="open-nav"></button>
			</h1>

			<form action="/search/" method="get">
				<input type="text" name="q" id="search-input" placeholder="Search" autofocus>
				<input type="submit" value="Search" style="display: none;">
			</form>

			<nav >
				<ul>
					<li class="nav-item top-level ">
						
						<a href="/"></a>
					</li>
				</ul>

				<ul>
					
					
						<li class="nav-item top-level ">
							
							<a href="/BUGS/hnormal/">BUGS Examples Volumn 1</a>
							<ul>
								
									<li class="nav-item "><a href="/BUGS/hnormal/">Hierarchical Normal Model</a></li>
								
									<li class="nav-item "><a href="/BUGS/seeds/">Random effect logistic regression</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/approximation/kde/">Likelihood Free Method</a>
							<ul>
								
									<li class="nav-item "><a href="/approximation/kde/">Kernel Density Estimation</a></li>
								
									<li class="nav-item "><a href="/approximation/pda/">Probability Density Approximation</a></li>
								
									<li class="nav-item "><a href="/approximation/ppda/">Parallel Probability Density Approximation</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/basics/model_array/">Modelling Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/basics/model_array/">Model Array</a></li>
								
									<li class="nav-item "><a href="/basics/simulation/">Simulation</a></li>
								
									<li class="nav-item "><a href="/basics/descriptive/">Descriptive Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/summary/">Summary Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/leastsq/">Least Square Method</a></li>
								
									<li class="nav-item "><a href="/basics/mle/">Maximising Likelihoods</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/bayes-basics/theorem/">Bayesian Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/bayes-basics/theorem/">Bayes' Theorem</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/prior/">Prior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/likelihood/">Model Likelihood</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/posterior/">Posterior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/diagnosis/">Checking Fitted Models</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/cognitive-model/lba/">Cognitive Model</a>
							<ul>
								
									<li class="nav-item "><a href="/cognitive-model/lba/">LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pm/">PM Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/plba/">PLBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm/">Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm1c/">One-choice Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/cddm/">Circular Drift-diffusion Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pddm/">PDDM</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lba3/">Three-accumulator LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lca/">LCA Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/fixed-effect-model/one_participant/">Fixed-effects Model</a>
							<ul>
								
									<li class="nav-item "><a href="/fixed-effect-model/one_participant/">One Participant</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/many_participants/">Multiple Participants</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/cddm12S/">CDDM</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/mcmc/mcmc/">Modern Bayesian Statistics</a>
							<ul>
								
									<li class="nav-item "><a href="/mcmc/mcmc/">Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/mcmc/rwm/">Random Walk Metropolis</a></li>
								
									<li class="nav-item "><a href="/mcmc/hastings/">Metropolis-Hastings</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level current">
							
							<a href="/random-effect-model/hlba/">Hierarchical Model</a>
							<ul>
								
									<li class="nav-item current"><a href="/random-effect-model/hlba/">HLBA Model</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hddm/">HDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision1/">Shooting Decision Model - Recovery Study</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision2/">Shooting Decision Model - Empirical Data</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hcddm/">HCDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hpm/">HPM Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/sampling/genetic/">Sampling Techniques</a>
							<ul>
								
									<li class="nav-item "><a href="/sampling/genetic/">Population-based Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/crossover/">Crossover</a></li>
								
									<li class="nav-item "><a href="/sampling/migration/">Migration</a></li>
								
									<li class="nav-item "><a href="/sampling/mutation/">Mutation</a></li>
								
									<li class="nav-item "><a href="/sampling/demc/">Differential Evolution Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/demcmc/">Differential Evolution Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/dgmc/">Distributed Genetic Monte Carlo</a></li>
								
							</ul>
						</li>
					
				</ul>

				<ul>
					<li class="nav-item top-level ">
						
						<a href="/changelog/">Change Log</a>
					</li>
				</ul>
			</nav>
		</header>

		<section class="main">
			<div class="page-header">
				<h2>Hierarchical Model</h2>
				<h3>HLBA Model</h3>
			</div>
			<article class="content">
				<p>In this tutorial, I used the DE-MCMC sampler (Turner et al., 2013) to fit
a hierarchical LBA model. This particular design has a stimulus (S) and a
frequency (F) factor.</p>

<h2 id="set-up-model">Set-up Model</h2>
<p>I used the usual DMC-style syntax to set up the model. I hope, at this point,
it might be clear for the reader who has read through the <em>Model Array</em>, that
the meaning of syntax in <em>p.map</em>.</p>

<blockquote>
  <p>p.map = list(A = “1”, B = “R”, t0 = “1”, mean_v = c(“F”, “M”), sd_v = “M”,
st0 = “1”),</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">ggdmc</span><span class="p">)</span>

<span class="k">model</span> <span class="p">&lt;-</span> <span class="n">BuildModel</span><span class="p">(</span>
          <span class="n">p</span><span class="p">.</span><span class="n">map</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">A</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">B</span> <span class="p">=</span> <span class="s2">"R"</span><span class="p">,</span> <span class="n">t0</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span>
                    <span class="n">mean_v</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="s2">"F"</span><span class="p">,</span> <span class="s2">"M"</span><span class="p">),</span> <span class="n">sd_v</span> <span class="p">=</span> <span class="s2">"M"</span><span class="p">,</span> <span class="n">st0</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">),</span>
          <span class="n">match</span><span class="p">.</span><span class="n">map</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">M</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">s1</span><span class="p">=</span><span class="m">1</span><span class="p">,</span> <span class="n">s2</span><span class="p">=</span><span class="m">2</span><span class="p">)),</span>
          <span class="n">factors</span>   <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">S</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="s2">"s1"</span><span class="p">,</span> <span class="s2">"s2"</span><span class="p">),</span><span class="n">F</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="s2">"f1"</span><span class="p">,</span> <span class="s2">"f2"</span><span class="p">)),</span>
          <span class="n">constants</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="n">sd_v</span><span class="p">.</span><span class="nb">false</span> <span class="p">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">st0</span> <span class="p">=</span> <span class="m">0</span><span class="p">),</span>
          <span class="n">responses</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="s2">"r1"</span><span class="p">,</span> <span class="s2">"r2"</span><span class="p">),</span>
          <span class="n">type</span>      <span class="p">=</span> <span class="s2">"norm"</span><span class="p">)</span>
<span class="p">##</span> <span class="n">Parameter</span> <span class="n">vector</span> <span class="n">names</span> <span class="p">(</span><span class="n">unordered</span><span class="p">)</span> <span class="n">are</span><span class="p">:</span> <span class="p">(</span> <span class="n">see</span> <span class="n">attr</span><span class="p">(,</span><span class="s2">"p.vector"</span><span class="p">)</span> <span class="p">)</span>
<span class="p">##</span> <span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="s2">"A"</span>               <span class="s2">"B.r1"</span>            <span class="s2">"B.r2"</span>            <span class="s2">"t0"</span>             
<span class="p">##</span> <span class="p">[</span><span class="m">5</span><span class="p">]</span> <span class="s2">"mean_v.f1.true"</span>  <span class="s2">"mean_v.f2.true"</span>  <span class="s2">"mean_v.f1.false"</span> <span class="s2">"mean_v.f2.false"</span>
<span class="p">##</span> <span class="p">[</span><span class="m">9</span><span class="p">]</span> <span class="s2">"sd_v.true"</span>      
<span class="p">##</span> 
<span class="p">##</span> <span class="n">Constants</span> <span class="n">are</span> <span class="p">(</span><span class="n">see</span> <span class="n">attr</span><span class="p">(,</span><span class="s2">"constants"</span><span class="p">)</span> <span class="p">):</span>
<span class="p">##</span> <span class="n">sd_v</span><span class="p">.</span><span class="nb">false</span>        <span class="n">st0</span> 
<span class="p">##</span>          <span class="m">1</span>          <span class="m">0</span> 
<span class="p">##</span> 
<span class="p">##</span> <span class="k">Model</span> <span class="n">type</span> <span class="p">=</span> <span class="n">norm</span> <span class="p">(</span><span class="n">posdrift</span> <span class="p">=</span> <span class="nb">TRUE</span> <span class="p">)</span>

<span class="n">npar</span> <span class="p">&lt;-</span> <span class="n">length</span><span class="p">(</span><span class="n">GetPNames</span><span class="p">(</span><span class="k">model</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>mean_v</strong> = c(“F”, “M”), refers to that the mean of the drift rate is
affected by the <em>F</em> and <em>M</em> factors. The former is an experimental factor and
the latter is a latent LBA specific factor. <strong>B = “R”</strong> refers to that
the travel distance parameter is affected by the response factor, which in
a binary decision task, is two levels. Here it is either “r1” or “r2”, defined
in the response option / argument.</p>

<blockquote>
  <p>responses = c(“r1”, “r2”),</p>
</blockquote>

<p>Therefore, the data.frame (an R way to store real or simulated data ) should
have an R column, similar to the below:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; dplyr::tbl_df(dat)
# A tibble: 40,000 x 5
   s     S     F     R        RT
   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;
 1 1     s1    f1    r1    0.745
 2 1     s1    f1    r1    0.883
 3 1     s1    f1    r1    0.884
 4 1     s1    f1    r1    0.678
 5 1     s1    f1    r1    0.729
 6 1     s1    f1    r1    0.803
 7 1     s1    f1    r1    0.735
 8 1     s1    f1    r1    0.756
 9 1     s1    f1    r1    0.847
10 1     s1    f1    r1    0.855
# ... with 39,990 more rows
</code></pre></div></div>

<ol>
  <li><em>s</em> refers to subject label,</li>
  <li><em>S</em> is stimulus factor,</li>
  <li><em>F</em> is frequency factor,</li>
  <li><em>R</em> is response factor,</li>
  <li><em>fct</em> refers to <em>factor</em>, namely a categorical variable,</li>
  <li><em>dbl</em> refers to <em>double</em>, namely a continuous, numerical variable.</li>
</ol>

<p>In this design, the <em>S</em> factor does not affect any model parameters, and the
<em>F</em> factor affects the mean of the drift rate, <strong>mean_v = c(“F”, “M”)</strong>. Because
this is a parameter recovery study, I simulated a data set based on the
above model. Similarly, I used a random-effects model to generate the data
set, so I defined a set of population distribution for each LBA parameters.
The names of the parameter were reported by the <em>BuildModel</em> function.</p>

<h2 id="simulate-data">Simulate Data</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pop.mean &lt;- c(A=.4, B.r1=.6, B.r2=.8, t0=.3, mean_v.f1.true=1.5,
              mean_v.f2.true=1, mean_v.f1.false=0, mean_v.f2.false=0,
              sd_v.true = .25)
pop.scale &lt;-c( rep(.1, 3), .05, rep(.2, 4), .1)
names(pop.scale) &lt;- names(pop.mean)

pop.prior &lt;- BuildPrior(
  dists = rep("tnorm", npar),
  p1    = pop.mean,
  p2    = pop.scale,
  lower = c(rep(0, 3), .1, rep(NA, 4), 0),
  upper = c(rep(NA, 3), 1, rep(NA, 5)))

dat &lt;- simulate(model, nsim = 30, nsub = 40, prior = pop.prior)
dmi &lt;- BuildDMI(dat, model)
ps &lt;- attr(dat, "parameters")
</code></pre></div></div>

<p>Then I set up prior distributions and hyper-prior distributions.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### FIT RANDOM EFFECTS
p.prior &lt;- BuildPrior(
  dists = rep("tnorm", npar),
  p1    = pop.mean,
  p2    = pop.scale*5,
  lower = c(0,0,0,.1,NA,NA,NA,NA,0),
  upper = c(NA,NA,NA,NA,NA,NA,NA,NA,NA))
mu.prior &lt;- BuildPrior(
  dists = rep("tnorm", npar),
  p1    = pop.mean,
  p2    = c(1,1,1,1,2,2,2,2,1),
  lower = c(0,0,0,.1,NA,NA,NA,NA,0),
  upper = c(NA,NA,NA,NA,NA,NA,NA,NA,NA))
  
sigma.prior &lt;- BuildPrior(
  dists = rep("beta", npar),
  p1    = c(A=1, B.r1=1, B.r2=1, t0=1, mean_v.f1.true=1,
            mean_v.f2.true=1, mean_v.f1.false=1, mean_v.f2.false=1,
            sd_v.true = 1),
  p2    = rep(1, npar))

###### Must names "pprior", "location", and "scale" to the prirors list  #######
priors &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior)
###### Must names "pprior", "location", and "scale" to the prirors list  #######

</code></pre></div></div>

<h2 id="sampling">Sampling</h2>
<p>Next, I started the sampling. When the <em>debug</em> argument is set TRUE,
the <em>run</em> function uses the conventional DE-MCMC sampler, with the
its original migration operator.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fit0 &lt;- StartNewsamples(dmi, priors)
fit  &lt;- run(fit0, thin = 8)

</code></pre></div></div>

<h2 id="model-diagnosis">Model Diagnosis</h2>
<p>Next, I checked if the model has converged and analyze the model estimation.
In this tutorial, I did the numerical checks firstly by calculating the
potential scale reduction factors (Brook &amp; Gelman,1998). All are less than 1.05,
suggesting all chains are well mixed.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rhat &lt;- hgelman(fit, verbose = TRUE)
## hyper     1     2     3     4     5     6     7     8     9    10    11    12    13
##  1.02  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01
##    14    15    16    17    18    19    20    21    22    23    24    25    26    27
##  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01
##    28    29    30    31    32    33    34    35    36    37    38    39    40
##  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01  1.01
</code></pre></div></div>

<p>Then, I calculated effective samples at the hyper parameters, for one participant at
the parameter of the data level, and similarly for all participants. This is to 
check if enough posterior samples are drawn.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hes &lt;- effectiveSize(hsam, hyper = TRUE)
## 
##               A.h1            B.r1.h1            B.r2.h1              t0.h1 
##               4655               1416               1152               2061 
##  mean_v.f1.true.h1  mean_v.f2.true.h1 mean_v.f1.false.h1 mean_v.f2.false.h1 
##               1748               2828               1504               1505 
##       sd_v.true.h1               A.h2            B.r1.h2            B.r2.h2 
##               3997               3406               3611               2630 
##              t0.h2  mean_v.f1.true.h2  mean_v.f2.true.h2 mean_v.f1.false.h2 
##               4906               5329               4615               4342 
## mean_v.f2.false.h2       sd_v.true.h2 
##               4481               3887

es1 &lt;- effectiveSize(hsam[[1]])
##               A            B.r1            B.r2              t0  mean_v.f1.true 
##           13531            2819            2629            6100            3755 
##  mean_v.f2.true mean_v.f1.false mean_v.f2.false       sd_v.true 
##            3793            4983            4239            6572

es &lt;- effectiveSize(hsam)
round(apply(data.frame(es), 1, mean))
round(apply(data.frame(es), 1, sd))
round(apply(data.frame(es), 1, max))
round(apply(data.frame(es), 1, min))
##               A            B.r1            B.r2              t0  mean_v.f1.true 
## Mean      11159            2972            2726            6577            4055
## SD         2728             295             172             903             562
## MAX       13967            3778            3196            8183            5273
## MIN        4525            2382            2331            4581            3092 
##  mean_v.f2.true mean_v.f1.false mean_v.f2.false       sd_v.true 
##            3654            4793            4683            8110 
##             598             435             455            1748 
##            5056            5582            5699           13163
##            2647            3960            3865            5830
</code></pre></div></div>

<p>Then I did the visual check by plotting the six types of trace and density plots.
Firstly, I plot the hyper parameters. By entering TRUE to the option, <em>save</em>, you
save the data in a <em>data.table</em> format<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, for further processing. For instance,
you can change the figure to fit the publication requirement.</p>

<ul>
  <li>Trace plots of posterior log-likelihood at hyper level</li>
  <li>Trace plots of the hyper parameters</li>
  <li>Posterior density plots the hyper parameters</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot(hsam, hyper = TRUE)
DT1 &lt;- plot(hsam, hyper = TRUE, pll = FALSE, save = TRUE)
plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE)
</code></pre></div></div>

<p><img src="/images/random-effect-model/HLBA/hyper-pll.png" alt="pll-hyper" /></p>

<p><img src="/images/random-effect-model/HLBA/hyper-den.png" alt="density-hyper" /></p>

<p>Next I checked the trace plots of the posterior log-likelihood and each of the
LBA parameters at the data level.</p>

<ul>
  <li>Trace plots of posterior log-likelihood at the data level</li>
  <li>Trace plots of the LBA parameters for each participants</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot(hsam)
plot(hsam, pll = FALSE)
</code></pre></div></div>

<p><img src="/images/random-effect-model/HLBA/trace-data.png" alt="density-hyper" /></p>

<p>Last, I checked the (posterior) density plots at the data level. There are too many
density plots for each participants (nsubject x nparameter = 360), so I did not 
present them here.</p>

<ul>
  <li>Posterior density plots the LBA parameters for each parameters</li>
</ul>

<p>Because this is a parameter recovery study, in the following, I used <em>summary</em>
function to check whether Bayesian estimates do recover the true parameters of
all participant as well as the mechanism of data generation, namely, <em>pop.mean</em>
and <em>pop.scale</em>.</p>

<p>There are five arguments in the <em>summary</em> function you need to know to trigger
the smart parameter recovery computation. First is <strong>hyper = TRUE</strong>, which
calculate the phi array / matrix, which stores hyper parameters. Second is
the <strong>recovery = TRUE</strong>, which informs the function to look for a true
parameter vector, which you should enter it at <strong>ps</strong> argument (<strong>ps = pop.mean</strong>).
Otherwise, the function will throw an error message, complaining that it
cannot find the true parameter vector.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>est1 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1)
## Error in summary_recoverone(samples, start, end, ps, digits, verbose) : 
## Names of p.vector do not match parameter names in samples
</code></pre></div></div>

<p>The fourth argument is <em>type = 1</em>. This is a hyper parameter recovery specific.
This is to recover the location (mostly <em>mean</em>) parameters. When <strong>type = 2</strong>, the
function will attempt to recover the scale parameters, which mostly refer to
standard deviations. The last useful argument is <strong>verbose = TRUE</strong>, printing
message.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>est1 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1)
## No print, the estimates are stored in est1.

est2 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.scale, type = 2,
verbose = TRUE)
## Storing estmates in est2 and print results rounding to the second digits
##                   A B.r1  B.r2 mean_v.f1.false mean_v.f1.true mean_v.f2.false
## True           0.10 0.10  0.10            0.20           0.20            0.20
## 2.5% Estimate  0.10 0.09  0.02            0.15           0.17            0.19
## 50% Estimate   0.14 0.12  0.06            0.22           0.21            0.26
## 97.5% Estimate 0.19 0.16  0.10            0.30           0.27            0.34
## Median-True    0.04 0.02 -0.04            0.02           0.01            0.06
##                mean_v.f2.true sd_v.true    t0
## True                     0.20      0.10  0.05
## 2.5% Estimate            0.21      0.11  0.03
## 50% Estimate             0.26      0.14  0.04
## 97.5% Estimate           0.34      0.21  0.06
## Median-True              0.06      0.04 -0.01
</code></pre></div></div>

<p>By checking the <em>Median-True</em>, namely 50% quantile minus true values, I can
confirm that I did recover the true hyper scale parameters. The lines,
<em>2.5% Estimate</em> and <em>97.5% Estimate</em> inform that the 95% credible intervals,
cover the true hyper parameters well. As expected the <em>A</em> and <em>B</em> parameters
are sometimes at the boundaries of the 95% intervals, as they are linearly
correlated.</p>

<p>More often, researchers concern the location estimates, which I printed out the
results stored in <em>est1</em>.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  A B.r1 B.r2 mean_v.f1.false mean_v.f1.true mean_v.f2.false
True           0.40 0.60 0.80            0.00           1.50            0.00
2.5% Estimate  0.35 0.61 0.83            0.00           1.47            0.00
50% Estimate   0.40 0.68 0.91            0.16           1.58            0.17
97.5% Estimate 0.45 0.76 0.99            0.31           1.68            0.32
Median-True    0.00 0.08 0.11            0.16           0.08            0.17
               mean_v.f2.true sd_v.true    t0
True                     1.00      0.25  0.30
2.5% Estimate            1.04      0.17  0.26
50% Estimate             1.14      0.25  0.28
97.5% Estimate           1.25      0.30  0.30
Median-True              0.14      0.00 -0.02
</code></pre></div></div>

<p>The estimates look pretty good. My estimation correctly reflects the difference of
the two threshold-related parameters. The true values are <strong>B.r1 = .6</strong> and
<strong>_B.r2 = .8</strong>. The estimates are <strong>B.r1 = .68</strong> and <strong>_B.r2 = .91</strong>
(B.r2 &gt; B.r1). Even the difference is very close (0.20 vs. 0.23).</p>

<p>As expected, the mean drift rates for the error accumulators (e.g., mean_v.f1.false)
are very difficult to estimate, because I set my prior distributions / belief
truncated at the 0 boundary, reflect how prior belief may affect the estimate.</p>

<p>More important is the estimates of the mean drift rate for the correct
accumulators (e.g., mean_v.f1.true). Again the condition f1 (e.g., high word
frequency) has faster drift than the condition f2 (e.g., low word frequency). The
estimates, f1 = 1.58 &gt; f2 = 1.14, closely match the true values
f1 = 1.50 &gt; f2 = 1.00.</p>

<p>There are more useful options in the <em>summary</em> function, which I will return to
in a later tutorial.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hest &lt;- summary(hsam, hyper = TRUE, hmeans = TRUE)
hest &lt;- summary(hsam, hyper = TRUE, hci = TRUE, prob = c("25%", "75%"))
hest &lt;- summary(hsam, hyper = TRUE, hci = TRUE, prob = c("25%", "75%"), digits = 3)

</code></pre></div></div>

<h2 id="posterior-predictive-check">Posterior predictive check</h2>

<p>You can also pipe the result to DMC to use its <em>h.post.predict.dmc</em> to conduct
posterior predictive check at the hyper parameter. Note this will take a while,
because piping back to DMC means using <em>R</em> language to process large
Bayesian MCMC data. Further tutorials for DMC, please refer to Heathcote and
colleagues (2018).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>setwd("/media/yslin/MERLIN/Documents/DMCpaper/")
source ("dmc/dmc.R")
load_model ("LBA","lba_B.R")
setwd("/media/yslin/MERLIN/Documents/ggdmc_paper/")
hpp &lt;- h.post.predict.dmc(hsam)
plot.pp.dmc(hpp)
tmp &lt;- lapply(hpp, function(x){plot.pp.dmc(x, style = "cdf") })
</code></pre></div></div>

<p><img src="/images/random-effect-model/HLBA/ppc_pdf1.png" alt="ppc-pdf" />
<img src="/images/random-effect-model/HLBA/ppc_cdf1.png" alt="ppc-cdf" /></p>

<h2 id="reference">Reference</h2>
<p>Heathcote, A., Lin, Y.-S., Reynolds, A., Strickland, L., Gretton, M. &amp; Matzke, D., (2018).
<a href="https://rdcu.be/2ccf">Dynamic model of choice</a>.
Behavior Research Methods. https://doi.org/10.3758/s13428-018-1067-y.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>a different way to store and manipulation data in R. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

			</article>
		</section>

		<script>
			document.getElementById("open-nav").addEventListener("click", function () {
				document.body.classList.toggle("nav-open");
			});

			
		</script>

		  
		</script>
	</body>
</html>
