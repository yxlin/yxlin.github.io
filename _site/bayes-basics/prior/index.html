<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Prior Distribution | Cognitive Models</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Prior Distribution" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In Bayesian computation, a prior distribution refers to a similar, but slightly different idea from the original Bayes’ theorem. I used the diffusion decisoin model (DDM, Ratcliff &amp; McKoon, 2008) as an example to illustrate the idea." />
<meta property="og:description" content="In Bayesian computation, a prior distribution refers to a similar, but slightly different idea from the original Bayes’ theorem. I used the diffusion decisoin model (DDM, Ratcliff &amp; McKoon, 2008) as an example to illustrate the idea." />
<link rel="canonical" href="http://localhost:4000/bayes-basics/prior/" />
<meta property="og:url" content="http://localhost:4000/bayes-basics/prior/" />
<meta property="og:site_name" content="Cognitive Models" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-19T14:22:09+00:00" />
<script type="application/ld+json">
{"datePublished":"2020-01-19T14:22:09+00:00","description":"In Bayesian computation, a prior distribution refers to a similar, but slightly different idea from the original Bayes’ theorem. I used the diffusion decisoin model (DDM, Ratcliff &amp; McKoon, 2008) as an example to illustrate the idea.","@type":"Article","url":"http://localhost:4000/bayes-basics/prior/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/siteicon.png"}},"headline":"Prior Distribution","dateModified":"2020-01-19T14:22:09+00:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Cognitive Models" />

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,400italic,700,700italic|Open+Sans:400,400italic,600,600italic,700,700italic|Inconsolata:400,700">
		<link rel="stylesheet" href="/css/main.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">

		
	</head>

	<body>
	  <header>
	    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
	    
			<h1>
				<a href="/"><img src="/images/emblem.svg" width="40" height="40" alt="Cognitive Models logo"></a>
				Cognitive Models
				<button type="button" class="open-nav" id="open-nav"></button>
			</h1>

			<form action="/search/" method="get">
				<input type="text" name="q" id="search-input" placeholder="Search" autofocus>
				<input type="submit" value="Search" style="display: none;">
			</form>

			<nav >
				<ul>
					<li class="nav-item top-level ">
						
						<a href="/"></a>
					</li>
				</ul>

				<ul>
					
					
						<li class="nav-item top-level ">
							
							<a href="/BUGS/hnormal/">BUGS Examples Volumn 1</a>
							<ul>
								
									<li class="nav-item "><a href="/BUGS/hnormal/">Hierarchical Normal Model</a></li>
								
									<li class="nav-item "><a href="/BUGS/seeds/">Random effect logistic regression</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/approximation/kde/">Likelihood Free Method</a>
							<ul>
								
									<li class="nav-item "><a href="/approximation/kde/">Kernel Density Estimation</a></li>
								
									<li class="nav-item "><a href="/approximation/pda/">Probability Density Approximation</a></li>
								
									<li class="nav-item "><a href="/approximation/ppda/">Parallel Probability Density Approximation</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/basics/model_array/">Modelling Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/basics/model_array/">Model Array</a></li>
								
									<li class="nav-item "><a href="/basics/simulation/">Simulation</a></li>
								
									<li class="nav-item "><a href="/basics/descriptive/">Descriptive Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/summary/">Summary Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/mle/">Maximising Likelihoods</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level current">
							
							<a href="/bayes-basics/theorem/">Bayesian Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/bayes-basics/theorem/">Bayes' Theorem</a></li>
								
									<li class="nav-item current"><a href="/bayes-basics/prior/">Prior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/likelihood/">Model Likelihood</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/posterior/">Posterior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/diagnosis/">Checking Fitted Models</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/cognitive-model/lba/">Cognitive Model</a>
							<ul>
								
									<li class="nav-item "><a href="/cognitive-model/lba/">LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pm/">PM Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/plba/">PLBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm/">Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/cddm/">Circular Drift-diffusion Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pddm/">PDDM</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lba3/">Three-accumulator LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lca/">LCA Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/fixed-effect-model/one_participant/">Fixed-effects Model</a>
							<ul>
								
									<li class="nav-item "><a href="/fixed-effect-model/one_participant/">One Participant</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/many_participants/">Multiple Participants</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/cddm12S/">CDDM</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/mcmc/mcmc/">Modern Bayesian Statistics</a>
							<ul>
								
									<li class="nav-item "><a href="/mcmc/mcmc/">Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/mcmc/rwm/">Random Walk Metropolis</a></li>
								
									<li class="nav-item "><a href="/mcmc/hastings/">Metropolis-Hastings</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/random-effect-model/hlba/">Hierarchical Model</a>
							<ul>
								
									<li class="nav-item "><a href="/random-effect-model/hlba/">HLBA Model</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hddm/">HDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision1/">Shooting Decision Model - Recovery Study</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision2/">Shooting Decision Model - Empirical Data</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hcddm/">HCDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hpm/">HPM Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/sampling/genetic/">Sampling Techniques</a>
							<ul>
								
									<li class="nav-item "><a href="/sampling/genetic/">Population-based Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/crossover/">Crossover</a></li>
								
									<li class="nav-item "><a href="/sampling/migration/">Migration</a></li>
								
									<li class="nav-item "><a href="/sampling/mutation/">Mutation</a></li>
								
									<li class="nav-item "><a href="/sampling/demc/">Differential Evolution Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/demcmc/">Differential Evolution Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/dgmc/">Distributed Genetic Monte Carlo</a></li>
								
							</ul>
						</li>
					
				</ul>

				<ul>
					<li class="nav-item top-level ">
						
						<a href="/changelog/">Change Log</a>
					</li>
				</ul>
			</nav>
		</header>

		<section class="main">
			<div class="page-header">
				<h2>Bayesian Basics</h2>
				<h3>Prior Distribution</h3>
			</div>
			<article class="content">
				<p>In Bayesian computation, a prior distribution refers to a similar, but
slightly different idea from the original Bayes’ theorem. I used the diffusion
decisoin model (DDM, Ratcliff &amp; McKoon, 2008) as an example to illustrate the idea.</p>

<p>The full DDM has eight parameters. In <em>ggdmc</em> (as well as DMC) syntax, they are
defined as following:</p>

<blockquote>
  <p>p.map = list(a = “1”, v = “1”, z = “1”, d = “1”, sz = “1”, sv = “1”,
             t0 = “1”, st0 = “1”),</p>
</blockquote>

<ol>
  <li>a: the boundary separation</li>
  <li>v: the mean of the drift rate</li>
  <li>z: the mean of the starting point of the diffusion relative to threshold separation</li>
  <li>d: differences in the non-decisional component between upper and lower threshold</li>
  <li>sz: the width of the support of the distribution of zr</li>
  <li>sv: the standard deviation of the drift rate</li>
  <li>t0: the mean of the non-decisional component of the response time</li>
  <li>st0: the width of the support of the distribution of t0</li>
</ol>

<p>The question is how do we determine the values for these parameters. This is where
prior distribution comes in.  We presume there are eight distributions jointly
determine the DDM prior distribution and these eight distributions are where
we draw the realized parameter values. This way, the parameter values are said 
stochastic, rather than deterministic. In other words, the value, for instance
boundary separation, <em>a</em>, changes every time we consult its prior distribution.
It is decided probabilistically by its prior distribution.</p>

<p>Below I list the full command, <em>BuildModel</em>, for setting up a DDM model.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">##</span> <span class="n">Use</span> <span class="n">verbose</span> <span class="n">option</span> <span class="k">to</span> <span class="n">suppress</span> <span class="n">printing</span> <span class="n">p</span><span class="p">.</span><span class="n">vector</span>
<span class="p">##</span> <span class="n">This</span> <span class="n">is</span> <span class="n">a</span> <span class="n">DDM</span> <span class="k">model</span> <span class="k">with</span> <span class="n">no</span> <span class="n">manipulation</span> <span class="n">factor</span>
<span class="k">model</span> <span class="p">&lt;-</span> <span class="n">BuildModel</span><span class="p">(</span>
  <span class="n">p</span><span class="p">.</span><span class="n">map</span>     <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">a</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">v</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">z</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">d</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">sz</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">sv</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span>
    <span class="n">t0</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">,</span> <span class="n">st0</span> <span class="p">=</span> <span class="s2">"1"</span><span class="p">),</span>
  <span class="n">match</span><span class="p">.</span><span class="n">map</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">M</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">left</span> <span class="p">=</span> <span class="s2">"LEFT"</span><span class="p">,</span> <span class="n">right</span> <span class="p">=</span> <span class="s2">"RIGHT"</span><span class="p">)),</span>
  <span class="n">factors</span>   <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">S</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="s2">"left"</span><span class="p">,</span> <span class="s2">"right"</span><span class="p">)),</span>
  <span class="n">constants</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="n">st0</span> <span class="p">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">d</span> <span class="p">=</span> <span class="m">0</span><span class="p">),</span>
  <span class="n">responses</span> <span class="p">=</span> <span class="n">c</span><span class="p">(</span><span class="s2">"LEFT"</span><span class="p">,</span> <span class="s2">"RIGHT"</span><span class="p">),</span>
  <span class="n">type</span>      <span class="p">=</span> <span class="s2">"rd"</span><span class="p">,</span>
  <span class="n">verbose</span>   <span class="p">=</span> <span class="nb">TRUE</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="set-up-priors">Set up Priors</h2>
<p>So in this example, we will want to set up six prior distributions, because
in the above model set-up, the <em>st0</em> and <em>d</em> have been set to constant as 0. That 
is, they are deterministic, not stochastic. <em>ggdmc</em> (as well as DMC)
has a function to build prior. Unimaginatively, it is called <em>BuildPrior</em>
(it is called <em>p.prior.dmc</em> in DMC).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p.prior &lt;- BuildPrior(
             p1    = c(a = 1.5, v = 3,  z = .5, sz = .3, sv = 1,  t0 = .2),
             p2    = c(a = 0.5, v = .5, z = .1, sz = .1, sv = .3, t0 =.05),
             lower = c(0, -5, 0, 0, 0, 0),
             upper = c(2, 7, 4, 4, 4, 1),
             dists = rep("tnorm", 6))

</code></pre></div></div>

<p>A list of options / arguments for the <em>BuildPrior</em> function can be found by enter:</p>

<blockquote>
  <p>?BuildPrior</p>
</blockquote>

<p>Here is a copy from the R documentation in <em>ggdmc</em> pacakge.</p>

<ul>
  <li>p1 simply means the first parameter of a distribution</li>
  <li>p2 simiarly mean the second parameter of a distribution</li>
  <li>lower is the lower support (i.e., the lower truncated boundary)</li>
  <li>upper is the upper support (i.e., the upper truncated boundary)</li>
  <li>dists is a string vector specifying the name of a distribution.</li>
</ul>

<p>Current version of <em>ggdmc</em> provides four types of prior distributions:</p>
<ol>
  <li><em>tnorm</em>, Normal and truncated normal, where: p1 = mean, p2 = sd. It specifies
a normal distribution when bounds are set -Inf and Inf,</li>
  <li><em>beta</em>, Beta, where: p1 = shape1 and p2 = shape2 (see ?pbeta in R). Note the uniform
distribution is a special case of the beta distribution when p1 and p2 = 1,</li>
  <li><em>gamma</em>, Gamma, where p1 = shape and p2 = scale (see ?pgamma in R). Note p2 is scale,
not rate,</li>
  <li><em>lnorm</em>, Lognormal, where p1 = meanlog and p2 = sdlog (see ?plnorm).</li>
</ol>

<p>In the <em>ggdmc</em> (as well as DMC) operation, the names (i.e., character strings)
are important for corret computation.</p>

<p>The two options, <em>lower</em> and <em>upper</em>, are to set the distribution support.</p>
<ul>
  <li>for <em>tnorm</em>, these define the lower and upper bounds; When the user enters <strong>NA</strong>
, the default behaviour of the function is to set the values as -Inf and Inf.
This make a truncated normal distribution becoming a normal distribution (see ?pnorm).</li>
  <li>for <em>beta</em>, these define the lower and upper bounds (i.e., scaled beta distribution).
The default behaviour for entering <em>NA</em> is to filled with the values of 0 and 1.
    <ul>
      <li>p1 = 1 &amp; p2 = 1 &amp; lower = 0 (default) &amp; upper = 1 (default) creates Uniform(0, 1)</li>
      <li>p1 = 1 &amp; p2 = 1 &amp; lower = l &amp; upper = u creates Uniform(l, u)</li>
    </ul>
  </li>
  <li>for gamma, lower shifts the distribution to exclude small values</li>
  <li>for lognormal, lower shifts the distribution to exclude small values</li>
</ul>

<h2 id="example-1-set-up-beta-and-uniform-prior">Example 1: Set up beta (and uniform) prior</h2>

<p>Currently, the below example is from Heathcote et al’s (2018) DMC tutorial of LNR model.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>beta.prior &lt;- BuildPrior(
  dists = c("beta", "beta", "beta", "beta", "beta"),
  p1 = c(meanlog.true = 1, meanlog.false = 1, sdlog.true = 1, sdlog.false = 1, t0 = 1),
  p2 = c(meanlog.true = 1, meanlog.false = 1, sdlog.true = 1, sdlog.false = 1, t0 = 1),
  lower = c(-4,-4, 0, 0, 0.1),
  upper = c( 4, 4, 4, 4, 1))
</code></pre></div></div>

<p>You can plot and print the prior distribution by using the <em>plot</em> and <em>print</em> functions.</p>

<blockquote>
  <p>plot(beta.prior)</p>
</blockquote>

<p><img src="/images/bayes/prior1.png" alt="betaprior" /></p>

<blockquote>
  <p>print(p.prior)</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##               p1 p2 lower upper log    dist  untrans
## meanlog.true   1  1    -4     4   1 beta_lu identity
## meanlog.false  1  1    -4     4   1 beta_lu identity
## sdlog.true     1  1     0     4   1 beta_lu identity
## sdlog.false    1  1     0     4   1 beta_lu identity
## t0             1  1   0.1     1   1 beta_lu identity
</code></pre></div></div>

<p>This is how to calculate log-prior likelihoods (i.e., probability densities)
for each model parameter and add them all together.</p>

<blockquote>
  <p>dprior(p.vector, p.prior)</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## meanlog.true meanlog.false    sdlog.true   sdlog.false            t0 
##    -2.0794415    -2.0794415    -1.3862944    -1.3862944     0.1053605
</code></pre></div></div>

<blockquote>
  <p>sumlogpriorNV(p.vector, p.prior)</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] -6.826111
</code></pre></div></div>

<h3 id="what-to-look-for-when-set-up-prior-distributions">What to look for when set up prior distributions</h3>
<p>For setting up prior distributions, key points are to look for first whether prior
distributions cover broad range (i.e., relatively uninformative) and second whether
their range cover abnormal values. For example, it is not possible to have
negative standard devation, so sd_v.true subpanel should not cover negative values.</p>


			</article>
		</section>

		<script>
			document.getElementById("open-nav").addEventListener("click", function () {
				document.body.classList.toggle("nav-open");
			});

			
		</script>

		  
		</script>
	</body>
</html>
