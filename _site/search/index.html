<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Search | Cognitive Models</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dynamic Models of Choice with Better Graphic Tools and Quicker Computations" />
<meta property="og:description" content="Dynamic Models of Choice with Better Graphic Tools and Quicker Computations" />
<link rel="canonical" href="http://localhost:4000/search/" />
<meta property="og:url" content="http://localhost:4000/search/" />
<meta property="og:site_name" content="Cognitive Models" />
<script type="application/ld+json">
{"description":"Dynamic Models of Choice with Better Graphic Tools and Quicker Computations","@type":"WebPage","url":"http://localhost:4000/search/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/siteicon.png"}},"headline":"Search","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Cognitive Models" />

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,400italic,700,700italic|Open+Sans:400,400italic,600,600italic,700,700italic|Inconsolata:400,700">
		<link rel="stylesheet" href="/css/main.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">

		
	</head>

	<body>
	  <header>
	    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
	    
			<h1>
				<a href="/"><img src="/images/emblem.svg" width="40" height="40" alt="Cognitive Models logo"></a>
				Cognitive Models
				<button type="button" class="open-nav" id="open-nav"></button>
			</h1>

			<form action="/search/" method="get">
				<input type="text" name="q" id="search-input" placeholder="Search" autofocus>
				<input type="submit" value="Search" style="display: none;">
			</form>

			<nav >
				<ul>
					<li class="nav-item top-level ">
						
						<a href="/"></a>
					</li>
				</ul>

				<ul>
					
					
						<li class="nav-item top-level ">
							
							<a href="/BUGS/hnormal/">BUGS Examples Volumn 1</a>
							<ul>
								
									<li class="nav-item "><a href="/BUGS/hnormal/">Hierarchical Normal Model</a></li>
								
									<li class="nav-item "><a href="/BUGS/seeds/">Random effect logistic regression</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/approximation/kde/">Likelihood Free Method</a>
							<ul>
								
									<li class="nav-item "><a href="/approximation/kde/">Kernel Density Estimation</a></li>
								
									<li class="nav-item "><a href="/approximation/pda/">Probability Density Approximation</a></li>
								
									<li class="nav-item "><a href="/approximation/ppda/">Parallel Probability Density Approximation</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/basics/model_array/">Modelling Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/basics/model_array/">Model Array</a></li>
								
									<li class="nav-item "><a href="/basics/simulation/">Simulation</a></li>
								
									<li class="nav-item "><a href="/basics/descriptive/">Descriptive Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/summary/">Summary Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/leastsq/">Least Square Method</a></li>
								
									<li class="nav-item "><a href="/basics/mle/">Maximising Likelihoods</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/bayes-basics/theorem/">Bayesian Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/bayes-basics/theorem/">Bayes' Theorem</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/prior/">Prior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/likelihood/">Model Likelihood</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/posterior/">Posterior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/diagnosis/">Checking Fitted Models</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/cognitive-model/lba/">Cognitive Model</a>
							<ul>
								
									<li class="nav-item "><a href="/cognitive-model/lba/">LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pm/">PM Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/plba/">PLBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm/">Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/cddm/">Circular Drift-diffusion Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pddm/">PDDM</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lba3/">Three-accumulator LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lca/">LCA Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/fixed-effect-model/one_participant/">Fixed-effects Model</a>
							<ul>
								
									<li class="nav-item "><a href="/fixed-effect-model/one_participant/">One Participant</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/many_participants/">Multiple Participants</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/cddm12S/">CDDM</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/mcmc/mcmc/">Modern Bayesian Statistics</a>
							<ul>
								
									<li class="nav-item "><a href="/mcmc/mcmc/">Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/mcmc/rwm/">Random Walk Metropolis</a></li>
								
									<li class="nav-item "><a href="/mcmc/hastings/">Metropolis-Hastings</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/random-effect-model/hlba/">Hierarchical Model</a>
							<ul>
								
									<li class="nav-item "><a href="/random-effect-model/hlba/">HLBA Model</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hddm/">HDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision1/">Shooting Decision Model - Recovery Study</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision2/">Shooting Decision Model - Empirical Data</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hcddm/">HCDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hpm/">HPM Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/sampling/genetic/">Sampling Techniques</a>
							<ul>
								
									<li class="nav-item "><a href="/sampling/genetic/">Population-based Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/crossover/">Crossover</a></li>
								
									<li class="nav-item "><a href="/sampling/migration/">Migration</a></li>
								
									<li class="nav-item "><a href="/sampling/mutation/">Mutation</a></li>
								
									<li class="nav-item "><a href="/sampling/demc/">Differential Evolution Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/demcmc/">Differential Evolution Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/dgmc/">Distributed Genetic Monte Carlo</a></li>
								
							</ul>
						</li>
					
				</ul>

				<ul>
					<li class="nav-item top-level ">
						
						<a href="/changelog/">Change Log</a>
					</li>
				</ul>
			</nav>
		</header>

		<section class="main">
			<div class="page-header">
				<h2>Cognitive Models</h2>
				<h3>Search</h3>
			</div>
			<article class="content">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					

					"bugs-hnormal": {
						"id": "bugs-hnormal",
						"title": "Hierarchical Normal Model",
						"category": "",
						"url": " /BUGS/hnormal/",
						"content": "Disclaimer: This tutorial uses an experimental (beta) version of ggdmc, which has added the functionality of fitting regression models. The software can be found in its GitHub. The aim of tutorial is to docuemnt one method to fit an hierarchical normal model, using the Rats data. Rats data were studied in Gelfand (1990) and used in the BUGS examples volumn I. This expands the scope of ggdmc, not only to fit cognitive models but also to fit standard regression models. I first convert the data from wide to long format. setwd(\"~ BUGS_Examples vol1 Rats \") tmp &lt;- dget(\"data dataBUGS.R\") d &lt;- data.frame(matrix(as.vector(tmp$Y), nrow = 30, byrow = TRUE)) names(d) &lt;- c(8, 15, 22, 29, 36) d$s &lt;- factor(1:tmp$N) long &lt;- melt(d, id.vars = c(\"s\"), variable.name = \"xfac\", value.name = \"RT\") dplyr::tbl_df(long) long$X &lt;- as.double(as.character(long$xfac)) - tmp$xbar long$S &lt;- factor(\"x1\") long$R &lt;- factor(\"r1\") d &lt;- long[, c(\"s\", \"S\", \"R\", \"X\", \"RT\")] The data can be visualized as many lines, each representing a subject (rat). p1 &lt;- ggplot(d1, aes(x = X, y = RT, group = s, colour = s)) + geom_line(size = 1) + geom_point() + ylab(\"Weight\") + ggtitle(\"Complete data\") + coord_cartesian(ylim = c(120, 380)) + scale_colour_grey(na.value = \"black\") + theme_bw(base_size = 20) + theme(legend.position = \"none\") Set-up Model The DDM composes of two complementary defective distribtions; thereby, two response types. Unlike the DDM, a regression model has only one response type; that is one (complete) distribution. Therefore, the match.map and responses arguments are set as NULL and “r1” (meaning only one response type). The argument regressors enters the independent predictive variable (typically denoted X). In the Rats example, the weights of thirty young rats were measured weekly for five weeks and the measurements taken at the end of each week (8th, 15th, 22rd, 29th, &amp; 36th day) were provided. The parameterization is a (intercept), b (slope) and the precision (). require(ggdmc) model &lt;- BuildModel( p.map = list(a = \"1\", b = \"1\", tau = \"1\"), match.map = NULL, regressors = c(8, 15, 22, 29, 36) - tmp$xbar,, factors = list(S = c(\"x1\")), responses = \"r1\", constants = NULL, type = \"glm\") ## Parameter vector names are: ( see attr(,\"p.vector\") ) ## [1] \"a\" \"b\" \"tau\" ## ## Constants are (see attr(,\"constants\") ): ## NULL ## ## Model type = glm Recovery Study I take values from the Rats example as the true parameters at the population level and use them to simulate an ideal data set, which has 1000 rats and each of them contributes 100 response. tnorm2 is truncated normal distribution, using mean and precision parametrization. When both the upper and lower are set NA, the tnorm becomes normal distribution. npar &lt;- length(GetPNames(model)) pop.location &lt;- c(a = 242.7, b = 6.189, tau = .03) pop.scale &lt;- c(a = .005, b = 3.879, tau = .04) ntrial &lt;- 100 pop.prior &lt;-BuildPrior( dists = rep(\"tnorm2\", npar), p1 = pop.location, p2 = pop.scale, lower = c(NA, 0, 0), upper = rep(NA, npar)) dat &lt;- simulate(model, nsub = 1000, nsim = ntrial, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") ## Extract true parameters for each individual This plots the distributions that generate the simulation data and shows the location parameters of these distributions as dashed lines. plot(pop.prior, ps = pop.mean) Set up Priors To randomly draw initial values for the data- and hyper-level parameters, I set up three sets of distributions and bind them as a list, named start. These are used to generate start values only. pstart &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(a = 242, b = 6.19, tau = .027), p2 = c(a = 14, b = .49, tau = 10), lower = c(NA, NA, 0), upper = rep(NA, npar)) lstart &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(a = 200, b = 5, tau = .01), p2 = c(a = 50, b = 1, tau = .01), lower = c(NA, NA, 0), upper = rep(NA, npar)) sstart &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(a = 10, b = .5, tau = .01), p2 = c(a = 5, b = .1, tau = .01), lower = c(NA, NA, 0), upper = rep(NA, npar)) start &lt;- list(pstart, lstart, sstart) Next, I set up the structure of the hierarchical model. It is important to understand the hierarchical structure, so I sketch a diagram to show how the codes of setting up the prior distributions associate with the model structure. p.prior &lt;- BuildPrior( dists = rep(\"tnorm2\", npar), p1 = c(a = NA, b = NA, tau = NA), ## the value are drawn from hyper-level p2 = rep(NA, 3), ## (mu and sigma) prior, so all set NA lower = c(NA, 0, 0), upper = rep(NA, npar)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm2\", npar), p1 = c(a = 200, b = 6, tau = 3) p2 = c(a = 1e-4, b = 1e-3, tau = 1e-2) lower = c(NA, 0, 0), upper = rep(NA, npar)) sigma.prior &lt;- BuildPrior( dists = rep(\"gamma\", npar), p1 = c(a = .01, b = .01, tau = .01), p2 = c(a = 1000, b = 1000, tau = 1000), lower = c(0, 0, 0), upper = rep(NA, npar)) prior &lt;- list(p.prior, mu.prior, sigma.prior) Sampling The function, StartNewhiersamples use the start priors only for drawing initial values. The prior distributions wil be used in the model fit. fit &lt;- run(StartNewhiersamples(5e2, dmi, start, prior)) fit &lt;- run(RestartHypersamples(5e2, fit, thin = 32)) Model Diagnosis As usually, I check the potential scale reduction factors (Brook &amp; Gelman,1998), effective sample sizes, and trace plots. rhat &lt;- hgelman(fit, verbose = TRUE) hes &lt;- effectiveSize(hsam, hyper = TRUE) es1 &lt;- effectiveSize(hsam[[1]]) ## a.h1 b.h1 tau.h1 a.h2 b.h2 tau.h2 ## 427.9587 767.7925 483.1228 613.3212 730.1198 462.4242 ## a b tau ## 569.3685 609.0303 572.1573 p0 &lt;- plot(fit, hyper = TRUE) p1 &lt;- plot(fit, hyper = TRUE, pll = FALSE, den = TRUE) est1 &lt;- summary(fit, hyper = TRUE, recover = TRUE, start = 101, ps = pop.mean, type = 1, verbose = TRUE, digits = 3) est2 &lt;- summary(fit, hyper = TRUE, recover = TRUE, start = 101, ps = pop.scale, type = 2, verbose = TRUE, digits = 3) est3 &lt;- summary(fit, recover = TRUE, ps = ps, verbose = TRUE) ## a b tau ## True 242.700 6.189 0.030 ## 2.5% Estimate 241.456 6.143 0.008 ## 50% Estimate 242.275 6.173 0.460 ## 97.5% Estimate 243.081 6.206 1.338 ## Median-True -0.425 -0.016 0.430 ## ## a b tau ## True 0.005 3.879 0.040 ## 2.5% Estimate 0.005 3.533 0.042 ## 50% Estimate 0.005 3.835 0.047 ## 97.5% Estimate 0.005 4.156 0.058 ## Median-True 0.000 -0.044 0.007 ## ## Summary each participant separately ## a b tau ## Mean 242.28 6.17 3.82 ## True 242.29 6.17 3.81 ## Diff 0.01 0.00 -0.01 ## Sd 14.14 0.51 2.80 ## True 14.13 0.51 2.91 ## Diff 0.00 0.00 0.11 Model Fit to Rats Data After verifying that the model structure is OK, I then fit the Rats data. ## Each rat contribute 5 trials observations DT &lt;- data.table(d) DT[, .N, .(s)] ## s N ## 1: 1 5 ## 2: 2 5 ## 3: 3 5 ## ... ## 30: 30 5 Now, I bind the Rats data with the model and start sampling. The estimates are fairly similar with BUGS and Stan estimations. The only significant difference is the tau.h1, which simply due the more complex structure used here. dmi &lt;- BuildDMI(d, model) fit0 &lt;- run(StartNewhiersamples(500, dmi, start, prior)) fit &lt;- run(RestartHypersamples(5e2, fit0, thin = 32)) est1 &lt;- summary(fit, hyper = TRUE, type = 1, verbose = TRUE) round( est1$quantiles, 3) # 2.5% 25% 50% 75% 97.5% # a.h1 237.37 240.69 242.49 244.12 247.75 # mu_alpha 237.09 240.68 242.47 244.29 247.84 ## Stan # alpha.c 237.10 240.90 242.70 244.50 248.10 ## BUGS # b.h1 5.98 6.10 6.17 6.24 6.37 # mu_beta 5.97 6.11 6.18 6.26 6.40 ## Stan # beta.c 5.97 6.12 6.19 6.26 6.40 ## BUGS # tau.h1 0.012 0.035 0.042 0.047 0.056 # Stan 0.020 0.024 0.027 0.030 0.036 # BUGS 0.020 0.024 0.027 0.030 0.036 # a.h2 0.003 0.004 0.005 0.006 0.008 # alpha.tau 0.003 0.004 0.005 0.006 0.008 ## BUGS # b.h2 2.049 3.111 3.838 4.683 6.714 # beta.tau 1.952 3.078 3.879 4.922 8.026 ## BUGS hes &lt;- effectiveSize(fit, hyper = TRUE) ## a.h1 b.h1 tau.h1 a.h2 b.h2 tau.h2 ## 4500.000 4534.287 4193.927 3807.489 4079.768 3139.033 round(apply(data.frame(es), 1, mean)) round(apply(data.frame(es), 1, sd)) round(apply(data.frame(es), 1, max)) round(apply(data.frame(es), 1, min)) ## a b tau ## Mean 4464 4360 4321 ## SD 139 242 348 ## MAX 4717 5020 5137 ## MIN 4100 3732 3455 Handling Missing Data Rats example also considered fitting missing data. This can be done by setting the missing data as NA or downloading the missing data set directly from BUGS site. d[6:10,5] &lt;- NA d[11:20,4:5] &lt;- NA d[21:25,3:5] &lt;- NA d[26:30,2:5] &lt;- NA and bind the data with the same model set up previously. Again, the results are fairly similar with using other Bayesian software. dmi &lt;- BuildDMI(d[!is.na(d$RT),], model) # 2.5% 25% 50% 75% 97.5% # a.h1 241.01 244.35 246.07 247.73 251.09 # alpha.c 240.30 243.90 245.80 247.70 251.30 BUGS # b.h1 6.362 6.526 6.605 6.688 6.844 # beta.c 6.286 6.477 6.572 6.669 6.870 BUGS # a.h2 0.003 0.004 0.005 0.006 0.009 # alpha.tau 0.003 0.004 0.005 0.006 0.009 BUGS # b.h2 1.640 2.757 3.595 4.479 7.241 # beta.tau 1.505 2.676 3.620 5.044 13.601 BUGS The predictions for the final four observations on rat 26 can be obtained by entering predict_one function with fit[[26]]. pp26 &lt;- predict_one(fit[[26]]) pred26 &lt;- pp26[, .(Mean = mean(RT)), .(X)] pred26[c(2,1,3,4,5),] ## X Mean ## 1: -14 160.8060 ## 2: -7 203.8068 Y[26, 2] = 204.6 ## 3: 0 249.4786 Y[26, 3] = 250.2 ## 4: 7 297.8309 Y[26, 4] = 295.6 ## 5: 14 339.6564 Y[26, 5] = 341.2 Reference Gelfand, A. E., Hills, S. E., Racine-Poon, A., &amp; Smith, A. F. (1990). Illustration of Bayesian inference in normal data models using Gibbs sampling. Journal of the American Statistical Association, 85(412), 972-985."
					}

					
				
			
		
			
				
					,
					

					"bugs-seeds": {
						"id": "bugs-seeds",
						"title": "Random effect logistic regression",
						"category": "",
						"url": " /BUGS/seeds/",
						"content": "Disclaimer: This tutorial uses an experimental (beta) version of ggdmc, which has added the functionality of fitting logistic regression models. The software can be found in its GitHub. This document has yet completed. The aim of tutorial is to document one method to fit the logistic regression model, using the Seeds data. Seeds data were studied in Crowder (1978), re-analysed by Breslow and Clayton (1993) and used in the BUGS examples volumn I. This document expands the scope of ggdmc to the logistic regression model. An ordinary logistic model can fit either binary (response) data (i.e., 0, 1, 0, …) or binomial data (i.e., proportional data, as the Seeds example). The simplest form of the random-effect (multilevel) logistic model is to presume observation units are drawn from a normal distribution. This two-level model can be compared to the model presuming observation units are as they been observed (i.e., fixed-effect logistic regression model). Here I use the formulation of anti-logit, because firstly it is easier to interpret the probability of success (i.e., ) and secondly it is practically how computer codes been implemented. The idea of transforming binary or binomial responses with logit is still conceptually important for the generalized linear model though. Because the Seeds data set was formatted as List, I convert the data to data frame format. rm(list = ls()) library(data.table); library(boot) ## Load Seeds data ------------ ## 2 x 2 design setwd(\"~ BUGS_Examples vol1 Seeds \") dat &lt;- list(S = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10, 8, 10, 8, 23, 0, 3, 22, 15, 32, 3), N = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7), ## seed variety; 0 = aegytpiao 75 1 = aegyptiao 73 X1 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), ## root extract; 0 = bean; 1 = cucumber X2 = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1), Ns = 21) d &lt;- data.table(S = dat$S, N = dat$N, P = dat$S dat$N, X1= dat$X1, X2 = dat$X2) dplyr::tbl_df(d) d$s &lt;- factor(1:dat$Ns) ## A tibble: 21 x 7 ## S N P X1 X2 logit s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 10 39 0.256 0 0 -1.06 1 ## 2 23 62 0.371 0 0 -0.528 2 ## 3 23 81 0.284 0 0 -0.925 3 ## 4 26 51 0.510 0 0 0.0392 4 ## 5 17 39 0.436 0 0 -0.258 5 ## 6 5 6 0.833 0 1 1.61 6 ## 7 53 74 0.716 0 1 0.926 7 ## 8 55 72 0.764 0 1 1.17 8 ## 9 32 51 0.627 0 1 0.521 9 ## 10 46 79 0.582 0 1 0.332 10 ## ... with 11 more rows S, the number of (successfully) germinated seeds on the ith plate (i = 1, … N); N, the number of total seeds on the ith plate; P, the proportion of germinated seeds; X1, a two-level seed factor, aegyptiao 75 vs. aegyptiao 73; X2, a two-level root extract factor, bean vs. cucumber; logit, as the column name says; s, subject, namely, the observation unit. The interaction plot shows that the root extract type, cucumber, has a drastic increase in successful germination when the seed type is aegyptiao 75, comparing to when the seed type is aegyptaio 73 and this change is small and in an opposite direction in the root extract type, bean. The data can be analysed with the ordinary logistic regression (OLR) model or multilevel logistic regression model. The OLR replicates the result in Table 3 (1st column) in Breslow and Clayton (1993). I use the display function in the arm package, which shows summary result concisely (AIC is calculated separately). m1 &lt;- glm(cbind(S, N-S) ~ X1 + X2, family = binomial, data = d) arm::display(m1) ## Breslow's result in Table 3 p15 ## glm(formula = cbind(S, N - S) ~ X1 + X2, family = binomial, data = d) ## coef.est coef.se ## (Intercept) -0.43 0.11 ## X1 -0.27 0.15 ## X2 1.06 0.14 ## --- ## n = 21, k = 3 ## residual deviance = 39.7, null deviance = 98.7 (difference = 59.0) ## AIC: 122.28 m2 &lt;- glm(cbind(S, N-S) ~ X1*X2, family = binomial, data = d) arm::display(m2) ## glm(formula = cbind(S, N - S) ~ X1 * X2, family = binomial, data = d) ## coef.est coef.se ## (Intercept) -0.56 0.13 ## X1 0.15 0.22 ## X2 1.32 0.18 ## X1:X2 -0.78 0.31 ## --- ## n = 21, k = 4 ## residual deviance = 33.3, null deviance = 98.7 (difference = 65.4) ## AIC: 117.87 require(lme4) m3 &lt;- glmer(cbind(S, N - S) ~ X1 * X2 + (1 | s), family = binomial(link=\"logit\"), data = d) arm::display(m3) ## glmer(formula = cbind(S, N - S) ~ X1 * X2 + (1 | s), data = d, ## family = binomial(link = \"logit\")) ## coef.est coef.se ## (Intercept) -0.55 0.17 ## X1 0.10 0.28 ## X2 1.34 0.24 ## X1:X2 -0.81 0.38 ## ## Error terms: ## Groups Name Std.Dev. ## s (Intercept) 0.23 ## Residual 1.00 ## --- ## number of obs: 21, groups: s, 21 ## AIC = 117.5, DIC = -74.6 ## deviance = 16.5   glm   glmer   BUGS   ggdmc     se se se se -0.558 0.126 -0.548 0.166 -0.557 0.197     0.146 0.223 0.097 0.277 0.086 0.317     1.318 0.177 1.337 0.236 1.348 0.276     -0.778 0.306 -0.810 0.384 -0.824 0.445     — — 0.235 — 0.286 0.146     Reference Breslow, N. E., &amp; Clayton, D. G. (1993). Approximate inference in generalized linear mixed models. Journal of the American statistical Association, 88(421), 9-25. Crowder, M. J. (1978). Beta-binomial anova for proportions. Applied statistics, 34-37."
					}

					
				
			
		
			
				
					,
					

					"approximation-kde": {
						"id": "approximation-kde",
						"title": "Kernel Density Estimation",
						"category": "",
						"url": " /approximation/kde/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"approximation-pda": {
						"id": "approximation-pda",
						"title": "Probability Density Approximation",
						"category": "",
						"url": " /approximation/pda/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"approximation-ppda": {
						"id": "approximation-ppda",
						"title": "Parallel Probability Density Approximation",
						"category": "",
						"url": " /approximation/ppda/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"basics-descriptive": {
						"id": "basics-descriptive",
						"title": "Descriptive Statistics",
						"category": "",
						"url": " /basics/descriptive/",
						"content": "In most RT modelling work, researchers usually want to examine the manifested statistics. Often, these are the average response times (RTs) and accuracy rates. In the following, I used the LNR model LNR (Heathcote &amp; Love, 2012), as an example to illustrate a method to calculate these statistics efficiently. The user wishes to understand and apply LNR model on her his work can find useful information in the DMC tutorials (Heathcote et al., 2018). This particular LNR model presumes one stimulus (S) factor, and similar to the LBA model, it has a latent matching (M) factor. library(data.table); library(ggdmc) model &lt;- BuildModel( p.map = list(meanlog = \"M\", sdlog = \"M\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(left = \"LEFT\", right = \"RIGHT\")), factors = list(S = c(\"left\", \"right\")), responses = c(\"LEFT\", \"RIGHT\"), constants = c(st0 = 0), type = \"lnr\") The arbitrary chosen true parameters generate a reasonable RT distribution, which similar with typical choice RT data, giving approximately 25% errors (I will show you this later). p.vector &lt;- c(meanlog.true = -1, meanlog.false = 0, sdlog.true = 1, sdlog.false = 1, t0 = .2) simulate function takes the first option, model to generate data based on the provided model. ps option expects a true parameter vector that matches the setting in the model object, nsim option expects the number of trial per condition. dat &lt;- simulate(model, ps = p.vector, nsim = 1024) d &lt;- data.table(dat) ## S R RT ## 1: left LEFT 0.3821405 ## 2: left LEFT 0.7859101 ## 3: left LEFT 0.5237262 ## 4: left RIGHT 0.3932804 ## 5: left LEFT 0.6604592 ## --- ## 2044: right RIGHT 0.7342084 ## 2045: right RIGHT 1.3628130 ## 2046: right RIGHT 0.3343844 ## 2047: right RIGHT 0.4913930 ## 2048: right RIGHT 0.6119065 S is the stimulus factor R is the response type RT stores response time in second By using data.table function .N, I confirmed that each condition does has 1024 trials. d[, .N, .(S)] ## S N ## 1: left 1024 ## 2: right 1024 A similar syntax, with S and R factors, I printed out the information regarding the hit, correct rejection, false alarm and miss responses. d[, .N, .(S, R)] ## S R N ## assuming the left is signal and right is noise ## 1: left LEFT 791 ## hit ## 2: left RIGHT 233 ## miss ## 3: right RIGHT 786 ## correct rejection ## 4: right LEFT 238 ## false alarm I used a ifelse chain to calculate a C column to indicate correct (TRUE) and error (FALSE) responses. In real world data, there would be some responses missing or participants pressing wrong keys, so the last else is “NA” to catch these situations. d$C &lt;- ifelse(d$S == \"left\" &amp; d$R == \"LEFT\", TRUE, ifelse(d$S == \"right\" &amp; d$R == \"RIGHT\", TRUE, ifelse(d$S == \"left\" &amp; d$R == \"RIGHT\", FALSE, ifelse(d$S == \"right\" &amp; d$R == \"LEFT\", FALSE, NA)))) The data table now looks like below. ## S R RT C ## 1: left LEFT 0.3821405 TRUE ## 2: left LEFT 0.7859101 TRUE ## 3: left LEFT 0.5237262 TRUE ## 4: left RIGHT 0.3932804 FALSE ## 5: left LEFT 0.6604592 TRUE ## --- ## 2044: right RIGHT 0.7342084 TRUE ## 2045: right RIGHT 1.3628130 TRUE ## 2046: right RIGHT 0.3343844 TRUE ## 2047: right RIGHT 0.4913930 TRUE ## 2048: right RIGHT 0.6119065 TRUE This is one way to calculate average RTs with data.table. d[, .(MRT = round(mean(RT), 2)), .(C)] ## C MRT ## 1: TRUE 0.61 ## 2: FALSE 0.71 The syntax to calculate the response proportions, namely correct and error rates, are less straightforward, but possible. Firstly, I calculated the counts for hit, correct rejection, miss, and false alarm and store them in prop. Then I made up a new column, called NN, to store the total number of trial. Lastly, I divided the four conditions by the total number of trial. I also used a round to print only to the two decimal place below zero. These are almost 25% error rates, as promised. prop &lt;- d[, .N, .(S, R)] prop[, NN := sum(N), .(S)] prop[, acc := round(N NN, 2)] prop ## S R N NN acc ## 1: left LEFT 791 1024 0.77 ## 2: left RIGHT 233 1024 0.23 ## 3: right RIGHT 786 1024 0.77 ## 4: right LEFT 238 1024 0.23 Real-world Example In this section, I will demonstrate more data processing techniques, using the empirical data (Holmes, Trueblood &amp; Heathcote (2016). This data set can be downloaded from my OSF site. One raw data format often found is one subject per text or csv file (*.txt or *.csv). For example, the file, “S125.2014-04-23_6-22-36.txt”, stores the data from participant, S125. There are 47 of them. All are in the same format. In another tutorial, I will illustrate how to handle similar but not identical formatted data files. block trial target CO1 CO2 ST resp RT correct 1 1 R 50 -1 -1 1 1076 1 1 2 L 50 -1 -1 0 733 1 1 3 R 50 -1 -1 1 637 1 1 4 R 50 -1 -1 1 517 1 ... How to read large data sets efficiently I stored data files in a standard location of usual R packaging. The folder, named data unsurprisingly, immediately in a project folder. And the analysis scripts are stored in a folder, called R. I then used list.files function to store all file names in a object, called fn. ?list.files dp &lt;- \"data Holmes_etal_CogPsych_2016_Data\"; ## data path fn &lt;- list.files(dp, pattern = \"*.txt\") ## file name print(fn) ## [1] \"S125.2014-04-23_6-22-36.txt\" ## [2] \"S126.2014-04-23_6-25-38.txt\" ## [3] \"S127.2014-04-23_6-26-46.txt\" ## ... ## [45] \"S169.2014-05-07_6-16-49.txt\" ## [46] \"S170.2014-05-07_6-18-16.txt\" ## [47] \"S171.2014-05-07_6-38-56.txt\" Next I created a DTLapply function to pipe the text files one after another to the fread of data.table to quickly process them. function(fn, dp) { v &lt;- lapply(seq_along(fn), function(i) { s &lt;- strsplit(fn[i], split = \"[.]\")[[1]][1] d &lt;- data.table::fread(file.path(dp, fn[i])) S &lt;- d$target R &lt;- d$resp RTSec &lt;- d$RT 1e3 C &lt;- d$correct return(d[, c(\"s\", \"S\", \"R\", \"RT\", \"C\") := list(s, S, R, RTSec, C)]) }) return(data.table::rbindlist(v)) } x0 &lt;- DTLapply(fn, dp) seq_along function will convert fn, which store 47 file name strings to numerical indices, for the looping. seq_along(fn) [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 function(i) inside the lapply function is an anonymous function, namely a function used internally by the lappy function. Inside this anonymous function is basically a quicker R for loop. The first line in the anonymous function is to extract the label for a participant. For example, if I just processed the first text file, the strsplit function splits the string whenever it finds a dot symbol. Because strsplit returns an R list, I took the first element in the first list. print(fn[1]) ## [1] \"S125.2014-04-23_6-22-36.txt\" strsplit(fn[1], split = \"[.]\") ## [[1]] ## [1] \"S125\" \"2014-04-23_6-22-36\" \"txt\" s &lt;- strsplit(fn[i], split = \"[.]\")[[1]][1] print(s) ## [1] \"S125\" Next line uses the convenient function, file.path in R base to construct a file path to a particular file. For example, if I extract the first participant. dp &lt;- \"data Holmes_etal_CogPsych_2016_Data\"; ## data path file.path(dp) ## [1] \"data Holmes_etal_CogPsych_2016_Data\" file.path(dp, fn[1]) ## [1] \"data Holmes_etal_CogPsych_2016_Data S125.2014-04-23_6-22-36.txt\" The function, file.path returns the complete relative file path to the data file, which is then read by the fread function. Note I can easily use the relative path method, because the folder structure follows strictly R packaging structure. d &lt;- data.table::fread(file.path(dp, fn[1])) ## block trial target CO1 CO2 ST resp RT correct ## 1: 1 1 R 50 -1 -1 1 1076 1 ## 2: 1 2 L 50 -1 -1 0 733 1 ## 3: 1 3 R 50 -1 -1 1 637 1 ## 4: 1 4 R 50 -1 -1 1 517 1 ## 5: 1 5 L 50 -1 -1 0 476 1 ## --- ## 1276: 20 68 L 15 -1 -1 -1 -1 1 ## 1277: 20 69 L 15 -1 -1 0 1077 0 ## 1278: 20 70 LR 15 15 529 0 1463 1 ## 1279: 20 71 L 15 -1 -1 0 1895 0 ## 1280: 20 72 LR 15 15 529 0 1311 1 The following four lines were simply to temporarily save the columns, “target”, “resp”, “RT” (converted to second), and “correct” to four different R vectors, “S”, “R”, “RTSec”, and “C”. These operations just converted the original column naming to my factor naming convention. For example, target column indicates whether a stimulus was one of the four levels: right (stationary trial), left (stationary trial), left and right (switching trial) or right and left moving dot (switching trial), so I converted it to as stimulus, S, factor. Similarly, R factor is from the resp response column, RT column was converted to second, and correct column was converted to C. S &lt;- d$target R &lt;- d$resp RTSec &lt;- d$RT 1e3 C &lt;- d$correct These four R vectors were then grouped together as a R list. list(s, S, R, RTSec, C) ## [[1]] ## [1] \"S125\" ## [[2]] ## [1] \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"L\" \"L\" \"R\" \"L\" \"L\" ## [15] \"L\" \"R\" \"L\" \"R\" \"R\" \"L\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"L\" \"R\" ## [29] \"L\" \"R\" \"L\" \"R\" \"L\" \"R\" \"L\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" ## ... ## [[3]] ## [1] 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 ## [25] 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 ## [49] 1 0 -1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 ## ... ## [[4]] ## [1] 1.076 0.733 0.637 0.517 0.476 0.419 0.493 0.486 0.685 0.581 ## [11] 0.460 0.462 0.666 0.557 0.446 0.438 0.549 0.358 0.486 0.516 ## [21] 0.484 0.589 0.679 0.743 0.550 0.646 0.516 0.486 0.588 0.463 ## [[5]] ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [38] 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 ## [75] 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 ## ... This list was directly inserted into the data.table, d, which created five new columns on top the original ones (i.e., block, trial, etc.). d[, c(\"s\", \"S\", \"R\", \"RT\", \"C\") := list(s, S, R, RTSec, C)] The result was then returned to the lapply function as its output. := is just the assignment symbol in data.table syntax. return(d[, c(\"s\", \"S\", \"R\", \"RT\", \"C\") := list(s, S, R, RTSec, C)]) Each participant file was looped through in a fastest possible way in R language and finally, each of them was glued together via the data.table function, rbindlist, which return as the output for my homemade DTLapply function. ## # A tibble: 60,160 x 13 ## block trial target CO1 CO2 ST resp RT correct s S R ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 1 R 50 -1 -1 1 1.08 1 S125 R 1 ## 2 1 2 L 50 -1 -1 0 0.733 1 S125 L 0 ## 3 1 3 R 50 -1 -1 1 0.637 1 S125 R 1 ## 4 1 4 R 50 -1 -1 1 0.517 1 S125 R 1 ## 5 1 5 L 50 -1 -1 0 0.476 1 S125 L 0 ## 6 1 6 R 50 -1 -1 1 0.419 1 S125 R 1 ## 7 1 7 R 50 -1 -1 1 0.493 1 S125 R 1 ## 8 1 8 R 50 -1 -1 1 0.486 1 S125 R 1 ## 9 1 9 L 50 -1 -1 0 0.685 1 S125 L 0 ## 10 1 10 L 50 -1 -1 0 0.581 1 S125 L 0 ## # ... with 60,150 more rows, and 1 more variable: C &lt;int&gt; How to trim off irregular participants It is not uncommon in a data set to have few participants who do not engage in performing a task or drop out in the middle of an experiment. With convincing evidence, we can exclude these participants. Here I demonstrated one way to conduct this operation. I used the match function, which has a symbol form, %in%. This is an R internal function, which uses efficient algorithm. ## Excluding 3 + 13 participants from model fitting, due to ## (1) a computer error and, ## (2) less than 70% accuracy on the stationary trials in 4-18 blocks. ## They are 126, 129, 130, 133, 134, 138, 139, 140, 143, 147, 148, 150, 155, ## 156, 161, 162 badsubjs &lt;- c(\"S126\", \"S129\", \"S130\", \"S133\", \"S134\", \"S138\", \"S139\", \"S140\", \"S143\", \"S147\", \"S148\", \"S150\", \"S155\", \"S156\", \"S161\", \"S162\") x1 &lt;- x0[ !(s %in% badsubjs) ] Reference Heathcote A., and Love J. (2012) Linear deterministic accumulator models of simple choice. frontiers in Psychology, 23. https: doi.org 10.3389 fpsyg.2012.00292. Holmes, W.R. et al, A new framework for modeling decisions about changing information: The Piecewise Linear Ballistic Accumulator model”, 2015, Cognitive Psychology 85, 1-29."
					}

					
				
			
		
			
				
					,
					

					"basics-leastsq": {
						"id": "basics-leastsq",
						"title": "Least Square Method",
						"category": "",
						"url": " /basics/leastsq/",
						"content": "This is a short note for doing least square minimization to fit a diffusion process model. The aim of the least square minimization (LSM) is to minimize a cost function, which returns the difference between the data and model predictions. One possible reason to fit data to a process model, instead of a standard model [e.g., ex-Wald, (Schwarz, 2001; Schwarz, 2002; Heathcote, 2004)] is to retain some flexibilities for later tweaking the process. A standard model usually has been thoroughtly studied, and thereby provides analytic likelihood functions. For example, one can find the analytic likelihood function of the drift-diffusion model in van Zandt (2000). (See, also e.g., equation (5) in Bogacz et al., 2006 for the standard stochastic process equation of the DDM). The method of least square minimization is useful when one wants to test a number of differernt variants that deviate from the standard stochastic process. For instance, one might hypothesize that people deploy different drift rates to different regions in the visual field because people perhaps pay more attention to a more interesting centre region of a stimulus, than other regions. One could assign a larger mean drift rate to the centre region, comparing with assigning smaller mean drift rates to the others. Another possible hypothetical variant is that one might assume within a trial, people change their drift rate significantly (not just small variation due to within-trial variability). This could be tested by tweaking the standrd diffusion process, such as, constructing a series of different Gaussian models, each of which sample different drift rates at different time point in a process. Nevertheless, one must note that the more variants one introduces to a process model that deviates from the standard process, the more likely that the altered process becomes difficult to fit (as well as prone to overfit the data). Only a handful of process models, for example the full drift-diffusion model (DDM; see e.g., the Appendix in Van Zandt, 2000 for its PDF and CDF), have derived their analytic likelihood functions. After tweaking a standard process, one must also derive a new probability density function (PDF; sometimes as well as CDF) based on the altered new process. (See the video provided by StatQuesta for an excellent explantion regarding the probability and the likelihood at https: www.youtube.com watch?v=pYxNSUDSFH4).). The process of deriving a new likelihood function could be sometimes cumbersome, if not very difficult or impossible. This of course assumes that one wants to apply the model fitting methods using likelihood functions. One advantage of deriving the analytic solution for a process model is that one can use the powerful maximum likelihood method to conduct model fitting. On the other hand, the LSM, often used in machine learning, is one method for model fitting, without using the likelihood function. The following code snippet is an R programme for a 1-D diffusion (process) model. The code snippet is to demonstrate how the process model is constructed. The real working programme is written in C++. I assumed a within-trial constant (mean) drift rate as in a typical case of diffusion process. See code comments to get further details. r1d_R &lt;- function(pvec, tmax, h) { ## p.vector &lt;- c(v=0, a=1, z=.5, t0=0, s=1) Tvec &lt;- seq(0, tmax, h) nmax &lt;- length(Tvec) ## Unit travelling distance; ie how far a particle travels per unit time ## 1. Here I used h (presumably to 1 ms) as the unit time ## 2. In the constant-drift model, the mean drift rate does not change ## within a process, although it is subjected to the influence of ## within-trial standard deviation (variability). That is, ## the \"constant\" refers to the mean drift rate is constant. ## 3. travel distance = drift rate * unit time ## the following one vector ( ie rep(1, nmax) ) is to make \"mut\" a ## vector. This pre-calculation helps to reduce computation time. mut &lt;- h * pvec[1] * rep(1, nmax) ## - Within-trial standard deviation, ## - The fifth element of pvec is the within-trial standard deviation sigma_wt &lt;- sqrt(h) * pvec[5] * rnorm(nmax) ## - Xt stores evidence values; ## - To plot the trace of a particle, one can return Xt. ## - Xt is the assumed input (ie 'features' in ML terminology). ## - In EAM, we assume Xt is due to neuronal responses to sensory stimuli. ## Thus, Xt is often called sensory evidence Xt &lt;- rep(NA, nmax) ## - The first value for the evidnece vector is the assumed starting point ## - pvec[3] is the absolute value of the starting point. ## - If one wants to limit the symmetric process, uncomment the following ## line ## Xt[1] &lt;- pvec[3] * pvec[2] ## assume pvec[3] is zr and convert it to z. current_evidence &lt;- Xt[1]; ## transient storage for the evidence ## Start the evidence accumulation process ## Note 1. We did not know when the process would stop beforehand ## Note 2. We assumed the studied process cannot exceed nmax * h seconds ## Note 3. Only when the latest evidence value exceeds the threshold value, ## a or 0, the process stops. i &lt;- 2 ## starting from i == 2 ## - pvec[2] is the upper boundary; ## - 0 is the lower boundary. while (current_evidence &lt; pvec[2] &amp;&amp; current_evidence &gt; 0 &amp;&amp; i &lt; nmax) { ## This is the typical diffusion process equation ## the updated evidence value = the latest evidence value + ## (drift rate * unit time) + within-trial standard deviation Xt[i] &lt;- Xt[i-1] + mut[i] + sigma_wt[i] current_evidence &lt;- Xt[i]; ## Store the updatd evidence value i &lt;- i + 1; ## increment step } RT &lt;- i * h + pvec[4]; ## decision time + t0 = response time is_broken &lt;- i == nmax; ## whether the simulation suppasses the assumed max time ## hit upper bound (1) or lower bound (0) R &lt;- ifelse(current_evidence &gt; pvec[2], 1, 0) ## Using list to return extra information. ## We do not usually return Xt, Tvec and is_broken return(list(Xt = Xt, Tvec= Tvec, RT=RT, R = R, is_broken=is_broken)) } Simulate a Diffusion Process To inspect an instance of a diffusion process, I designated a parameter vector and considered it as a “true” parameter vector. This is just to conduct the simulation. In a regular model fitting, one would not know the true values of the parameters. One aim of fitting a model to data is to find a set optimal parameters that accounts the data. I assumed a two-second time span for the diffusion process and used a 1-ms time step. tmax &lt;- 2 h &lt;- 1e-3 p.vector &lt;- c(v=0, a=1, z=.5, t0=0, s=1) res1 &lt;- r1d_R(pvec=p.vector, tmax=tmax, h=h) ## To locate the first instance of NA idx &lt;- sum(!is.na(res1$Xt)); plot(res1$Tvec[1:idx], res1$Xt[1:idx], type='l', ylim=c(0, 1), xlab='DT (s)', ylab='Evidence') abline(h=p.vector[2], lty='dotted', lwd=1.5) abline(h=0, lty='dotted', lwd=1.5) points(x=0, y=p.vector[3], col='red', cex =2) Usually, the instance represents or, said simulates, an unobservable cognitve process that happens when one responds to a trial. For example, in a driving simulator study for an automatic vehicle, in a trial, a participant may sit insider the simulator and engage in some tasks. When, for example, the simulated fog is unveiled, the participant suddenly is able to see the front view and perhaps notice another vehcile is in the front. At this moment, the participant was instructed to make a judgement to decide whether to take over control of the vehicle and disengage the automatic driving system. One might assume the stimulus composes of the front vehicle, its surrondings, as well as the participant’s own kinetmatic sense of her vehicle (speed, accelaration etc), her psychological assessment of the distance between her AV and the vehicle in the front. The stimulus then presumably elicits usually unobservable “sensory evidence” in the particpant’s mind. The “sensory evidence” is the input (i.e., “Evidence” in the previous figure). In a 2AFC diffusion model, the outputs usually are a pair of numbers. The most well-known is the response time (RT) and the other is response choice. The latter can be represented as 0 and 1 in a binary-choice task. The outputs are usually more easily to observe in a typical, standard psychological task. The input, however, is not. Responses, Choices, and Accuracy In a typical psychological task, participants respond usually by entering their response via pressing some keys on a computer keyboard. For example, pressing “z” for option 1, and “ ” for option 2. This action is recorded, in every trial. Researchers can then later infer which option participants have chosen in every single trial. Note that participants may commit to choose option 1, (optio 2); however, a stimulus could belong to option 2. This is an outcome of mismatch. This brings us to the idea of matching responses to stimuli. In other words, a response, in a binary task, could result in two different outcomes, correct or incorrect. For example in a two-choice lexical decision task, one would respond “word” or “non-word” to a stimuls, which could be real word (W) or a pesudo-word (NW). Only after a response is committed has the outcome become apparent. Table 1. A binary-choice stimulus-response table.   W NW word O X non-wrod X O Objective Function In the following, I showed a simple method to fit a two-choice diffusion model, using the LSM. First, I set up an objective function. The aim of designing the objective function is to get the difference of the predictions and the data. As typically been done in the literature applying diffusion models, I compared the five percentils, .1, .3, .5, .7 and .9. The following code snippet showed this calculation. sq_diff &lt;- c( (pred_q0 - data_q0)^2, (pred_q1 - data_q1)^2) To make the demonstration simple, I aimed only to recover the drift rate and fixed the other parameters (a, zr, t0, and s). I wrote another stand-alone C++ function, which simply used a for-loop wrap around the above r1d function and added a few checks on the data quality. I named this function, “rdiffusion”. Next, the objective function took the drift rate parameter from the optimization routine and put it at the first position of the “pvec” object. I fixed the second to fifth parameters by manually entering their values. The objective function then simulated “nsim” number of diffusion processes. I passed 10,000 to the nsim object. nsim &lt;- 1e4 Then, I removed the problematic trials, storing their indices into the “bad” object. I designated NA to those process suppassing the assumed upper time limit (i.e., tmax). I also designated 0 and 1, respectively, to the procoesses that result in hitting lower and upper boundaries. Thus, the line with, “pred_R == 1”, was to extract the indices for the simulated trials hitting the upper boundary. The line, “upper_count &lt;- sum(upper)” was to count how many simulated trials result in choice 1 (i.e., htting upper boundary). This was to gauge the wild parameter values at the early stage of optimzation. The optimization routine may cast some drift rate values, resulting in the process that produces no responses (i.e., outside the parameter space, under the assumptions). bad &lt;- (is.na(tmp[,2])) || (tmp[,2] == 2) sim &lt;- tmp[!bad, ] pred_RT &lt;- sim[,1] pred_R &lt;- sim[,2] upper &lt;- pred_R == 1 lower &lt;- pred_R == 0 upper_count &lt;- sum(upper) lower_count &lt;- sum(lower) Next, the objective function returns a very large number, 1e9, if the parameters resulting in abnormal diffusion processes. Otherwise, I separated the RTs for the choice 1 and choice 2, respectively, for the data and for the predictions and then compared their five percentiles. Finally, the sum of the differences was sent back to the optimization routine. data_RT &lt;- data[,1] data_R &lt;- data[,2] d_upper &lt;- data[,2] == 1 d_lower &lt;- data[,2] == 0 RT_c0 &lt;- data_RT[d_upper] RT_c1 &lt;- data_RT[d_lower] pred_c0 &lt;- pred_RT[upper] pred_c1 &lt;- pred_RT[lower] pred_q0 &lt;- quantile(pred_c0, probs = seq(.1, .9, .2)) pred_q1 &lt;- quantile(pred_c1, probs = seq(.1, .9, .2)) data_q0 &lt;- quantile(RT_c0, probs = seq(.1, .9, .2)) data_q1 &lt;- quantile(RT_c1, probs = seq(.1, .9, .2)) sq_diff &lt;- c( (pred_q0 - data_q0)^2, (pred_q1 - data_q1)^2) out &lt;- sum(sq_diff) The complete objective function is listed in the following code snippet. objective_fun &lt;- function(par, data, tmax, h, nsim) { pvec &lt;- c(par[1], 1, .5, 0, 1) tmp &lt;- rdiffusion(nsim, pvec, tmax, h) bad &lt;- (is.na(tmp[,2])) || (tmp[,2] == 2) sim &lt;- tmp[!bad, ] pred_RT &lt;- sim[,1] pred_R &lt;- sim[,2] upper &lt;- pred_R == 1 lower &lt;- pred_R == 0 upper_count &lt;- sum(upper) lower_count &lt;- sum(lower) if (any(is.na(pred_R))) { ## return a very big value, so the algorithm throws out ## this parameter out &lt;- 1e9 } else if ( is.na(sum(upper_count)) || is.na(sum(lower_count)) ) { ## return a very big value, so the algorithm throws out ## this parameter out &lt;- 1e9 } else { data_RT &lt;- data[,1] data_R &lt;- data[,2] d_upper &lt;- data[,2] == 1 d_lower &lt;- data[,2] == 0 RT_c0 &lt;- data_RT[d_upper] RT_c1 &lt;- data_RT[d_lower] pred_c0 &lt;- pred_RT[upper] pred_c1 &lt;- pred_RT[lower] pred_q0 &lt;- quantile(pred_c0, probs = seq(.1, .9, .2)) pred_q1 &lt;- quantile(pred_c1, probs = seq(.1, .9, .2)) data_q0 &lt;- quantile(RT_c0, probs = seq(.1, .9, .2)) data_q1 &lt;- quantile(RT_c1, probs = seq(.1, .9, .2)) sq_diff &lt;- c( (pred_q0 - data_q0)^2, (pred_q1 - data_q1)^2) out &lt;- sum(sq_diff) } return(out) } I used the optimize routine to search the parameters. To make the estimation simple, I limited the range of estimation to 0 to 5. The optimize is for searching one dimension space. See ?optimize for further details. For higher dimension, one must use other optimization routines. fit &lt;- optimize(f=objective_fun, interval=c(0, 5), data = dat, tmax=tmax, h=h, nsim=nsim) To estimate the variability, I used a simple resampling method, via the parallel routine, mclapply, to conduct 100 parameter-recovery studies. doit &lt;- function(p.vector, n, tmax, h, nsim) { dat &lt;- rdiffusion(n, p.vector, tmax, h) fit &lt;- optimize(f=objective_fun, interval=c(0, 5), data = dat, tmax=tmax, h=h, nsim=nsim) return(fit) } The following is the code snippet for launching the parameter-recovery studies.. ## Assume the \"real\" process has the parameters, p.vector ## The aim is to recover the drift rate , 1.51 p.vector &lt;- c(v=1.51, a=1, zr=.5, t0=0, s=1) tmax &lt;- 2 h &lt;- 1e-3 ncore &lt;- 3 ## Assume we have collected \"real\" empirical data, which has 5,000 trials n &lt;- 5e3 ## I requested the objective function to simulate 10,000 trials to ## construct the simulated historgram, every time the optimization routine ## make a guess about what the drift rate could be. nsim &lt;- 1e4 ## Each parallel thread run an independent parameter-recovery study. ## Here I ran 100 separate parameter-recovery studies. ## About 5.24 mins on a very good CPU parameters &lt;- parallel::mclapply(1:100, function(i) try(doit(p.vector, n, tmax, h, nsim), TRUE), mc.cores = getOption(\"mc.cores\", ncore)) Results The figure showed most estimated drift rates are around the true value (red line), with a roughly normally distributed shape. Recovering the Boundary Separation To recover the parameter of boundary separation, I added the error rate statistics in the cost function. Moverover, I add a normalization factor, becaues the error rate and the RT percentiles are on different scales. mymean &lt;- function(x=NULL, nozero=0) { ## A function from Bogacz and Cohen (2004) if(is.null(x)) { out &lt;- 1 } else { out &lt;- ifelse(mean(x)==0 &amp;&amp; nozero, 1, mean(x)) } return(out) } doit &lt;- function(p.vector, n, tmax, h, nsim) { tmp &lt;- rdiffusion(n, p.vector, tmax, h) tmp &lt;- tmp[!is.na(tmp[,2]), ] tmp &lt;- tmp[!is.na(tmp[,1]), ] over_tmax &lt;- tmp[,2] == 2 dat &lt;- tmp[!over_tmax, ] fit &lt;- subplex(par = runif(1), fn = objective_fun, hessian = FALSE, data = dat, tmax=tmax, h=h, nsim=nsim) return(fit) } objective_fun &lt;- function(par, data, tmax, h, nsim) { pvec &lt;- c(2.35, par, .75, 0, 1) tmp &lt;- rdiffusion(nsim, pvec, tmax, h) tmp &lt;- tmp[!is.na(tmp[,2]), ] tmp &lt;- tmp[!is.na(tmp[,1]), ] over_tmax &lt;- tmp[,2] == 2 sim &lt;- tmp[!over_tmax, ] pred_RT &lt;- sim[,1] pred_R &lt;- sim[,2] upper &lt;- pred_R == 1 lower &lt;- pred_R == 0 upper_count &lt;- sum(upper) lower_count &lt;- sum(lower) if ( (any(is.na(pred_R))) || is.na(sum(upper_count)) || is.na(sum(lower_count)) || (length(pred_RT) == 0) ) { error &lt;- 1e9 } else { data_RT &lt;- data[,1] data_R &lt;- data[,2] d_upper &lt;- data[,2] == 1 d_lower &lt;- data[,2] == 0 RT_c0 &lt;- data_RT[d_upper] RT_c1 &lt;- data_RT[d_lower] data_er &lt;- 1 - mean(data_R) pred_er &lt;- 1 - mean(pred_R) pred_c0 &lt;- pred_RT[upper] pred_c1 &lt;- pred_RT[lower] pred_q0 &lt;- quantile(pred_c0, probs = seq(.1, .9, .2)) pred_q1 &lt;- quantile(pred_c1, probs = seq(.1, .9, .2)) data_q0 &lt;- quantile(RT_c0, probs = seq(.1, .9, .2)) data_q1 &lt;- quantile(RT_c1, probs = seq(.1, .9, .2)) error &lt;- sum( (data_q0 - pred_q0)^2 mymean(data_q0, 1)^2 ) + sum( (data_q1 - pred_q1)^2 mymean(data_q1, 1)^2 ) + sum( (data_er - pred_er)^2 mymean(data_er)^2 ) } return(error) } This time I used a different optimization routine, subplex (Rowan, 1990). tmax &lt;- 2 h &lt;- 1e-3 n &lt;- 1e3 ## Assumed number of data points p.vector &lt;- c(v=2.35, a=1.8, z=.75, t0=0, s=1) ## True values nsim &lt;- 1e4 ## number of model simulation ncore &lt;- 3 ## 3176.639 s parameters &lt;- parallel::mclapply(1:100, function(i) try(doit(p.vector, n, tmax, h, nsim), TRUE), mc.cores = getOption(\"mc.cores\", ncore)) save(parameters, p.vector, file = \"data fit_one_a.RData\") Results The figure showed estimates distributed around the true value, 1.8 (red line). Recovering More Than One Parameter Searching high-dimensional space might take substantial amount of time, but the computer codes essentially do not change too much. A parameter-recovery study to recover both the drift rate and the boundary separation took about 93 minutes, using a high-end CPU (2020) Next To fit empirical data, one must adjust the difference in time units in the model and experiment. Smart readers would have noticed that the above model assumed a time unit, h, which is part of the model assumption. In order to fit empirical data, one must check whether this assumption is plausible and perhaps adjust accordingly."
					}

					
				
			
		
			
				
					,
					

					"basics-mle": {
						"id": "basics-mle",
						"title": "Maximising Likelihoods",
						"category": "",
						"url": " /basics/mle/",
						"content": "This is a short note for one method to conduct maximum likelihood estimation (MLE) to fit the LBA model. In essence, the MLE is not a very difficult statistical technique, but there are some trivialities regarding to the cognitive model and its influence on the usage of optimiser that must be addressed. Otherwise, you would not recover the parameters in the LBA model (or cognitive models in general). In short, you must adjust either the objective function, the method of data preparation, or the method of proposing parameters according to a specific cognitive model. For example, the non-decision time must not go below 0 second. If you do not (or cannot) add this constraint on the optimiser (e.g., the R function, optim), the resulting fit may not converge or the estimates (even converged) will be unreasonable, psychologically speaking. Below I use ggdmc to conduct a simulation study to demonstrate the point. Simulation study Firstly, I use the BuildModel function to set up a null model with only a stimulus factor (denoted S). That is, the model parameters do not associate with any factors. Next I arbitrarily set up a true parameter vector, p.vector and request 100 trials per condition. My aim is to recover the true parameters. require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"M\", sd_v = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2)), factors = list(S = c(\"s1\", \"s2\")), constants = c(st0 = 0, sd_v = 1), responses = c(\"r1\", \"r2\"), type = \"norm\") p.vector &lt;- c(A = .75, B = 1.25, t0 = .15, mean_v.true = 2.5, mean_v.false = 1.5) ntrial &lt;- 1e2 ## I used the seed option to make sure I always replicate the result. dat &lt;- simulate(model, nsim = ntrial, ps = p.vector, seed = 123) dmi &lt;- BuildDMI(dat, model) Description statistics As a good practice, we check some basic descriptive statistics. Here is the response time distributions, drawn as one correct RT and one error RT histograms. Note there are two histograms (i.e., distributions). This is one of the specific feature in the choice RT models. This is sometimes dubbed defective distributions, meaning multiple distributions jointly composing a complete model (integrated to 1). The likelihood function in the ggdmc has considered this, so you will not see how the internal C++ codes handle this triviality. But if you use the bare-bones LBA density functions, say “ggdmc:::n1PDFfixedt0” (meaning node 1 probability density function), “ggdmc:::fptcdf” or “ggdmc:::fptpdf”, you need to handle the calculation of “defective distributions” accordingly. These are the functions originally from Brown and Heathcote(2008), but since version 0.2.6.7, ggdmc has no longer exposed them in R interface. By the way, the top x axis in the above figure labels TRUE, representing correct responses and FALSE, representing error responses. It is not unusual to observe more correct responses than error responses, so the simulation produces realistic data. Since version 0.2.7.6, ggdmc uses S4 class to replace original informal S3 class. So the data is now stored as a slot in the dmi object. ## This is to create a column in the data frame to indicate ## correct and error responses. dmi@data$C &lt;- ifelse(dmi@data$S == \"s1\" &amp; dmi@data$R == \"r1\", TRUE, ifelse(dmi@data$S == \"s2\" &amp; dmi@data$R == \"r2\", TRUE, ifelse(dmi@data$S == \"s1\" &amp; dmi@data$R == \"r2\" ,FALSE, ifelse(dmi@data$S == \"s2\" &amp; dmi@data$R == \"r1\", FALSE, NA)))) prop.table(table(dmi@data$C)) ## FALSE == error responses (25.5%) ## TRUE == correct responses (74.5%) ## FALSE TRUE ## 0.255 0.745 ## The maximum (log) likelihoods ## den &lt;- likelihood(p.vector, dmi) sum(log(den)) ## [1] -112.7387 Maximum likelihood estimation The following is the objective function. Note data must be a data model instance. This requirement is to use ggdmc internal to handle many trivialities, for instance, the defective distributions, experimental design, transforming parameter (), etc. If you use the bare-bones density functions, you must handle these trivialities. Also I use negative log likelihood. objective_fun &lt;- function(par, data) { ## Internally, C++ likelihood function will read model type, and ## new Likelihood constructor (in Likelihood.hpp) will read S4 slot. ## So here data variable is OK to be a data-model instance den &lt;- likelihood(par, data) return(-sum(log(den))) } init_par[3] &lt;- runif(1, 0, min(dmi$RT)) This line makes starting non-decision time not less than the minimal RT in the data. This is another psychological consideration. It may help. However, it does not guarantee the optimiser won’t propose a non-decision time less than minimal RT in the data. init_par &lt;- runif(5) init_par[3] &lt;- runif(1, 0, min(dmi@data$RT)) names(init_par) &lt;- c(\"A\", \"B\", \"t0\", \"mean_v.true\", \"mean_v.false\") res &lt;- nlminb(objective_fun, start = init_par, data = dmi, lower = 0) round(res$par, 2) ## remember to check res$convergence Below is a list of possible estimates. The last line show the true parameter vector for the convenience of comparison. The first column shows the numbers of trial per condition. At the size of 1e5, the recovered values almost equal to the true values. ## A B t0 mean_v.true mean_v.false ## 1e2 0.79 0.98 0.17 2.26 0.77 ## 1e2 0.86 1.74 0.04 2.80 1.82 ## 1e2 0.91 0.67 0.28 2.04 1.02 ## 1e2 0.72 1.36 0.14 2.74 1.60 ## 1e3 0.71 1.15 0.16 2.32 1.40 ## 1e3 0.61 1.63 0.08 2.70 1.76 ## 1e4 0.71 1.28 0.15 2.51 1.50 ## 1e5 0.75 1.24 0.15 2.49 1.49 ## true 0.75 1.25 0.15 2.50 1.50 Instead of using the optim function, I opt to nlminb function. This is again a model specific consideration. In the LBA model, A, B, and t0 must not be less than 0, so it will help if we can impose this constraint. Both optim and nlminb offer an argument, lower, to constraint the parameter proposals. However, if you impose the lower constraint, optim allows only (?) the optimisation method, “L-BFGS-B”, which does not handle well infinite. Unfortunately, in fitting the LBA model, it is likely some parameter proposals result in infinite log-likelihoods. Bonus A better way to initialise a parameter proposal is to use prior distributions. rprior in ggdmc allows you to easily do this. This is a step towards Bayesian. p.prior &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"beta\", \"tnorm\", \"tnorm\"), p1 = c(A = 1, B = 1, t0 = 1, mean_v.true = 1, mean_v.false = 1), p2 = c(1, 1, 1, 1, 1), lower = c(rep(0, 3), rep(NA, 2)), upper = c(rep(NA, 2), 1, rep(NA, 2))) init_par &lt;- rprior(p.prior) ## A B t0 mean_v.true mean_v.false ## 0.40 0.65 0.24 0.89 -0.26"
					}

					
				
			
		
			
				
					,
					

					"basics-model-array": {
						"id": "basics-model-array",
						"title": "Model Array",
						"category": "",
						"url": " /basics/model_array/",
						"content": "The method ggdmc adapts different factorial designs is to use Boolean model matrices, which associate experimental conditions with latent variables model (free) parameters. The model parameters are often designed to account for cognitive operations that cannot be directly observed. Three examples are the rate of the degradation of memory strength, the rate of (sensory) evidence accumulation, and the response threshold. Take regression models for example. One might be interested in examining intercepts and slopes, the two regression model parameters by themselves usually do not carry psychological meanings. Of course, one can construct a framework to harness the (regression) model parameters. For example, in traditional visual search studies, mean response times (MRTs) are often associated with the display sizes and the slopes of the MRT-Display size function were conveniently interpreted as search efficiency (Treisman &amp; Gelade, 1980). This was useful strategy as a staring point, but needs further refinement to get more insights (e.g., to understand speed-accuracy trade-off issue, serial vs. parallel processing etc.). It is therefore and often needed to refine the basic regression model to further accommodate many intricate cognitive constructs. ggdmc hard-wires, the diffusion decision and the linear ballistic models and applies the method of Boolean matrices to serve the purpose of adapting factorial designs and that of accounting for latent variables of RT models. The first step in ggdmc is to set up a 3-D model array. Build Models BuildModel creates a model array, which composes of many model matrices. Each of them represents a response. The content of a model matrix indicates the correspondence of parameters and design cells. For example, if a data set has a two-level stimulus factor, affecting the drift rate (as in DDM), a model matrix will have two drift rate parameters, say, v.d1 and v.d2 (d stands for difficulty). One could understand this idea of correspondence between an experimental factor and its parameter mapping by examining the following example. Example 1 In this example, I used the LBA model (Brown and Heathcote, 2008) to illustrate, fitting data from a single participant. The LBA’s B parameter depends only on response (R). The mean and the standard deviation of the drift rates depends on M (matching) factor. The experimental design has one two-level stimulus factor (S). The following model presumes the S factor has no effect on any model parameter. The accuracy is determined by S and R. The M factor is a specific latent factor just for the LBA model. model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = \"M\", sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") p.map means parameter map. match.map matches the stimulus type to the response type to determine if a response is correct or error. factors means experimental factors, constants specifies which model parameter to fix as constant values. This is to enforce model assumptions. responses indicates response types, by specifying character strings or numbers. Lastly, type specifies the model types, such as the diffusion decision model (rd) or the LBA (norm). For illustration purpose, I simulated some realistic response time data. I made up a true parameter vector. This is usually unknown and estimated from data. p.vector &lt;- c(A = .75, B.r1 = .25, B.r2 = .15, t0 = .2, mean_v.true = 2.5, mean_v.false = 1.5, sd_v.true = 0.5) print will show the model array together with its attributes that have been added into in the BuildModel step. print(model) ## r1 ## A B.r1 B.r2 t0 mean_v.true mean_v.false sd_v.true sd_v.false st0 ## s1.r1 TRUE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE ## s2.r1 TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## s1.r2 TRUE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE ## s2.r2 TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## r2 ## A B.r1 B.r2 t0 mean_v.true mean_v.false sd_v.true sd_v.false st0 ## s1.r1 TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## s2.r1 TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## s1.r2 TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## s2.r2 TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## model has the following attributes: ## [1] \"dim\" \"dimnames\" \"all.par\" \"p.vector\" \"pca\" \"par.names\" ## [7] \"type\" \"factors\" \"responses\" \"constants\" \"posdrift\" \"n1.order\" ## [13] \"match.cell\" \"match.map\" \"class\" print, when supplied with a true parameter vector, will show how the factorial design is assigned to model parameters. Understanding the assigning process is an advanced topic. I will return to it at a different tutorial. Note that I, using Brown and Heathcote’s (2008) convention, differentiate the lowercase b and uppercase B in the LBA model. The former means the threshold parameter, and the latter is the travel distance parameter. The LBA model assumes b = A + B. print(model, p.vector) ## \"s1.r1\" ## A b t0 mean_v sd_v st0 ## 0.75 1.0 0.2 2.5 0.5 0 ## 0.75 0.9 0.2 1.5 1.0 0 ## \"s2.r1\" ## A b t0 mean_v sd_v st0 ## 0.75 1.0 0.2 1.5 1.0 0 ## 0.75 0.9 0.2 2.5 0.5 0 ## \"s1.r2\" ## A b t0 mean_v sd_v st0 ## 0.75 0.9 0.2 1.5 1.0 0 ## 0.75 1.0 0.2 2.5 0.5 0 ## \"s2.r2\" ## A b t0 mean_v sd_v st0 ## 0.75 0.9 0.2 2.5 0.5 0 ## 0.75 1.0 0.2 1.5 1.0 0"
					}

					
				
			
		
			
				
					,
					

					"basics-simulation": {
						"id": "basics-simulation",
						"title": "Simulation",
						"category": "",
						"url": " /basics/simulation/",
						"content": "This lesson has two sections. First demonstrates a method to simulate one-participant data. The function, simulate, in the ggdmc package creates a data frame based on the parameter vector and the model (both are defined by a user) with nsim observations for each row in model. ps is the true parameter vector. Second section shows a method to conduct a process model. Specifically, the section conducts a simulation experiment to describe the British tea example on p 37 in Maxwell &amp; Delaney (2004). See Maxwell and Delaney (2004) for an analytic method to calculate the same probabilities. Here I directly model the Britich tea example, approximating the same probabilities. The analytic method is just to use a binominal distribution and the idea of combinations and permutations. One-participant simulation This line define one S (stimulus) factor with two levels. So this model defines one two experimental conditions. factors = list(S = c(“s1”, “s2”)), Below are the R codes for defining a model and for simulating data from the model. require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = \"M\", sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\")), ## one factor with two levels, so only constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") p.vector &lt;- c(A = .75, B.r1 = .25, B.r2 = .15, t0 = .2, mean_v.true = 2.5, mean_v.false = 1.5, sd_v.true = 0.5) This just is to simulate only one observation per condition to check the function. set.seed(123) ## Set seed to get the same simulation dat &lt;- simulate(model, nsim = 1, ps = p.vector) ## S R RT ## 1 s1 r1 0.3327392 ## 2 s2 r1 0.3797985 The following simulates 500 observations per condition. So in total, there are 1000 observations. ntrial &lt;- 5e2 ## number of trials per condition dat &lt;- simulate(model, nsim = ntrial, ps = p.vector) dplyr::tbl_df(dat) ## A tibble: 1,000 x 3 ## S R RT ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 s1 r2 0.533 ## 2 s1 r2 0.494 ## 3 s1 r1 0.497 ## 4 s1 r2 0.310 ## 5 s1 r1 0.462 ## 6 s1 r2 0.345 ## 7 s1 r2 0.430 ## 8 s1 r1 0.384 ## 9 s1 r2 0.310 ## 10 s1 r1 0.302 # # ... with 990 more rows Note that model and data are in fact two separate objects. To fit data with certain models, we need to bind them together with BuildDMI. This is useful to facilitate model comparison. That is, a data set can bind with many different models, so we can compare them to see which model may fit the data better so perhaps provide a better account. I used a term, data-model instance (dmi), coined by Matthew Gretton. dmi &lt;- BuildDMI(dat, model) We can the codes introduced in the “Descriptive Statistics” to check the correct 10%, 50%, 90% quantile RTs and accuracy, separately, for each level of the stimulus factor. First I convert the dmi data frame to a data table and then create a new accuracy (logical) column, C. require(data.table) d &lt;- data.table(dmi) d$C &lt;- ifelse(d$S == \"s1\" &amp; d$R == \"r1\", TRUE, ifelse(d$S == \"s2\" &amp; d$R == \"r2\", TRUE, ifelse(d$S == \"s1\" &amp; d$R == \"r2\", FALSE, ifelse(d$S == \"s2\" &amp; d$R == \"r1\", FALSE, NA)))) d[, .(q1 = round(quantile(RT, .1), 2), q5 = round(quantile(RT, .5), 2), q9 = round(quantile(RT, .9), 2)), .(C, S)] ## C S q1 q5 q9 ## 1: TRUE s1 0.32 0.42 0.56 ## 2: TRUE s2 0.28 0.39 0.52 ## 3: FALSE s1 0.27 0.37 0.50 ## 4: FALSE s2 0.32 0.39 0.54 pro &lt;- d[, .N, .(C, S)] pro[, NN := sum(N), .(S)] pro[, value := N NN] cp &lt;- pro[C == TRUE] ## correct percentage ## C S N NN value ## 1: TRUE s1 333 500 0.666 ## 2: TRUE s2 391 500 0.782 ep &lt;- pro[C == FALSE] ## error percentage ## C S N NN value ## 1: FALSE s1 167 500 0.334 ## 2: FALSE s2 109 500 0.218 Plot the RT distributions require(ggplot2) bw &lt;- .01 ## 10 ms binwidth p0 &lt;- ggplot(d, aes(RT)) + geom_histogram(binwidth = bw, fill = \"white\", colour = \"black\") + facet_grid(.~C) + theme_bw(base_size = 18) print(p0) British tea example This section shows how we may test an hypothetical question directly via a simulation. Quoted from Maxwell and Delaney (p. 37, 2004) “A lady declares that by tasting a cup of tea made with milk, she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. (Fisher, 1935 1971, p. 11)” This is essential a binominal decision making. That is, the decision maker (“the lady”) in question will be presented one cup of tea after another and then her task is to decide if the cup is made by milk or tea is added first. This following function, British.tea implements a process model to describe the above “British tea example”. That is, it conducts a simulation experiment of presenting 8 (i.e., n) cups of tea to a participant. The n equals 8 is decided arbitrarily here. One additional information (i.e., assumption) is that the participant is told half of the cups are milk first and tea and vice versa. So when simulating the chance only scenario, we need also to take this into consideration. That is, after making a decision for a cup (either MT or TM), the (chance) probability state should adjust accordingly. ##' British tea example ##' ##' The function runs a simulation study to test the British tea example ##' ##' @param the number of observation (cups of tea) ##' @param correct correct sequence: First four cups are tea and milk ## (TM = 1), the next four cups are milk and then tea (MT = 0). ##' @param verbose print more information ##' ##' @export British.tea &lt;- function(n = 8, correct = c(1,1,1,1, 0,0,0,0), verbose = TRUE) { MT &lt;- n 2 ## 0 indicates milk and then tea (MT) TM &lt;- n 2 ## 1 indicates tea and then milk (TM) ## Create three containers ## 1. x0 is a \"n x 2\" matrix to store the evolution of chance probabilities ## 2. res is a n-element numeric vector ## 3. acc is a n logical vector; default value is FALSE x0 &lt;- matrix(numeric(n*2), ncol = 2) res &lt;- numeric(n) acc &lt;- rep(FALSE, n) ## Begin the experiment, presenting one cup after another for (i in 1:n) { if (verbose) cat(\"Cup\", i, \"in total\", sum(MT, TM), \" cup(s)\\n\") ## store the chance probabilities of MT and TM in probs probs &lt;- c(MT (MT + TM), TM (MT + TM)) if (verbose) cat(\"Chances probabilities of (MT, TM): \", probs, \"\\n\") x0[i, ] &lt;- probs decision &lt;- sample(c(0, 1), 1, prob = probs); if (decision == 0) { if (verbose) cat(\"This cup is made by adding milk first\\n\") MT &lt;- MT - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else if (decision == 1) { if (verbose) cat(\"This cup is made by adding tea first\\n\") TM &lt;- TM - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else cat(\"Unexpected situation\\n\") if (verbose) cat(\"Current state\", i, \": \", c(MT, TM), \"\\n\\n\") } if (verbose) cat(\"Done\\n\") return(list(x0, res, correct, acc)) } The simulation starts from the for loop. for (i in 1:n) {…} The for loop presents a cup of tea after another until the nth cup. Before the participant make a decision, the chance probabilities of the two possible outcomes are stored in x0 variable. probs &lt;- c(probMT, probTM) x0[i, ] &lt;- probs And then the sample function acts as a chance mechanism to simulate the participant’s (chance) decision making process. decision &lt;- sample(c(0, 1), 1, replace = TRUE, prob = probs); The function randomly choose two numbers, c(0, 1), with the probabilities, probs to for the first and second number. ## Begin the experiment, presenting one cup after another for (i in 1:n) { if (verbose) cat(\"Cup\", i, \"in total\", sum(MT, TM), \" cup(s)\\n\") probMT &lt;- MT (MT + TM) ## chance probability of MT 0 probTM &lt;- TM (MT + TM) ## chance probability of TM 1 probs &lt;- c(probMT, probTM) if (verbose) cat(\"Chances probabilities of (MT, TM): \", probs, \"\\n\") x0[i, ] &lt;- probs decision &lt;- sample(c(0, 1), 1, prob = probs); if (decision == 0) { if (verbose) cat(\"This cup is made by adding milk first\\n\") MT &lt;- MT - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else if (decision == 1) { if (verbose) cat(\"This cup is made by adding tea first\\n\") TM &lt;- TM - 1 res[i] &lt;- decision if (decision == correct[i]) acc[i] &lt;- TRUE } else cat(\"Unexpected situation\\n\") if (verbose) cat(\"Current state\", i, \": \", c(MT, TM), \"\\n\\n\") } Conduct one experiment and print information ncup &lt;- 8 cor &lt;- c(rep(1, 4), rep(0, 4)); res &lt;- British.tea(ncup, cor, TRUE) Cup 1 in total 8 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding tea first Current state 1 : 4 3 Cup 2 in total 7 cup(s). Chances probabilities of (MT, TM): 0.5714286 0.4285714 This cup is made by adding milk first Current state 2 : 3 3 Cup 3 in total 6 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding tea first Current state 3 : 3 2 Cup 4 in total 5 cup(s). Chances probabilities of (MT, TM): 0.6 0.4 This cup is made by adding milk first Current state 4 : 2 2 Cup 5 in total 4 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding milk first Current state 5 : 1 2 Cup 6 in total 3 cup(s). Chances probabilities of (MT, TM): 0.3333333 0.6666667 This cup is made by adding tea first Current state 6 : 1 1 Cup 7 in total 2 cup(s). Chances probabilities of (MT, TM): 0.5 0.5 This cup is made by adding tea first Current state 7 : 1 0 Cup 8 in total 1 cup(s). Chances probabilities of (MT, TM): 1 0 This cup is made by adding milk first Current state 8 : 0 0 Done Now I replicate the experiments separately for 512, 4096, 32768, 262144, and 2097152 times and store each result in a list, called exp. n &lt;- 8^(3:7); exp &lt;- vector(\"list\", length(n)) ## Use parallel package to conduct experiments ## 100.636 s library(parallel) cl &lt;- makeCluster(detectCores()) clusterExport(cl, c(\"British.tea\", \"ncup\", \"cor\")) system.time( for (i in 1:length(n)) { exp[[i]] &lt;- parSapply(cl, 1:n[i], function(i, ...) {British.tea(ncup, cor, FALSE)} ) } ) stopCluster(cl) ## Without using parallel ## for(i in 1:length(n)) { ## exp[[i]] &lt;- replicate(n[i], British.tea(ncup, cor, FALSE)) ## } res3 &lt;- numeric(length(n)); res3 ## to store the result when 6 corrects res4 &lt;- numeric(length(n)); res4 ## to store the result when 8 corrects ## Collect results for(i in 1:length(n)) { c3 &lt;- 0 c4 &lt;- 0 for(j in 1:n[i]) { ## Calculate exactly 4 corrects if(all(exp[[i]][,j][[4]])) c4 &lt;- c4 + 1 if(sum(exp[[i]][,j][[4]]) == 6) c3 &lt;- c3 + 1 } res3[i] &lt;- c3 n[i] res4[i] &lt;- c4 n[i] } round(res3, 4) ## [1] 0.2578 0.2324 0.2306 0.2288 0.2286 round(res4, 4) ## [1] 0.0137 0.0137 0.0140 0.0141 0.0143 require(ggplot2); require(data.table) ## Plot the result ## (How to add differernt horizontal lines on each facet) DT &lt;- data.table(x= rep(n, 2), y = c(res3, res4), gp = rep(c(\"6\", \"8\"), each = 5), ref = rep(c(16 70, 1 70), each = 5)) ## Dashlines show theoretically probabilities p0 &lt;- ggplot(DT, aes(x, y)) + geom_point(size = 3) + geom_hline(aes(yintercept = ref), linetype = \"dashed\") + ## scale_x_log10(name = \"N\") + xlab(\"N\") + ylab(\"Probability\") + facet_grid(gp~., scales = \"free\") + theme_bw(base_size = 22) print(p0)"
					}

					
				
			
		
			
				
					,
					

					"basics-summary": {
						"id": "basics-summary",
						"title": "Summary Statistics",
						"category": "",
						"url": " /basics/summary/",
						"content": "In analyzing response time data with two choices, researchers would usually examine average response times (RTs) and response proportions. Depending on the model a researcher wishes to presume, the response proportions can simply be correct and error rates, or, if using SDT model, hits, correct rejections, false alarms and misses. Here I used Pleskac, Cesario, and Johnson’s (2017) data in the first-person shooter task (FPST; Correll et al., 2002) to illustrate one method to calculate average RTs and response proportions across participants. Firstly, I use fread function to load the data file, which is in csv format. The data set provides clear and good column names. That is, the column names have informed the coding method. I simply just followed column names to code the factor levels and later checked against the data in the paper. Of course, I had also checked against the figures of behaviour analyses in the paper to make sure I did correctly identify the dependent and independent variables, which are listed in the following. S: stimulus factor, gun vs. non-gun objects. BC: blurry or clear object CT: context, a safe or dangerous neighborhood RACE: race, a black or white target R: response factor, shoot or not to shoot RT: response times s: subject participant nominal labels library(data.table); study3 &lt;- fread(\"data race Study3 original Study3TrialData.csv\") study3$S &lt;- factor(ifelse(study3$Object0NG1G == 0, \"non\", \"gun\")) study3$BC &lt;- factor(ifelse(study3$Blurry0Clear1Blur == 0, \"clear\", \"blur\")) study3$CT &lt;- factor(ifelse(study3$Context1Safe2Danger == 0, \"safe\", \"danger\")) study3$RACE &lt;- factor(ifelse(study3$Race012B == 0, \"white\", \"black\")) study3$R &lt;- factor(ifelse(study3$Resp0NS1Sh == 0, \"not\", \"shoot\")) study3$RT &lt;- study3$RT 1e3 study3$s &lt;- factor(study3$Subject) factor is a R function converting variables, numeric or character, to categorical (i.e., nominal) variable. After reorganizing the columns, I removed the replicated columns by assigning them as NULL. This is a data.table specific syntax. study3[, c(\"Subject\", \"NewSubject\", \"conditionRaceDangerBlurbject\", \"conditionRaceDangerBlur\", \"Object0NG1G\", \"Blurry0Clear1Blur\", \"Context1Safe2Danger\", \"Race012B\", \"Resp0NS1Sh\", \"DiffusionRT\") := NULL] There are NaN response times in this data set. One method is to replace them with random RTs drawn from uniform distribution, with the range of valid RTs. This was achieved by using the data.table internal function .I. I firstly found the (row) index of these NaN RTs, and then replaced them. Of course, we can simply just remove them. ## save organized data to a temporary object, so I can roll back. dtmp &lt;- data.table(study3) minmax &lt;- range(study3$RT, na.rm = TRUE); minmax idx &lt;- dtmp[, .I[is.nan(RT)]]; idx dtmp[idx, RT := runif(1, minmax[1], minmax[2])] d &lt;- dtmp ## scoring a correctness column d$C &lt;- ifelse(d$S == \"gun\" &amp; d$R == \"shoot\", TRUE, ifelse(d$S == \"non\" &amp; d$R == \"not\", TRUE, ifelse(d$S == \"gun\" &amp; d$R == \"not\", FALSE, ifelse(d$S == \"non\" &amp; d$R == \"shoot\", FALSE, NA)))) Now the data table looks like: dplyr::tbl_df(d) ## # A tibble: 12,033 x 8 ## RT S BC CT RACE R s C ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;lgl&gt; ## 1 0.753 gun blur safe black shoot 11 TRUE ## 2 0.851 non blur safe white not 11 TRUE ## 3 0.742 gun clear safe black shoot 11 TRUE ## 4 0.636 non clear safe white not 11 TRUE ## 5 0.644 gun blur safe black not 11 FALSE ## 6 0.625 non clear safe black shoot 11 FALSE ## 7 0.889 non clear safe white not 11 TRUE ## 8 0.597 gun blur safe black shoot 11 TRUE ## 9 0.724 gun clear safe white shoot 11 TRUE ## 10 0.656 non blur safe white not 11 TRUE ## # ... with 12,023 more rows Censoring RT data Censoring outliers is a difficult task (Ratcliff, 1993). Here I illustrated one way to do it via Heathcote’s rc, a collection of very useful R functions and my summarise, also a collection of useful R functions. First, I used R’s source function to load this large collection of R functions. source(\"~ rc data.analysis.R\") source(\"~ rc utils.R\") source(\"~ functions summarise.R\") ## Scoring ------------ se3 &lt;- score.rc(data.frame(d), S = \"s\", R = \"R\", RT = \"RT\", SC = \"C\", F = c(\"BC\", \"CT\", \"RACE\", \"S\")) ## Spreading 11851 of 12033 RTs that are ties given preceision 0.001 . ## 497 have ties out of 679 unique values ## ## Added the following manifest design ## S RACE CT BC R rcell ## 1 gun black danger blur not 1 ## 2 gun black danger blur shoot 1 ## 3 gun black danger clear not 2 ## ... ## 30 non white safe blur shoot 15 ## 31 non white safe clear not 16 ## 32 non white safe clear shoot 16 score.rc function takes first argument data.frame, which is the data as seen previously. Because I stored it as data.table, I needed to convert it back to data.frame. Just a note. Although data.table may accommodate many functions operating in data.frame, there are some operations in rc functions, which cannot work in data.table. Note the second argument, uppercase S, which takes the subject column, instead of the column of stimulus factor. The R and RT arguments take response column and the response time column. SC takes the column of score correctness, which is purely my guess. I cannot be sure why it is called SC. The last useful argument is F, which takes user-defined factors, including the stimulus factor. score.rc detects the identical (ties) RTs and spread them into finer scale. For example, in this data set, there are 31 trials with 60y ms. table(d$RT) ## 0.01 0.015 0.019 0.025 ## 1 1 1 1 ## 0.027 0.03 0.035 0.036 ## 1 1 1 1 ## ... ## 0.386 0.387 0.388 0.389 ## 7 3 1 3 ## 0.39 0.391 0.392 0.393 ## 1 4 3 3 ## 0.394 0.395 0.396 0.397 ## 2 6 7 3 ## ... ## 0.606 0.607 0.608 0.609 ## 50 31 40 39 ## 0.61 0.611 0.612 0.613 ## 42 49 31 41 ## 0.614 0.615 0.616 0.617 ## 49 55 42 39 ## ... If I printed them all out, the data set after scoring spreads these RT to a se3[se3$RT &gt;= .607 &amp; se3$RT &lt; .608,] cell rcell s BC CT RACE S C R RT 539 31 16 19 clear safe white non TRUE not 0.6070000 838 31 16 24 clear safe white non TRUE not 0.6075488 1909 26 13 39 blur danger white non FALSE shoot 0.6070313 2108 6 3 44 blur safe black gun TRUE shoot 0.6072812 2321 4 2 50 clear danger black gun TRUE shoot 0.6079634 2354 19 10 50 clear danger black non TRUE not 0.6071250 2939 15 8 62 clear safe white gun FALSE not 0.6072188 3083 32 16 62 clear safe white non FALSE shoot 0.6077683 3319 12 6 72 clear danger white gun TRUE shoot 0.6075244 3762 19 10 82 clear danger black non TRUE not 0.6079146 4802 23 12 120 clear safe black non TRUE not 0.6076951 5211 17 9 129 blur danger black non TRUE not 0.6075976 5391 19 10 129 clear danger black non TRUE not 0.6074063 6095 1 1 184 blur danger black gun FALSE not 0.6073125 7057 10 5 201 blur danger white gun TRUE shoot 0.6077195 7201 17 9 201 blur danger black non TRUE not 0.6078659 7288 6 3 214 blur safe black gun TRUE shoot 0.6073438 7345 25 13 214 blur danger white non TRUE not 0.6071875 7747 23 12 218 clear safe black non TRUE not 0.6077439 8259 8 4 231 clear safe black gun TRUE shoot 0.6076707 8305 19 10 231 clear danger black non TRUE not 0.6079390 8671 12 6 235 clear danger white gun TRUE shoot 0.6071563 8923 19 10 247 clear danger black non TRUE not 0.6073750 9002 31 16 247 clear safe white non TRUE not 0.6074375 9045 31 16 247 clear safe white non TRUE not 0.6070625 9509 31 16 286 clear safe white non TRUE not 0.6076220 9551 31 16 286 clear safe white non TRUE not 0.6078415 9692 6 3 286 blur safe black gun TRUE shoot 0.6075732 9903 27 14 288 clear danger white non TRUE not 0.6079878 10432 25 13 307 blur danger white non TRUE not 0.6077927 10534 10 5 308 blur danger white gun TRUE shoot 0.6078902 10666 21 11 308 blur safe black non TRUE not 0.6074688 10979 16 8 325 clear safe white gun TRUE shoot 0.6078171 11201 9 5 326 blur danger white gun FALSE not 0.6070937 11642 19 10 344 clear danger black non TRUE not 0.6072500 11866 25 13 348 blur danger white non TRUE not 0.6076463 The original data set is to the millisecond scale. d[RT == .607] RT S BC CT RACE R s C 1: 0.607 gun blur danger black not 11 FALSE 2: 0.607 non blur danger black not 19 TRUE 3: 0.607 non clear safe white not 19 TRUE 4: 0.607 gun blur safe black shoot 24 TRUE 5: 0.607 non clear danger white not 28 TRUE 6: 0.607 non clear safe white not 37 TRUE 7: 0.607 non blur danger white shoot 39 FALSE 8: 0.607 non blur danger black not 44 TRUE 9: 0.607 gun blur safe black shoot 44 TRUE 10: 0.607 non clear danger black not 50 TRUE 11: 0.607 gun clear danger white shoot 50 TRUE 12: 0.607 gun clear safe white not 62 FALSE 13: 0.607 non clear danger black not 129 TRUE 14: 0.607 gun blur danger black not 184 FALSE 15: 0.607 non clear safe white not 184 TRUE 16: 0.607 non blur danger black not 184 TRUE 17: 0.607 gun blur safe black shoot 214 TRUE 18: 0.607 non blur danger white not 214 TRUE 19: 0.607 non clear safe white not 218 TRUE 20: 0.607 gun clear danger white shoot 235 TRUE 21: 0.607 non clear danger black not 247 TRUE 22: 0.607 non clear safe white not 247 TRUE 23: 0.607 non clear safe white not 247 TRUE 24: 0.607 gun clear danger white shoot 288 TRUE 25: 0.607 non blur safe black not 308 TRUE 26: 0.607 non clear safe white not 325 TRUE 27: 0.607 gun blur safe white shoot 326 TRUE 28: 0.607 gun blur danger black shoot 326 TRUE 29: 0.607 gun blur danger white not 326 FALSE 30: 0.607 non blur danger black not 344 TRUE 31: 0.607 non clear danger black not 344 TRUE RT S BC CT RACE R s C The scored data set, se3, will also attach two new columns, cell and rcell, indicating the experimental design. In this example, it has 32 cell, so cell is from 1 to 32 and rcell is from 1 to 16, because this is a two-choice experiment, response, shoot and not to shoot in cell 1 and cell 2, belong to the same experimental design, but with different response types. ## 1 gun black danger blur not 1 ## 2 gun black danger blur shoot 1 ## 3 gun black danger clear not 2 ## 4 gun black danger clear shoot 2 A usual practice is to take 3 times the standard deviation, respectively in each participants. This can be achieved via tapply function. If the data set is large, one can use data.table to achieve the same aim, which I will demonstrate in a later tutorial. sd3 &lt;- tapply(se3$RT, se3$s, mean) + tapply(se3$RT, se3$s, sd) * 3; A second useful function in rc collection is the make.rc, which does the censoring work. It takes a first argument of the scored data set, from score.rc and a second argument, correct.name, indicating the character string for the correctness column, and the last two arguments, for the lower and upper bounds of the censoring. me3 &lt;- make.rc(se3, correct.name = \"C\", minrt = .2, maxrt = sd3) How to average across trials mv: measurement dependent variable gvs: grouping variables wvs: within variables acc0 &lt;- summarySE(d, mv = \"error\", gvs = c(\"s\", \"BC\", \"CT\", \"RACE\", \"S\")) mrt0 &lt;- summarySE(d[C == TRUE], mv = \"RT\", gvs = c(\"s\", \"BC\", \"CT\", \"RACE\", \"S\")) ## Within se average across subjects for pc and nt figA &lt;- summarySEwithin(acc0, wvs = c(\"BC\",\"CT\", \"RACE\", \"S\"), mv = \"error\") figB &lt;- summarySEwithin(mrt0, wvs = c(\"BC\",\"CT\", \"RACE\", \"S\"), mv = \"RT\") names(figA) &lt;- c(\"BC\", \"CT\", \"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") names(figB) &lt;- c(\"BC\", \"CT\", \"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") Reference Ratcliff, R. (1993). Methods for dealing with reaction time outliers. Psychological bulletin, 114(3), 510."
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-diagnosis": {
						"id": "bayes-basics-diagnosis",
						"title": "Checking Fitted Models",
						"category": "",
						"url": " /bayes-basics/diagnosis/",
						"content": "This page (temporarily) documents four different plots for checking fitted models. The example simulates a data set from the regression normal model. rm(list = ls()) model &lt;- BuildModel( p.map = list(a = \"1\", b = \"1\", tau = \"1\"), match.map = NULL, regressors= c(8, 15, 22, 29, 36), factors = list(S = c(\"x1\")), responses = \"r1\", constants = NULL, type = \"glm\") p.vector &lt;- c(a = 242.7, b = 6.185, tau = .01) ntrial &lt;- 1000 dat &lt;- simulate(model, nsim = ntrial, ps = p.vector) dmi &lt;- BuildDMI(dat, model) npar &lt;- length(GetPNames(model)) start &lt;- BuildPrior( dists = c(\"tnorm2\", \"tnorm2\", \"gamma\"), p1 = c(a = 240, b = 6, tau = .01), p2 = c(a = 1e-6, b = 1e-6, tau = .1), lower = c(NA, NA, NA), upper = rep(NA, npar)) p.prior &lt;- BuildPrior( dists = c(\"tnorm2\", \"tnorm2\", \"gamma\"), p1 = c(a = 200, b = 0, tau = .1), p2 = c(a = 1e-6, b = 1e-6, tau = .1), lower = c(NA, NA, NA), upper = rep(NA, npar)) ## Sampling ----------- fit0 &lt;- Start_glm(5e2, dmi, start, p.prior, thin = 8) fit &lt;- run(fit0, pm0 = .05) fit &lt;- run(RestartSamples(5e2, fit, thin = 8)) Trace and density plots The first two are trace and density plots. p0 &lt;- plot(fit) p1 &lt;- plot(fit, pll = FALSE, den = TRUE) Autocorrelation plots The third is the autocorrelation plot. This first figure plots all chains and the second randomly selects a subset of three chains to construct the figure. The latter function is useful when fitting a model with many parameters and the model fit uses a large number of chains. By default, the “autocor” calculates to 50 lags. p2 &lt;- autocor(fit) p3 &lt;- autocor(fit, nsubchain = 3) Correlation matrix The fourth is the plot of correlation matrix. This plot is useful to check (post hoc) the association among model parameters. This needs to use the ggpairs function in GGally package. pairs.model &lt;- function(x, start = 1, end = NA, ...) { if (x$n.chains == 1) stop (\"MCMC needs multiple chains to check convergence\") if (is.null(x$theta)) stop(\"Use hyper mcmc_list\") if (is.na(end)) end &lt;- x$nmc if (end &lt;= start) stop(\"End must be greater than start\") d &lt;- ConvertChains(x, start, end, FALSE) D_wide &lt;- data.table::dcast.data.table(d, Iteration + Chain ~ Parameter, value.var = \"value\") bracket_names &lt;- names(D_wide) par_cols &lt;- !(bracket_names %in% c(\"Iteration\", \"Chain\")) p0 &lt;- GGally::ggpairs(D_wide, columnLabels = bracket_names[par_cols], columns = which(par_cols), ...) print(p0) return(invisible(p0)) } p5 &lt;- pairs.model(fit) p6 &lt;- pairs.model(fit, lower = list(continuous = \"density\")) The additional option in p6 is to choose to plot density contour. lower = list(continuous = “density”)"
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-likelihood": {
						"id": "bayes-basics-likelihood",
						"title": "Model Likelihood",
						"category": "",
						"url": " /bayes-basics/likelihood/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-posterior": {
						"id": "bayes-basics-posterior",
						"title": "Posterior Distribution",
						"category": "",
						"url": " /bayes-basics/posterior/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-prior": {
						"id": "bayes-basics-prior",
						"title": "Prior Distribution",
						"category": "",
						"url": " /bayes-basics/prior/",
						"content": "In Bayesian computation, a prior distribution refers to a similar, but slightly different idea from the original Bayes’ theorem. I used the diffusion decisoin model (DDM, Ratcliff &amp; McKoon, 2008) as an example to illustrate the idea. The full DDM has eight parameters. In ggdmc (as well as DMC) syntax, they are defined as following: p.map = list(a = “1”, v = “1”, z = “1”, d = “1”, sz = “1”, sv = “1”, t0 = “1”, st0 = “1”), a: the boundary separation v: the mean of the drift rate z: the mean of the starting point of the diffusion relative to threshold separation d: differences in the non-decisional component between upper and lower threshold sz: the width of the support of the distribution of zr sv: the standard deviation of the drift rate t0: the mean of the non-decisional component of the response time st0: the width of the support of the distribution of t0 The question is how do we determine the values for these parameters. This is where prior distribution comes in. We presume there are eight distributions jointly determine the DDM prior distribution and these eight distributions are where we draw the realized parameter values. This way, the parameter values are said stochastic, rather than deterministic. In other words, the value, for instance boundary separation, a, changes every time we consult its prior distribution. It is decided probabilistically by its prior distribution. Below I list the full command, BuildModel, for setting up a DDM model. ## Use verbose option to suppress printing p.vector ## This is a DDM model with no manipulation factor model &lt;- BuildModel( p.map = list(a = \"1\", v = \"1\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(left = \"LEFT\", right = \"RIGHT\")), factors = list(S = c(\"left\", \"right\")), constants = c(st0 = 0, d = 0), responses = c(\"LEFT\", \"RIGHT\"), type = \"rd\", verbose = TRUE) Set up Priors So in this example, we will want to set up six prior distributions, because in the above model set-up, the st0 and d have been set to constant as 0. That is, they are deterministic, not stochastic. ggdmc (as well as DMC) has a function to build prior. Unimaginatively, it is called BuildPrior (it is called p.prior.dmc in DMC). p.prior &lt;- BuildPrior( p1 = c(a = 1.5, v = 3, z = .5, sz = .3, sv = 1, t0 = .2), p2 = c(a = 0.5, v = .5, z = .1, sz = .1, sv = .3, t0 =.05), lower = c(0, -5, 0, 0, 0, 0), upper = c(2, 7, 4, 4, 4, 1), dists = rep(\"tnorm\", 6)) A list of options arguments for the BuildPrior function can be found by enter: ?BuildPrior Here is a copy from the R documentation in ggdmc pacakge. p1 simply means the first parameter of a distribution p2 simiarly mean the second parameter of a distribution lower is the lower support (i.e., the lower truncated boundary) upper is the upper support (i.e., the upper truncated boundary) dists is a string vector specifying the name of a distribution. Current version of ggdmc provides four types of prior distributions: tnorm, Normal and truncated normal, where: p1 = mean, p2 = sd. It specifies a normal distribution when bounds are set -Inf and Inf, beta, Beta, where: p1 = shape1 and p2 = shape2 (see ?pbeta in R). Note the uniform distribution is a special case of the beta distribution when p1 and p2 = 1, gamma, Gamma, where p1 = shape and p2 = scale (see ?pgamma in R). Note p2 is scale, not rate, lnorm, Lognormal, where p1 = meanlog and p2 = sdlog (see ?plnorm). In the ggdmc (as well as DMC) operation, the names (i.e., character strings) are important for corret computation. The two options, lower and upper, are to set the distribution support. for tnorm, these define the lower and upper bounds; When the user enters NA , the default behaviour of the function is to set the values as -Inf and Inf. This make a truncated normal distribution becoming a normal distribution (see ?pnorm). for beta, these define the lower and upper bounds (i.e., scaled beta distribution). The default behaviour for entering NA is to filled with the values of 0 and 1. p1 = 1 &amp; p2 = 1 &amp; lower = 0 (default) &amp; upper = 1 (default) creates Uniform(0, 1) p1 = 1 &amp; p2 = 1 &amp; lower = l &amp; upper = u creates Uniform(l, u) for gamma, lower shifts the distribution to exclude small values for lognormal, lower shifts the distribution to exclude small values Example 1: Set up beta (and uniform) prior Currently, the below example is from Heathcote et al’s (2018) DMC tutorial of LNR model. beta.prior &lt;- BuildPrior( dists = c(\"beta\", \"beta\", \"beta\", \"beta\", \"beta\"), p1 = c(meanlog.true = 1, meanlog.false = 1, sdlog.true = 1, sdlog.false = 1, t0 = 1), p2 = c(meanlog.true = 1, meanlog.false = 1, sdlog.true = 1, sdlog.false = 1, t0 = 1), lower = c(-4,-4, 0, 0, 0.1), upper = c( 4, 4, 4, 4, 1)) You can plot and print the prior distribution by using the plot and print functions. plot(beta.prior) print(p.prior) ## p1 p2 lower upper log dist untrans ## meanlog.true 1 1 -4 4 1 beta_lu identity ## meanlog.false 1 1 -4 4 1 beta_lu identity ## sdlog.true 1 1 0 4 1 beta_lu identity ## sdlog.false 1 1 0 4 1 beta_lu identity ## t0 1 1 0.1 1 1 beta_lu identity This is how to calculate log-prior likelihoods (i.e., probability densities) for each model parameter and add them all together. dprior(p.vector, p.prior) ## meanlog.true meanlog.false sdlog.true sdlog.false t0 ## -2.0794415 -2.0794415 -1.3862944 -1.3862944 0.1053605 sumlogpriorNV(p.vector, p.prior) ## [1] -6.826111 What to look for when set up prior distributions For setting up prior distributions, key points are to look for first whether prior distributions cover broad range (i.e., relatively uninformative) and second whether their range cover abnormal values. For example, it is not possible to have negative standard devation, so sd_v.true subpanel should not cover negative values."
					}

					
				
			
		
			
				
					,
					

					"bayes-basics-theorem": {
						"id": "bayes-basics-theorem",
						"title": "Bayes' Theorem",
						"category": "",
						"url": " /bayes-basics/theorem/",
						"content": "How do we reach decisions to act on something? One way to answer this question is the action driven by a decision bringing positive feedback. The feedback is often accompanied by monetary or other forms of rewards; it thereby motivates us to make such decisions in the future. In the terminology of psychological experiments, the feedback could conceive as been shown in the form of data, which are collected from human and or animal subjects. In other words, the theories (our prior beliefs) behind every decision that entails an action (prediction) and we then collect data to check how close the prediction fit the data. How? Often the closer our prediction matches the resultant data, the more rewards we might receive. Hence when there are some mismatches between the predictions and the data, we would likely modify our theories beliefs. They then become posterior belief. This intuitive idea of human decision-making is described by the well-known Bayes’ theorem (Bayes, Price, &amp; Canton, 1763): y represents data. For example, a serial of response times in seconds, c(0.533, 0.494, 0.494, …); θ represents a set of parameters. That is, it is a parameter vector; P(θ) represents our prior belief in the form of a probability distribution, which is fully accounted for the parameter vector. This is often dubbed the prior distribution. P(y | θ) represents the mechanism accounting for the data. This is often dubbed (data’s) likelihood function. P(θ | y) represents posterior belief, which similar to the prior belief, is often dubbed the posterior distribution."
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-cddm": {
						"id": "cognitive-model-cddm",
						"title": "Circular Drift-diffusion Model",
						"category": "",
						"url": " /cognitive-model/cddm/",
						"content": "Disclaimer: We have striven to minimize the number of errors. However, we cannot guarantee the note is 100% accurate. This tutorial requires the CDDM module, which is part of an ongoing research project, so has not released, yet. Circular drift-diffusion model (CDDM) is a two-dimension process model. It could be viewed as an extension of the one-dimension diffusion model. One assumption of the 1-D diffusion model is it posits a single unit accumulator accrues evidence towards two opposing, an upper and a lower, boundaries. As illustrated in the right panel in the following figure, when the accumulator moves towards one boundary, it at the same time moves away from the other. This inevitable situation is in fact a powerful design that restricts the model to account for a set of processes. Therefore, the model provides a concise and successful account for the cognitive processes in the now ubiquitous two-alternative forced-choice (2AFC) task in cognitive psychology. Figure 1. 1-D and 2-D random walk processes. However, when one wants to model the tasks allowing more than two response types, it is not immediately clear how the 1-D diffusion model can extend to this situation. One usual option is to use the accumulator models (Ratcliff, Smith, Brown, &amp; McKoon, 2016), for example, the LBA model, the LCA model or the feed forward inhibition model (Brown &amp; Heathcote, 2008; Usher &amp; McClellend, 2001; Mazurek et al., 2003; Niwa &amp; Ditterich, 2008, Roe et al., 2001). These models use the absolute, as oppose to the relative, evidence criteria, as the stopping rule for the process. In this design, each response type corresponds to one unit accumulator, accuring evidence towards either one or multiple response thresholds. Although it was discussed largely regarding to its utility to model the continuous report task (Smith, 2016), the circular drift-diffusion model can also account for the tasks with more than two responses. The aim of this tutorial is to demonstrate how this can be done using ggdmc. This tutorial is divided into three sections. First, we introduce three CDDM core functions, dcircle, rcircle, and rcircle_process. Following the convention of R language, d refers to probability density function and r refers to random number generation. Instead of recording only the response time and angle, rcircle_process simulates the 2-D diffusion process and records the trace, as shown in the left panel in the illustration figure. The source codes, CDDM.hpp and CDDM.cpp, provide the implementation details regarding the three functions. In the second section, we replicated the simulation studies in Smith (2016) to check the accuracy of our CDDM module. Next, we conducted a series of parameter recovery studies, using maximum likelihood estimation. The purpose of the recovery study is to provide a template when one wishes to fit an empirical data set from a continuous report task. For using the Bayesian method to fit the CDDM, please go to the tutorials of fixed-effect and hierarchical models. Core functions The 2-D diffusion model has four main parameters, v1, v2, a, and t0. v1 and v2 are the average increments of the evidence on the x and y axes. They are the two components of the drift vector v, whose magnitude is the Euclidean norm ||v||. As the equation suggestes, a second way to describe the drift vector is via the left-hand side of the equation, using the magnitude and phase angle, of the drift vector. The drift vector drives the evidence growth towards the decision boundary, a, which, in the case of 2-D diffusion model, is the circumference of a disc (Figure 1). The stochastic component of the accumulation process is driven by the within-trial variability, , the diffusion coefficient. For simplicity, the SD, , corresponding to the two components of the drift vector are assumed as identical and independent of each other. We code it as sigma1 and sigma2 to remind the user that this is an assumption and the user can relax it by tweaking the source codes. For now, we recommend to set them as 1. The time it takes the accumulator hit the boundary is the decision time. The response time is the decision time plus with the non-decision time, t0. In summary, the parameter vector composes of: v1, the mean drift rate on the x axis, v2, the mean drift rate on the y axis, a, the response criterion, t0, the non-decision time sigma1, the within trial drift-rate standard deviation on the x axis, sigma2, the within trial drift-rate standard deviation on the y axis. eta1 and eta2 are two parameters related to the drift rate SD. They are usually set as 0. Random-walk Process Figure 1 was generated by the rcircle_process, the 2-D random-walk and r1d, the 1-D random-walk processes. require(ggdmc) ## random walk 2d ## Set the upper bound of simulation time and each time step as 0.1 ms tmax &lt;- 2 h &lt;- 1e-4 p.vector &lt;- c(v1=0, v2=0, a=1, t0=0, sigma1=1, sigma2=1, eta1=0, eta2=0) res0 &lt;- rcircle_process(P=p.vector, tmax=tmax, h=h) str(res0) ## List of 3 ## $ out : num [1:3, 1] 0.732 -2.551 0 ## $ xPos: num [1:20001, 1] 0 0 0.0074 -0.0104 -0.0172 ... ## $ yPos: num [1:20001, 1] 0 0 -0.0183 -0.0258 -0.0364 ... ## random walk 1d p.vector &lt;- c(v=0, a=1, z=.5, t0=0, s=1) res1 &lt;- r1d(P=p.vector, tmax=tmax, h=h) idx &lt;- sum(!is.na(res0$Xt)); idx str(res1) ## List of 3 ## $ T : num [1:20001, 1] 0e+00 1e-04 2e-04 3e-04 4e-04 5e-04 6e-04 7e-04 8e-04 9e-04 ... ## $ out: num [1:2, 1] 0.221 0 ## $ Xt : num [1:20001, 1] 0.5 0.51 0.513 0.507 0.51 ... ## Plot the traces of 1-D and 2-D diffusion processes png(filename='random_walk_2d.png', 800, 600) par(mfrow=c(1,2), pty=\"s\") plotCircle(res0$xPos[,1], res0$yPos[,1], a=1) plot(res1$T[1:idx], res1$Xt[1:idx], type='l', ylim=c(0, 1), xlab='DT (s)', ylab='Evidence') abline(h=1, lty='dotted', lwd=1.5) abline(h=0, lty='dotted', lwd=1.5) points(x=0, y=p.vector[3], col='red') dev.off() Random number generation To simulate multiple observations, rcircle allows the user to enter the number of observation via the n option. The implementation is simply a for loop running the code of the rcirle_process function repeatedly. Another useful option in rcircle is nw, which allows the user to divide the response angles into nw response categories. n &lt;- 150000 tmax &lt;- 2 h &lt;- 1e-4 p.vector &lt;- c(v1=1, v2=1, a=1, t0=0, sigma1=1, sigma2=1, eta1=0, eta2=0) ## Took ~160 s. Divide the angles evenly into 11 categories res0 &lt;- rcircle(n=n, P=p.vector, tmax=tmax, h=h, nw=11) d &lt;- data.frame(res0) names(d) &lt;- c(\"R\", \"RT\", \"A\") ## R stores the centered angles of the response category. ## A stores the actual hitting angles. # dplyr::tbl_df(d) # A tibble: 150,000 x 3 # R RT A # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 -0.286 0.116 -0.310 # 2 0.857 0.262 0.968 # 3 -0.286 0.329 -0.151 # 4 0.857 0.467 0.774 # ... # … with 149,990 more rows Joint densities of zero-drift process In the following, we simulated 50,000 observations from the zero-drift process by setting v1 and v2 to 0. n &lt;- 50000 ## 50,000 observations h &lt;- 1e-4 ## Define 0.1 ms as one time step tmax &lt;- 2 ## Define maximum decision time nw &lt;- 11 ## Divide the hitting angles into 11 categories w &lt;- 2*pi nw ## The width of each category of the hitting angles ## Define zero-drift parameter vector p.vector &lt;- c(v1=0, v2=0, a=1, t0=0, sigma1=1, sigma2=1, eta1=0, eta2=0) ## ~60 seconds res0 &lt;- rcircle(n=n, P=p.vector, tmax=tmax, h=h, nw=11) ## The simulation result is stored as a numerical matrix, so we convert it to ## an R data.frame d &lt;- data.frame(R = factor(round(res0[,1], 2)), RT = res0[,2], A = res0[,3]) Because we divide the hitting angles into 11 bins and are handeling bivariate data, we create a customerised histogram function to count the numbers of observation in each bin. Note one bin in this case is indexed by both the response times and hitting angles, so the densities, Gt, is a nw ntime matrix. ## See the end of this tutorial for the implementation of the function res1 &lt;- histogram_cddm(d, P=p.vector, nw=nw, tmax=tmax, h=h) # List of 9 # $ Theta : num [1:11] -3.142 -2.57 -1.999 -1.428 -0.857 ... # $ Mt : num [1:11] 0.499 0.514 0.51 0.5 0.511 ... # $ Pt : num [1:11] 0.161 0.158 0.156 0.158 0.163 ... # $ time_grid: num [1:20001] 0e+00 1e-04 2e-04 3e-04 4e-04 5e-04 6e-04 7e-04 8e-04 9e-04 ... # $ Gt : num [1:11, 1:20001] 1.08e-16 9.96e-17 1.06e-16 9.77e-17 1.32e-16 ... # $ Pmt : num [1:11] 1 1 1 1 1 1 1 1 1 1 ... # $ Mtscale : num [1:11, 1:20001] 1 1 1 1 1 1 1 1 1 1 ... # $ Gt_count : num [1:11, 1:20001] 0 0 0 0 0 0 0 0 0 0 ... # $ d :'data.frame': 220011 obs. of 3 variables: # ..$ R : Factor w 11 levels \"-3.14\",\"-2.57\",..: 1 1 1 1 1 1 1 1 1 1 ... # ..$ RT: num [1:220011] 0e+00 1e-04 2e-04 3e-04 4e-04 5e-04 6e-04 7e-04 8e-04 9e-04 ... # ..$ D : num [1:220011] 1.90e-16 1.09e-16 2.21e-16 3.46e-17 2.05e-16 ... Gt is the joint density matrix, with nw row and ntime column. The time grid is constructed based on the maximum simulation time, tmax and the user-defined time step, h. d is a data.frame, which rearranges the densities stored in Gt, the hitting angles stored in Theta, and the response times stored in time_grid. Probability Density Function dcircle calculates the predicted probability densities of the 2-D diffusion process. den &lt;- dcircle(d$RT, d$A, P=p.vector, tmax=tmax, kmax=50, sz=2 h, nw=50) ## Because den is a column vector. We convet it to a row vector. d$D &lt;- den[,1] The following figure show the joing densities calculated from the simulations. thetai &lt;- levels(res1$d$R) dat0 &lt;- vector(\"list\", length=nw) dat1 &lt;- vector(\"list\", length=nw) for(i in 1:nw) { tmp0 &lt;- res1$d[res1$d$R==thetai[i],] ## simulation dat0[[i]] &lt;- tmp0[order(tmp0$RT),] tmp1 &lt;- d[d$R==thetai[i],] ## predicted dat1[[i]] &lt;- tmp1[order(tmp1$RT),] } colors &lt;- RColorBrewer:::brewer.pal(9, \"Set1\") par(pty='s') plot(dat0[[1]]$RT, dat0[[1]]$D, col=\"grey60\", ylim=c(0, .4), type='l', xlab='Time(s)', ylab='Joint density', cex.lab=2, cex.axis = 1.5) lines(dat1[[1]]$RT, dat1[[1]]$D, lwd=2) for(i in 2:nw) { lines(dat0[[i]]$RT, dat0[[i]]$D, lwd=1, col=colors[i]) lines(dat1[[i]]$RT, dat1[[i]]$D, lwd=2) } dev.off() Figure 2. The joint densities of response times and response angles of zero-drift prcess. The black line shows the predicted densities. Because the simulated observations are from the zero-drift process, the hitting angles do not affect the densities. This can be seen in Figure 3, which presents the histogrames in individual subplot. ## require ggplot2 x0 &lt;- res1$d x1 &lt;- d x0$TYPE &lt;- 'Simulation' x1$TYPE &lt;- 'Prediction' tmp0 &lt;- x0[, c(\"RT\", \"D\", \"R\", \"TYPE\")] tmp1 &lt;- x1[, c(\"RT\", \"D\", \"R\", \"TYPE\")] x2 &lt;- rbind(tmp0, tmp1) p0 &lt;- ggplot(x2, aes(x=RT, y=D, colour=R)) + geom_line(aes(size=TYPE)) + scale_size_manual(values = c(1, .25) ) + xlab(\"RT(s)\") + ylab(\"Joint density\") + coord_cartesian(ylim=c(0, .4)) + facet_wrap(~R) + theme_bw(base_size=14) + theme(aspect.ratio=1, legend.position = 'none') print(p0) Figure 3. The upper panel shows the hitting-angle categories. Similarly, we compare the predicted and simulated densities of the non-zero drift 2-D diffusion process. ## Test 1, nonzero-drift simulation -------- n &lt;- 150000 tmax &lt;- 2 h &lt;- 1e-4 nw &lt;- 11 p.vector &lt;- c(v1=1, v2=1, a=1, t0=0, sigma1=1, sigma2=1, eta1=0, eta2=0) ## 179.4 s res0 &lt;- rcircle(n=n, P=p.vector, tmax=tmax, h=h, nw=nw) d &lt;- data.frame(R=factor(round(res0[,1], 2)), RT=res0[,2], A=res0[,3]) res1 &lt;- histogram_cddm(d, P=p.vector, nw=nw, tmax=tmax, h=h) x0 &lt;- res1$d thetai &lt;- levels(res1$d$R) ## Predicted density ----------------------- res2 &lt;- dcircle300(p.vector, tmax=2, kmax=50, sz=300, nw=nw) x1 &lt;- NULL for(i in 1:nw) { x1 &lt;- rbind(x1, data.frame(RT = res2$DT, A = rep(res2$R[i], 300), D = res2$Gt[i,])) } res3 &lt;- divider(x1, nw=nw) x1 &lt;- dplyr::tbl_df(res3$d) ## overwrite x1 dplyr::tbl_df(x0) dplyr::tbl_df(x1) dat0 &lt;- vector(\"list\", length=nw) dat1 &lt;- vector(\"list\", length=nw) for(i in 1:nw) { tmp0 &lt;- x0[x0$R==thetai[i],] dat0[[i]] &lt;- tmp0[order(tmp0$RT),] tmp1 &lt;- x1[x1$R==thetai[i],] dat1[[i]] &lt;- tmp1[order(tmp1$RT),] } colors &lt;- RColorBrewer:::brewer.pal(9, \"Set1\") par(pty='s') plot(dat0[[1]]$RT, dat0[[1]]$D, col=\"grey60\", ylim=c(0, 1), type='l', xlab='Time(s)', ylab='Joint density', cex.lab=2, cex.axis = 1.5) lines(dat1[[1]]$RT, dat1[[1]]$D) for(i in 2:nw) { lines(dat0[[i]]$RT, dat0[[i]]$D, lwd=1, col=colors[i]) lines(dat1[[i]]$RT, dat1[[i]]$D, col=colors[i]) } ## ggplot 2 x0$TYPE &lt;- 'Simulation' x1$TYPE &lt;- 'Prediction' tmp0 &lt;- x0[, c(\"RT\", \"D\", \"R\", \"TYPE\")] tmp1 &lt;- x2[, c(\"RT\", \"D\", \"R\", \"TYPE\")] x2 &lt;- rbind(tmp0, tmp1) p0 &lt;- ggplot(x2, aes(x=RT, y=D, colour=R)) + geom_line(aes(linetype = TYPE)) + xlab(\"DT(s)\") + ylab(\"Joint density\") + coord_cartesian(ylim=c(0, 1)) + # facet_wrap(~TYPE) + theme_bw(base_size=14) + theme(aspect.ratio=1, legend.position = 'none') Figure 4. The nonzero probabiity densities of the 2-D diffusion process. Helper functions ## This function is adpated from Smith's (2016) dirichlet1.m histogram_cddm &lt;- function(d, P, nw=11, tmax=2, h=1e-4) { # d # pvec=p.vector # nw=11 # tmax=2 # h=1e-4 v1 &lt;- P[1]; v2 &lt;- P[2]; a &lt;- P[3]; t0 &lt;- P[4] s1 &lt;- P[5]; s2 &lt;- P[6]; DT &lt;- d[,2] - P[4] A &lt;- d[,3] ## Angles w &lt;- 2*pi nw Theta &lt;- seq(-pi, pi-w, w) Thetabound &lt;- c(Theta + w 2); ## Time time_grid &lt;- seq(0, tmax, h) ntime_grid &lt;- length(time_grid); tbound &lt;- c(time_grid[1] - h 2, time_grid + h 2); Mt &lt;- Nt &lt;- Pmt &lt;- numeric(nw) Gt &lt;- Gt_count &lt;- matrix(numeric(nw*ntime_grid), nrow=nw) n &lt;- nrow(d) for (i in 1:n) { tmp0 &lt;- which( A[i] &lt;= Thetabound ) thetaindex &lt;- min( tmp0, na.rm=TRUE ) ## between -pi and Thetabound(1), pool into last if ( is.infinite(thetaindex) ) thetaindex &lt;- 1 tindex &lt;- min( max( which( DT[i] &gt; tbound) ), ntime_grid) ## Pool into last bin. Nt[thetaindex] = Nt[thetaindex] + 1; Mt[thetaindex] = Mt[thetaindex] + DT[i]; Gt[thetaindex,tindex] &lt;- Gt[thetaindex, tindex] + 1; Gt_count[thetaindex,tindex] &lt;- Gt_count[thetaindex, tindex] + 1; } Mt &lt;- Mt Nt; ## Average time per theta group Pt &lt;- Nt n; ## Gt densities filter &lt;- cos( seq(-pi 2, pi 2, .025)) filter &lt;- filter sum(filter); # Normalize mass in filter for (i in 1:nw) { Gtfi &lt;- pracma::conv(Gt[i,], filter) ## %Gt(i,:) = Gtfi(1:szt). (Nt(i) * h +eps); % conditional Gt[i,] &lt;- Gtfi[1:ntime_grid] (n * h); ## % joint density } for (i in 1:nw) { Pmt[i] &lt;- exp(a*cos(Theta[i])*v1 s1^2 + a*sin(Theta[i])*v2 s2^2) } Commonscale &lt;- exp(-0.5 * (v1^2 s1^2 + v2^2 s2^2) * time_grid); # Multiply theta-dependent drift term by invariant time-dependent term Mtscale &lt;- as.matrix(Pmt) %*% Commonscale; Pt &lt;- Pt w; # % To make into a density estimate. thidx &lt;- factor(round(Theta, 2)) ## ------------------------------------------------------------------------## ## Note that to get the joint density of hitting ## angles and resposne times, the densities were divided by w ## ------------------------------------------------------------------------## x0 &lt;- NULL for(i in 1:nw) { tmp0 &lt;- data.frame(R = thidx[i], RT = time_grid+t0, D = Gt[i,] w) x0 &lt;- rbind(x0, tmp0) } return(list(Theta=Theta, Mt=Mt, Pt=Pt, time_grid=time_grid, Gt=Gt, Pmt=Pmt, Mtscale=Mtscale, Gt_count=Gt_count, d=x0)) }"
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-ddm": {
						"id": "cognitive-model-ddm",
						"title": "Diffusion Decision Model",
						"category": "",
						"url": " /cognitive-model/ddm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-lba": {
						"id": "cognitive-model-lba",
						"title": "LBA Model",
						"category": "",
						"url": " /cognitive-model/lba/",
						"content": "This lesson demonstrates how to control the “golem” (McElreath, 2016), the canonical linear ballistic accumulation (LBA) model (Brown &amp; Heathcote, 2008). Please refer to the LBA paper for more details. Here we focus only on how to use this model in the context of Bayesian MCMC. The LBA model posits a latent matching (M) factor and a response factor (R) on top of regular experimental factors. For most people who are not familiar with the LBA model, the two factors are unfortunately confusing. Also for the modelling technicalities, the LBA model must fix one of the parameters in the mean_v or sd_v in at least one design cell. This is to serve as scaling purpose, similar to the moment-to-moment variability in the decision diffusion model. For example, the following code fixes sd_v = 1. require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"M\", sd_v = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2)), factors = list(S = c(\"s1\", \"s2\")), constants = c(st0 = 0, sd_v = 1), responses = c(\"r1\", \"r2\"), type = \"norm\") In the above model, I define only one experimental factor, S, for stimulus, which has two levels, s1 and s2. The accuracy, reflected by the M factor, is mapped by s1 = 1 and s2 = 2, meaning that a correct response for s1 (or s2) stimulus is response r1 (or r2) and an error response for s1 (or s2) stimulus is r2 (or r1). Below I use simulate to generate an example data set. p.vector &lt;- c(A = .75, B = 1.25, t0 = .15, mean_v.true = 2.5, mean_v.false = 1.5) dat &lt;- simulate(model, nsim = 30, ps = p.vector) dmi &lt;- BuildDMI(dat, model) ## DMI stands for data model instance. dplyr::tbl_df(dmi) # A tibble: 60 x 3 ## S R RT ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 s1 r1 0.608 ## 2 s1 r1 0.972 ## 3 s1 r2 0.817 ## 4 s1 r1 0.718 ## 5 s1 r1 0.618 ## 6 s1 r1 1.17 ## 7 s1 r1 0.730 ## 8 s1 r2 0.727 ## 9 s1 r1 0.711 ## 10 s1 r1 0.688 I use an imaginary experiment with a design of one binary stimulus factor (S), such as left vs. right motion random dots. match.map = list(M = list(left = “LEFT”, right = “RIGHT”)), responses = c(“LEFT”, “RIGHT”), In another tutorial, I will fit the model to an empirical data set (Cox &amp; Criss, 2017) to demonstrate fitting HLBA model. The above match.map code shows the usage of strings, instead of numbers. The “left” and “LFET” could mean the random dots moving left and a left response. From an experimenter’s perspective, this imaginary experiment only has one stimulus (S) factor, which has two levels, random dots moving towards right and moving towards left as defined below. factors = list(S = c(“left”, “right”)), Below is the complete model definition. model &lt;- BuildModel(p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"M\", sd_v = \"M\", st0 = \"1\"), constants = c(st0 = 0, sd_v.false = 1, mean_v.false = 0), match.map = list(M = list(left = \"LEFT\", right = \"RIGHT\")), factors = list(S = c(\"left\", \"right\")), responses = c(\"LEFT\", \"RIGHT\"), type = \"norm\") The first option in the BuildModel function, p.map indicates the experimental design. In this example, I assumed the S factor does not affect any LBA latent variables operations. Therefore, I entered p.map as: p.map = list(A = “1”, B = “1”, t0 = “1”, mean_v = “M”, sd_v = “M”, st0 = “1”), The notations of the parameters in the LBA model refer to: A, the variability of the starting point, B, the travelling distance of accumulators, b, (not shown in the p.map) the decision threshold, t0, the non-decision time mean_v, the means of the drift rates, sd_v, the standard deviations of the drift rates, st0, the variability of the non-decision time component. The A = “1”, for instance, indicates that the variability of the starting point is fit by the intercept, 1. The M factor, because it is defined by the LBA model as a latent factor, you still see it in the p.map. It indicates there are two drift rate means in mean_v, one for each accumulator: the accumulator for the correct matched responses and the accumulator for the error mismatched responses. Similarly, this is also applied to the standard deviation of the drift rates, sd_v. The only effect in the model defined in the p.map is that the drift rate for a correct response is larger than that for an error response. This is an assumption based on, in general, psychological literature. This is artificially set at constants = c(st0 = 0, sd_v.false = 1, mean_v.false = 0), which enforces mean_v.false = 0. This is to presume (also frequent observed phenomenon) that manifested accuracy rate should usually be greater than chance (50%). mean_v.false stands for the mean of the drift rate of the error (false) accumulator. Because it is always zero, the correct drift rate, mean_v.true, if drawn from a truncated normal distribution bounded by 0 and Inf, will always be larger than the error drift rate. Demo 1 Fast and error prone performance This demonstration shows how I control the LBA golem to simulate fast and error prone RT distributions. I defined a true parameter vector, defining sd_v.true = (0.66), which is smaller than sd_v.false = 1. This seems often seen in empirical data. pvec1 &lt;- c(A = 1, B = 0, t0 = .2, mean_v.true = 1, sd_v.true = 0.66) dat1 &lt;- simulate(model, ps = pvec1, nsim = 1e4) dmi1 &lt;- BuildDMI(dat1, model) In the following, I used functions in dplyr to print out the mean response times and accuracy for each stimulus types. The results showed: Error and correct responses have similar average RTs. Stimulus type 1 and stimulus type 2 have similar rates of correctness. library(dplyr) ## dplyr library(dplyr) dat1$C &lt;- dat1$S == tolower(dat1$R) d &lt;- dplyr::tbl_df(dat1) ## Print average RTs and accuracy rates for each condition group_by(d, S, C) %&gt;% summarize(m = mean(RT)) ## A tibble: 4 x 3 ## Groups: S [?] ## S C m ## &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 left FALSE 0.624 ## 2 left TRUE 0.645 ## 3 right FALSE 0.639 ## 4 right TRUE 0.634 group_by(d, S, C) %&gt;% summarize(m = length(RT) 1e4) ## A tibble: 4 x 3 ## Groups: S [?] ## S C m ## &lt;fct&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 left FALSE 0.391 ## 2 left TRUE 0.609 ## 3 right FALSE 0.392 ## 4 right TRUE 0.608 ## data.table library(data.table) DT &lt;- data.table(dat1) ## Print average RTs and accuracy for each condition DT[, .(MRT = round(mean(RT), 3)), .(S, C)] ## S C MRT ## 1: left TRUE 0.645 ## 2: left FALSE 0.624 ## 3: right TRUE 0.634 ## 4: right FALSE 0.639 prop &lt;- DT[, .N, .(S, C)] prop[, NN := sum(N), .(S)] prop[, acc := round(N NN, 2)] ## Print accuracy rates for each condition prop ## S C N NN acc ## 1: left TRUE 6092 10000 0.61 ## 2: left FALSE 3908 10000 0.39 ## 3: right TRUE 6079 10000 0.61 ## 4: right FALSE 3921 10000 0.39"
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-lba3": {
						"id": "cognitive-model-lba3",
						"title": "Three-accumulator LBA Model",
						"category": "",
						"url": " /cognitive-model/lba3/",
						"content": "We have striven to minimize the number of errors. However, we canot guarantee the note is 100% accurate. We have updated the predict_one function for S4 class (19 01 2020). This update is tested on Ubuntu 18.04.3 LTS (Intel® Core™ i5-8400 CPU @ 2.80GHz × 6; Memory: 7.7 GB) This is a quick note for fitting 3-accumulator LBA model. First, some pre-analysis set up work. ## version 0.2.7.8 ## devtools::install_github(\"yxlin ggdmc\") loadedPackages &lt;-c(\"ggdmc\", \"data.table\", \"ggplot2\", \"gridExtra\", \"ggthemes\") sapply(loadedPackages, require, character.only=TRUE) ## A function for generating posterior predictive samples for one participant fit predict_one &lt;- function(object, npost = 100, xlim = NA, seed = NULL) { facs &lt;- attr(object@dmi@model, \"factors\"); fnames &lt;- names(facs); ns &lt;- table( object@dmi@data[, fnames], dnn = fnames) nsample &lt;- object@nchain * object@nmc; pnames &lt;- object@pnames; thetas &lt;- matrix(aperm(object@theta, c(3,2,1)), ncol = object@npar) colnames(thetas) &lt;- pnames if (is.na(npost)) stop(\"Must specify npost!\") use &lt;- sample(1:nsample, npost, replace = FALSE); npost &lt;- length(use) posts &lt;- thetas[use, ] ntrial &lt;- sum(ns) v &lt;- lapply(1:npost, function(i) { simulate(object@dmi@model, nsim = ns, ps = posts[i,], seed = seed) }) out &lt;- data.table::rbindlist(v) reps &lt;- rep(1:npost, each = ntrial) out &lt;- cbind(reps, out) if (!any(is.na(xlim))) out &lt;- out[RT &gt; xlim[1] &amp; RT &lt; xlim[2]] attr(out, \"data\") &lt;- object@dmi return(out) } In this example, we assumed three accumulators corresponding to three responses. Let’s say they are “Word”, “Nonword”, and “Pseudo-word”. They are coded respectively as W, N and P. This is to assume we had run some (visual) lexical-decision experiments, instructing participants to decide whether a stimulus is a word, a non-word, or a make-up word. The three types of stimuli are coded as ww, nn and pn. model &lt;- BuildModel( p.map = list(A = \"1\", B = \"1\", t0 = \"1\", mean_v = \"R\", sd_v = \"1\", st0 = \"1\"), match.map = list(M = list(ww = \"W\", nn = \"N\", pn = \"P\")), factors = list(S = c(\"ww\", \"nn\", \"pn\")), constants = c(st0 = 0, sd_v = 1), responses = c(\"W\", \"N\", \"P\"), type = \"norm\") ## Parameter vector names are: ( see attr(,\"p.vector\") ) ## [1] \"A\" \"B\" \"t0\" \"mean_v.W\" \"mean_v.N\" \"mean_v.P\" ## ## Constants are (see attr(,\"constants\") ): ## st0 sd_v ## 0 1 ## ## Model type = norm (posdrift = TRUE ) Firstly, as usual, we conducted a small recovery study. That is, we designated a parameter vector with specific values and on the basis of this particular parameter vector, we simulated a data set and fit such data set with the model to see if it could recover the values reasonably well. For now, we will show only the recovery study. p.vector &lt;- c(A = 1.25, B = .25, t0 = .2, mean_v.W = 2.5, mean_v.N = 1.5, mean_v.P = 1.2) ## ggdmc adapts print function to help inspect model print(model) ## The model array is huge ## W ## A B t0 mean_v.W mean_v.N mean_v.P sd_v st0 ## ww.W TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## nn.W TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## pn.W TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## ww.N TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## nn.N TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## pn.N TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## ww.P TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## nn.P TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## pn.P TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## N ## A B t0 mean_v.W mean_v.N mean_v.P sd_v st0 ## ww.W TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## nn.W TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## pn.W TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## ww.N TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## nn.N TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## pn.N TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## ww.P TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## nn.P TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## pn.P TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## P ## A B t0 mean_v.W mean_v.N mean_v.P sd_v st0 ## ww.W TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## nn.W TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## pn.W TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## ww.N TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## nn.N TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## pn.N TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## ww.P TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## nn.P TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## pn.P TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## The model object carries this many attributes ## Attributes: ## [1] \"dim\" \"dimnames\" \"all.par\" \"p.vector\" \"par.names\" \"type\" \"factors\" ## [8] \"responses\" \"constants\" \"posdrift\" \"n1.order\" \"match.cell\" \"match.map\" \"is.r1\" ## [15] \"class\" print(model, p.vector) ## The following is how ggdmc allocates the parameters to each accumulator. ## [1] \"ww.W\" ## A b t0 mean_v sd_v st0 ## 1 1.25 1.5 0.2 2.5 1 0 ## 2 1.25 1.5 0.2 1.5 1 0 ## 3 1.25 1.5 0.2 1.2 1 0 ## [1] \"nn.W\" ## A b t0 mean_v sd_v st0 ## 1 1.25 1.5 0.2 2.5 1 0 ## 2 1.25 1.5 0.2 1.5 1 0 ## 3 1.25 1.5 0.2 1.2 1 0 ## [1] \"pn.W\" ## A b t0 mean_v sd_v st0 ## 1 1.25 1.5 0.2 2.5 1 0 ## 2 1.25 1.5 0.2 1.5 1 0 ## 3 1.25 1.5 0.2 1.2 1 0 ... ## To see what other options in the simulate function ## ?ggdmc:::simulate nsim &lt;- 2048 dat &lt;- simulate(model, nsim = nsim, ps = p.vector) We used data.table to inspect the data frame. This makes no difference when the data set is small. d &lt;- data.table(dat) dmi &lt;- BuildDMI(dat, model) ## Check the factor levels sapply(d[, .(S,R)], levels) ## S R ## [1,] \"ww\" \"W\" ## [2,] \"nn\" \"N\" ## [3,] \"pn\" \"P\" To inspect the response time distributions, we designated the response proportions for each of the response types. ww1 &lt;- d[S == \"ww\" &amp; R == \"W\" &amp; RT &lt;= 10, \"RT\"] ww1 &lt;- d[S == \"ww\" &amp; R == \"W\" &amp; RT &lt;= 10, \"RT\"] ww2 &lt;- d[S == \"ww\" &amp; R == \"N\" &amp; RT &lt;= 10, \"RT\"] ww3 &lt;- d[S == \"ww\" &amp; R == \"P\" &amp; RT &lt;= 10, \"RT\"] nn1 &lt;- d[S == \"nn\" &amp; R == \"W\" &amp; RT &lt;= 10, \"RT\"] nn2 &lt;- d[S == \"nn\" &amp; R == \"N\" &amp; RT &lt;= 10, \"RT\"] nn3 &lt;- d[S == \"nn\" &amp; R == \"P\" &amp; RT &lt;= 10, \"RT\"] pn1 &lt;- d[S == \"pn\" &amp; R == \"W\" &amp; RT &lt;= 10, \"RT\"] pn2 &lt;- d[S == \"pn\" &amp; R == \"N\" &amp; RT &lt;= 10, \"RT\"] pn3 &lt;- d[S == \"pn\" &amp; R == \"P\" &amp; RT &lt;= 10, \"RT\"] xlim &lt;- c(0, 5) par(mfrow=c(1, 3), mar = c(4, 4, 0.82, 1)) hist(ww1$RT, breaks = \"fd\", freq = TRUE, xlim = xlim, main='Word', xlab='RT(s)', cex.lab=1.5) hist(ww2$RT, breaks = \"fd\", freq = TRUE, add = TRUE, col = \"lightblue\") hist(ww3$RT, breaks = \"fd\", freq = TRUE, add = TRUE, col = \"orange\") hist(nn1$RT, breaks = \"fd\", freq = TRUE, xlim = xlim, main='Non-word', xlab='RT(s)', ylab='', cex.lab=1.5) hist(nn2$RT, breaks = \"fd\", freq = TRUE, add = TRUE, col = \"lightblue\") hist(nn3$RT, breaks = \"fd\", freq = TRUE, add = TRUE, col = \"orange\") hist(pn1$RT, breaks = \"fd\", freq = TRUE, xlim = xlim, main='Pseudo-word', xlab='RT(s)', ylab='', cex.lab=1.5) hist(pn2$RT, breaks = \"fd\", freq = TRUE, add = TRUE, col = \"lightblue\") hist(pn3$RT, breaks = \"fd\", freq = TRUE, add = TRUE, col = \"orange\") par(mfrow=c(1, 1)) Prior distribution p.prior &lt;- BuildPrior( dists = c(\"tnorm\", \"tnorm\", \"beta\", \"tnorm\", \"tnorm\", \"tnorm\"), p1 = c(A = .3, B = 1, t0 = 1, mean_v.W = 1, mean_v.N = 0, mean_v.P = .1), p2 = c(1, 1, 1, 3, 3, 3), lower = c(0, 0, 0, NA, NA, NA), upper = c(NA, NA, 1, NA, NA, NA)) ## Visually check the prior distributions plot(p.prior, ps = p.vector) Sampling The default number of iteration is 200 for StartNewsamples function. ## ?run to see add and other options in run function fit0 &lt;- StartNewsamples(dmi, p.prior, thin = 2) fit &lt;- run(fit0, thin = 2, block = FALSE) ## gelman function also provide subchain option. ## Note the method to call this option is different res &lt;- gelman(fit, verbose = TRUE, subchain = 1:3) ## Calculate chains: 1 2 3 ## Multivariate psrf: ## Point est. Upper C.I. ## A 1.02 1.08 ## B 1.00 1.01 ## t0 1.01 1.01 ## mean_v.W 1.01 1.03 ## mean_v.N 1.01 1.03 ## mean_v.P 1.01 1.02 ## By convention, most Bayesian inference checks 3 or 4 chains p1 &lt;- plot(fit) p2 &lt;- plot(fit, pll=F, den=T) p3 &lt;- plot(fit, subchain = TRUE) p4 &lt;- plot(fit, pll=F, den=T, subchain = TRUE) png(file = \"LBA3A-checks.png\", 800, 600) grid.arrange(p1, p2, p3, p4) dev.off() es &lt;- effectiveSize(fit, verbose = TRUE) ## A B t0 mean_v.W mean_v.N mean_v.P ## 843.04 838.92 863.98 828.97 840.93 894.25 est &lt;- summary(fit, ps = p.vector, verbose = TRUE, recovery = TRUE) ## Recovery summarises only default quantiles: 2.5% 25% 50% 75% 97.5% ## A B mean_v.N mean_v.P mean_v.W t0 ## True 1.2500 0.2500 1.5000 1.2000 2.5000 0.2000 ## 2.5% Estimate 1.1516 0.2153 1.3745 1.0190 2.3058 0.1900 ## 50% Estimate 1.2724 0.2495 1.5667 1.2116 2.5075 0.2000 ## 97.5% Estimate 1.4053 0.2920 1.7573 1.4070 2.7250 0.2085 ## Median-True 0.0224 -0.0005 0.0667 0.0116 0.0075 0.0000 The posterior predictive figure shows the data and posterior predictions are consistent, confirming the model does work well. pp &lt;- predict_one(fit, xlim = c(0, 5)) original_data &lt;- fit@dmi@data dplyr::tbl_df(original_data) d &lt;- data.table(original_data) ## A different way to check data frame dplyr::tbl_df(original_data) d &lt;- data.table(original_data) ## Response proportions d[, .N, .(S)] d[, .N 100, .(S, R)] ## Score for the correct and error response dat$C &lt;- ifelse(dat$S == \"ww\" &amp; dat$R == \"W\", \"O\", ifelse(dat$S == \"nn\" &amp; dat$R == \"N\", \"O\", ifelse(dat$S == \"pn\" &amp; dat$R == \"P\", \"O\", ifelse(dat$S == \"ww\" &amp; dat$R == \"N\", \"X\", ifelse(dat$S == \"ww\" &amp; dat$R == \"P\", \"X\", ifelse(dat$S == \"nn\" &amp; dat$R == \"W\", \"X\", ifelse(dat$S == \"nn\" &amp; dat$R == \"P\", \"X\", ifelse(dat$S == \"pn\" &amp; dat$R == \"N\", \"X\", ifelse(dat$S == \"pn\" &amp; dat$R == \"W\", \"X\", NA))))))))) pp$C &lt;- ifelse(pp$S == \"ww\" &amp; pp$R == \"W\", \"O\", ifelse(pp$S == \"nn\" &amp; pp$R == \"N\", \"O\", ifelse(pp$S == \"pn\" &amp; pp$R == \"P\", \"O\", ifelse(pp$S == \"ww\" &amp; pp$R == \"N\", \"X\", ifelse(pp$S == \"ww\" &amp; pp$R == \"P\", \"X\", ifelse(pp$S == \"nn\" &amp; pp$R == \"W\", \"X\", ifelse(pp$S == \"nn\" &amp; pp$R == \"P\", \"X\", ifelse(pp$S == \"pn\" &amp; pp$R == \"N\", \"X\", ifelse(pp$S == \"pn\" &amp; pp$R == \"W\", \"X\", NA))))))))) dat0 &lt;- dat dat0$reps &lt;- NA dat0$type &lt;- \"Data\" pp$reps &lt;- factor(pp$reps) pp$type &lt;- \"Simulation\" combined_data &lt;- rbind(dat0, pp) dplyr::tbl_df(combined_data) p1 &lt;- ggplot(combined_data, aes(RT, color = reps, size = type)) + geom_freqpoly(binwidth = .10) + scale_size_manual(values = c(1, .3)) + scale_color_grey(na.value = \"black\") + ylab(\"Count\") + facet_grid(S ~ C) + theme_bw(base_size = 16) + theme(strip.background = element_blank(), legend.position=\"none\") png(file = \"LBA3A.png\", 800, 600) print(p1) dev.off() Extending to four or more accumulators Here is a barebone template of 4 stimuli mapping to 4 responses. This is a 15-parameter model; thus, one should expect it would take up a lot of computation time, especially in fitting hierarchical model. It would save time, if one fits fixed-effect model first, using multiple cores. require(ggdmc) ## Assume four stimuli that have a shape and color (blue_diamond, blue_heart, ## green_diamond, and green_heart) (courtesy of davidt0x) ## ## I assume a difficulty hierarchy (with no theoretical basis) (from easy to hard): green_diamond (e1) &gt; green_heart (e2) &gt; blue_diamond (e3) &gt; blue_heart (e4). I also assume the easier stimulus, the higher its drift rate is and the more variable (hence the higher value) its drift rate standard deviation would be. model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = c(\"S\", \"M\"), sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(\"blue_diamond\" = \"BD\", \"blue_heart\" = \"BH\", \"green_diamond\" = \"GD\", \"green_heart\" = \"GH\")), factors = list(S = c(\"blue_diamond\", \"blue_heart\", \"green_diamond\", \"green_heart\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"BD\", \"BH\", \"GD\", \"GH\"), type = \"norm\") pop.mean &lt;- c(A=.4, B.BD=.5, B.BH=.6, B.GD=.7, B.GH=.8, t0=.3, mean_v.blue_diamond.true = 1.5, mean_v.blue_heart.true = 1.0, mean_v.green_diamond.true = 2.5, mean_v.green_heart.true = 2.0, mean_v.blue_diamond.false = .20, mean_v.blue_heart.false = .25, mean_v.green_diamond.false = .10, mean_v.green_heart.false = .15, sd_v.true = .25) pop.scale &lt;-c(A=.1, B.BD=.1, B.BH=.1, B.GD=.1, B.GH=.1, t0=.05, mean_v.blue_diamond.true = .2, mean_v.blue_heart.true = .2, mean_v.green_diamond.true = .2, mean_v.green_heart.true = .2, mean_v.blue_diamond.false = .2, mean_v.blue_heart.false = .2, mean_v.green_diamond.false = .2, mean_v.green_heart.false = .2, sd_v.true = .1) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", model@npar), p1 = pop.mean, p2 = pop.scale, lower = c(0,0,0,0,0, .05, NA,NA,NA,NA, NA,NA,NA,NA, 0), upper = c(NA,NA,NA,NA,NA, 1, NA,NA,NA,NA, NA,NA,NA,NA, NA)) ## plot(pop.prior) ## Simulate some data ---------- ## Assume 12 participants, each contributing 30 trials per condition. dat &lt;- simulate(model, nsub = 12, nsim = 30, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", model@npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,0,0,0,0, .05, NA,NA,NA,NA, NA,NA,NA,NA, 0), upper = c(NA,NA,NA,NA,NA, 1, NA,NA,NA,NA, NA,NA,NA,NA, NA)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", model@npar), p1 = pop.mean, p2 = c(1,1,1,1,1, 1, 2,2,2,2, 2,2,2,2, 2), lower = c(0,0,0,0,0, .05, NA,NA,NA,NA, NA,NA,NA,NA, 0), upper = c(NA,NA,NA,NA,NA, 1, NA,NA,NA,NA, NA,NA,NA,NA, NA)) plot(p.prior, ps=ps) plot(mu.prior, ps = pop.mean) sigma.prior &lt;- BuildPrior( dists = rep(\"unif\", model@npar), p1 = c(A = 0, B.BD = 0, B.BH = 0, B.GD=0, B.GH=0, t0 = 0, mean_v.blue_diamond.true = 0, mean_v.blue_heart.true = 0, mean_v.green_diamond.true = 0, mean_v.green_heart.true = 0, mean_v.blue_diamond.false = 0, mean_v.blue_heart.false = 0, mean_v.green_diamond.false = 0, mean_v.green_heart.false = 0, sd_v.true = 0), p2 = rep(5, model@npar)) ## plot(sigma.prior, ps=pop.scale) ## Sampling ------------- priors &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior) ## Enter only the participant-level prior distribution will render the function to ## run fixed-effect model ## Use nmc = 10 to estimate how much time would take. ## 42.22 fit0 &lt;- StartNewsamples(dmi, prior=p.prior, block = FALSE, ncore=12) ## Fixed-effect model fit fit &lt;- run(fit0, ncore = 12, block = FALSE) hfit0 &lt;- StartNewsamples(dmi, prior=priors) ## Random-effect model fit hfit &lt;- run(hfit0) ## ncore has no effect in hierarchical fit ## Model diagnoses plot(fit) plot(fit, subchain=TRUE, nsubchain=3) plot(fit, subchain=TRUE, nsubchain=2) res &lt;- gelman(fit, verbose = TRUE) res &lt;- gelman(fit, verbose = TRUE, subchain=1:4) res &lt;- gelman(fit, verbose = TRUE, subchain=5:8) res &lt;- gelman(fit, verbose = TRUE, subchain=9:12) ## Check if parameter recovery well in fixed-effect model fit est0 &lt;- summary(fit, recovery = TRUE, ps = ps, verbose =TRUE) How to fix array dimension inconsistency If data were stored by a previous version of ggdmc or by DMC, their arraies are arranged differently as noted here. The following is one convenient way to transpose them. ## First make sure they are indeed needed to be transposed dim(fit0@theta) dim(fit0@summed_log_prior) dim(fit0@log_likelihoods) dim(fit0$theta) dim(fit0$summed_log_prior) dim(fit0$log_likelihoods) ## Use aperm and t to transpose arrays and matrices fit0@theta &lt;- aperm(fit0@theta, c(2, 1, 3)) fit0@summed_log_prior &lt;- t(fit0@summed_log_prior) fit0@log_likelihoods &lt;- t(fit0@log_likelihoods) ## Make the new object a posterior class class(fit0) &lt;- c(\"posterior\")"
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-lca": {
						"id": "cognitive-model-lca",
						"title": "LCA Model",
						"category": "",
						"url": " /cognitive-model/lca/",
						"content": "The code here needs the LCA and C++-based PDA modules This tutorial demonstrates the method of conducting maximum likelihood parameter estimation for the leaky competing accumulator model. You will need the subplex routines for optimization, because I use the PDA to construct the simulated PDF of the LCA model. The simulated PDF is an approximation of analytic PDF, so sPDF is noisy. The subplex is designed to handle such situation. The package can be downloaded from https: github.com kingaa subplex or CRAN. rm(list = ls()) setwd('~ Documents LCA5 tests Group3 ') ## load(\"LCA1S_MLE_1e2_subplex.RData\") ## load(\"LCA1S_MLE_1e3_subplex.RData\") require(ggdmc); require(subplex) model &lt;- BuildModel( p.map = list(kappa = \"1\", beta = \"1\", Z=\"1\", s = \"1\", t0 = \"1\", I = \"M\", x0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2, s3=3)), factors = list(S = c(\"s1\", \"s2\", \"s3\")), constants = c(s = .1), responses = c(\"r1\", \"r2\", \"r3\"), type = \"lca\") p.vector &lt;- c(kappa=1.15, beta=1, Z=0.5, t0=.200, I.true=1.2, I.false=1, x0 =.15) nsim &lt;- ntrial &lt;- 1e2 ## nsim &lt;- ntrial &lt;- 1e3 ## I use the seed option to make sure I always replicate the result. dat &lt;- simulate(model, nsim = ntrial, ps = p.vector, seed = 123) dmi &lt;- BuildDMI(dat, model) d &lt;- data.table::data.table(dat) ## This is to create a column in the data frame to indicate ## correct and error responses. ## sapply(d[, .(S,R)], levels) dmi$C &lt;- ifelse(dmi$S == \"s1\" &amp; dmi$R == \"r1\", TRUE, ifelse(dmi$S == \"s2\" &amp; dmi$R == \"r2\", TRUE, ifelse(dmi$S == \"s3\" &amp; dmi$R == \"r3\", TRUE, ifelse(dmi$S == \"s1\" &amp; dmi$R == \"s3\", FALSE, ifelse(dmi$S == \"s1\" &amp; dmi$R == \"r2\" ,FALSE, ifelse(dmi$S == \"s2\" &amp; dmi$R == \"r1\", FALSE, ifelse(dmi$S == \"s2\" &amp; dmi$R == \"r3\", FALSE, ifelse(dmi$S == \"s3\" &amp; dmi$R == \"r1\", FALSE, ifelse(dmi$S == \"s3\" &amp; dmi$R == \"r2\", FALSE, NA))))))))) prop.table(table(dmi$C)) ## The maximum (log) likelihoods ## den &lt;- likelihood(p.vector, dmi) ## sum(log(den)) objective_fun &lt;- function(par, data) { den &lt;- likelihood(par, data) return(-sum(log(den))) } init_par &lt;- runif(length(p.vector)) init_par[4] &lt;- runif(1, 0, min(dmi$RT)) names(init_par) &lt;- names(p.vector) ## Note the LCA PDF was calculated by using on PDA method (nsim = 16384) ## 8.1 hrs for 1e2 observations on Intel i5-6200U ## 5.14 hrs on Intel i7 res &lt;- subplex(par = init_par, fn = objective_fun, data = dmi) str(res) round(res$par, 2) ## remember to check res$convergence save(res, dat, p.vector, file = \"LCA1S_MLE_1e2_subplex.RData\") save(res, dat, p.vector, file = \"LCA1S_MLE_1e3_subplex.RData\") ## kappa beta Z t0 I.true I.false x0 ## True 1.15 1 0.5 0.20 1.2 1 .15 ## 1e2 0.77 0.33 0.64 0.06 0.83 0.83 0.33 ## 1e3 0.33 0.92 0.37 0.32 0.53 0.53 0.14"
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-pddm": {
						"id": "cognitive-model-pddm",
						"title": "PDDM",
						"category": "",
						"url": " /cognitive-model/pddm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-plba": {
						"id": "cognitive-model-plba",
						"title": "PLBA Model",
						"category": "",
						"url": " /cognitive-model/plba/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"cognitive-model-pm": {
						"id": "cognitive-model-pm",
						"title": "PM Model",
						"category": "",
						"url": " /cognitive-model/pm/",
						"content": "Disclaimer: This tutorial is to fit Strickland et al’s (2018) PM model. For any questions regarding the model, please contact luke.strickland@uwa.edu.au When we mention memory, say a childhood event happening in the past, we are often talking about the retrospective memory, our memory for the past. Prospective memory, on the other hand, refers to we memorize something in order to do it in the future. For example, we set a reminder in the calender in our mobile phone to remind ourselves to do weekly shopping, say every Friday evening. When the reminder chimes, we then associate the reminder chime with the memory of “time to do the shopping. The prospective memory paradigm is a cognitive task testing such memory. It engages participants in some ongoing tasks with two basic choices. Take the lexical decision-making task (Wagenmakers, Ratcliff, Gomez, &amp; McKoon, 2008) as an example, the choices are word vs. non-word. In this tutorial, we fit the prospective memory model to a simulated data set. In particular, we use a semi-factorial design from the Stricland et al (2018). In addition to the two basic choices, word vs. non-word, the PM paradigms require participants to remember a third type of stimuli associated with a third response. This third type of stimuli is PM targets. For example, in addition to the typical word and non-word stimuli, participants occassionally were presented with a word describing an animal, like badger, otter, dolphin, wallaby, and so on, and the participants was instructed to choose the third response for the animal words. Experimental Design The semi-factorial design we illustrate here tested two factors. The first factor is the stimulus factor, which has three levels, word, non-word and PM target. The second factor is the PM factor, which has three conditions. Condition 1 is the ‘control’ condition, in which participants responded to a typical 2AFC lexical decision-making task. Condition 2 is a ‘focal’ PM condition, with three stimulus types - word, non-word, and PM target, each associated with three separate response types. Comparing to the third condition described later, focal PM targets are easier to detect in the context of the ongoing task. For examine, the PM targets in a focal PM condition could be dog, cat, lion, tiger, panda, kangaroos and so on, those animals that are often mentioned. Condition 3 is a ‘non-focal’ PM condition, again with three types of stimuli - word, non-word, and PM target, each corresponding to three responses. Comparing to the focal condition, non-focal PM targets are more difficult to detect. Again, continuing with the animal example, non-focal PM target could be wombat, cheetah, echidna, devil, solenodon, and so on, those animals that are less frequently mentioned in everyday dialect. In summary, factor 1, denoted as S, is a within-block manipulation of three-level stimulus type. Factor 2 is a between-block manipulation of three-level prospective memory. The three factor 2 levels are (1) no requirment to engage prospective memory, (2) easy prospective memory and (3) hard prospective memory. We denoted this factor as cond. In addition to the two factor, to determine whether a response is hit, correct rejection, false alarm or miss, we use a resposne factor, denoted as R, which has three levels, word, nonword and PM responses. S, non-word (n), word (w), PM (p) cond, focal (F), non-focal (H), control (C). R, nonword response (N), word response (W), PM response (P). Note we differentiate the upper- and the lower-case letters. We assume an accumulator model where participants swap between two- and three-accumulator architectures. The model specification results in an incomplete crossing of the two factors, as there was no PM stimuli in the control blocks of trials. Table 1. Semi-factorial design.   w n p C X X   F X X X H X X X To set up a model object, we first create a list, named FR, pooling all three factors together. FR &lt;- list(S = c(\"n\",\"w\",\"p\"), cond = c(\"C\",\"F\",\"H\"), R = c(\"N\",\"W\",\"P\")) In prospective memory paradigms, we define the false alarms with regard to the PM responses as participants commit a PM response on a non-PM trial. We observe this type of false alarm are rare, and thereby the drift rate parameter associated with the PM-false-alarm accumulator are not constrained by much data. Thus, it is a good idea to pool those rates into one PM false alarm parameter (fa). Table 2. Focal and non-focal conditions. The signal-detection categorization with regard to PM targets. O and X represents correct and incorrect responses. These two categories are correct rejections with regard to PM targets. R S w n p W O X miss N X O miss P fa fa hit To establish the relationship in Table 2 for the three PM conditions, we create a string vector, storing the factor levels. The fa label represents false alarms. We use a trick to model the situation that the control condition has no PM targets and thereby participants would not even contemplate a PM response. Therefore, it makes sense to assume in the control blocks, participants engage a two-accumulator decision-making process, rather than 3-accumulator process, which possibly happens in the two PM conditions. We create a FAKERATE label to signify this nuanced modelling approach. lev &lt;- c(\"CnN\",\"CwN\", \"CnW\",\"CwW\", \"FnN\",\"FwN\",\"FpN\", \"FnW\",\"FwW\",\"FpW\", \"fa\",\"FpP\", \"HnN\",\"HwN\",\"HpN\", \"HnW\",\"HwW\",\"HpW\", \"HpP\", \"FAKERATE\") Table 3-1. Focal condition, using lev labels. R S w n p W FwW FnW FpW N FwN FnN FpN P fa fa FpP Table 3-2. Non-focal condition, using lev labels. R S w n p W HwW HnW HpW N HwN HnN HpN P fa fa HpP Table 3-3. Control condition, using lev labels. R S w n W CwW CnW N CwN CnN Secondly, we use the MakeEmptyMap function to create a NA vector. Each of the elements in the vector is lablled by the 27 full-crossed factorial combinations. require(ggdmc) map_mean_v &lt;- ggdmc:::MakeEmptyMap(FR, lev) print(map_mean_v) ## n.C.N w.C.N p.C.N n.F.N w.F.N p.F.N n.H.N w.H.N p.H.N n.C.W w.C.W p.C.W ## &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## n.F.W w.F.W p.F.W n.H.W w.H.W p.H.W n.C.P w.C.P p.C.P n.F.P w.F.P p.F.P ## &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## n.H.P w.H.P p.H.P ## &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; length(map_mean_v) ## [1] 27 levels(map_mean_v) ## [1] \"CnN\" \"CwN\" \"CnW\" \"CwW\" \"FnN\" \"FwN\" \"FpN\" ## [8] \"FnW\" \"FwW\" \"FpW\" \"fa\" \"FpP\" \"HnN\" \"HwN\" ## [15] \"HpN\" \"HnW\" \"HwW\" \"HpW\" \"HpP\" \"FAKERATE\" Then we manually relabel the 27 elements, rendering the control condition to model two-accumulator process and the PM conditions to model three-accumulator process. That is, we label, for example p.C.N (i.e., a non-word response to a PM target in the control block), as FAKERATE. Except the five FAKERATE and four fa conditions, the other conditions just remove the dot symbol and rearrange the labelling sequence of three factors as cond-S-R. map_mean_v[1:27] &lt;- c( \"CnN\",\"CwN\",\"FAKERATE\", \"FnN\",\"FwN\",\"FpN\", \"HnN\",\"HwN\",\"HpN\", \"CnW\",\"CwW\",\"FAKERATE\", \"FnW\",\"FwW\",\"FpW\", \"HnW\",\"HwW\",\"HpW\", \"FAKERATE\",\"FAKERATE\",\"FAKERATE\", \"fa\",\"fa\",\"FpP\", \"fa\",\"fa\",\"HpP\" ) The following table compares the changes of labelling.                 map_mean_v before n.C.N w.C.N p.C.N n.F.N w.F.N p.F.N n.H.N map_mean_v after CnN CwN FAKERATE FnN FwN FpN HnN                 map_mean_v before w.H.N p.H.N n.C.W w.C.W p.C.W n.F.W w.F.W map_mean_v after HwN HpN CnW CwW FAKERATE FnW FwW                 map_mean_v before p.F.W n.H.W w.H.W p.H.W n.C.P w.C.P p.C.P map_mean_v after FpW HnW HWW HpW FAKERATE FAKERATE FAKERATE               map_mean_v before n.F.P w.F.P p.F.P n.H.P w.H.P p.H.P map_mean_v after fa fa FpP fa fa HpP Instead of assigning the regular “M” factor to the mean_v parameter, which controls the LBA accumulator, we associate the mean_v parameter with the newly created map_mean_v vector. In the syntax of BuildModel, we assign the map_mean_v to a MAPMV object by enter a list to the match.map option. Model 0 The following model assumes the PM condition associates with the decision threshold and the drift rate is associated with the PM, stimulus and response conditions, following the above map_mean_v set up. model0 &lt;- BuildModel( p.map = list(A = \"1\", B = c(\"cond\", \"R\"), t0 = \"1\", mean_v = c(\"MAPMV\"), sd_v = \"1\", st0 = \"1\", N = \"cond\"), match.map = list(M = list(n = \"N\", w = \"W\", p = \"P\"), MAPMV = map_mean_v), factors = list(S = c(\"n\",\"w\",\"p\"), cond = c(\"C\",\"F\", \"H\")), constants = c(N.C = 2, N.F = 3, N.H = 3, st0 = 0, B.C.P = Inf, mean_v.FAKERATE = 1, sd_v = 1), responses = c(\"N\", \"W\", \"P\"), type = \"norm\") ## Parameter vector names are: ( see attr(,\"p.vector\") ) ## [1] \"A\" \"B.C.N\" \"B.F.N\" \"B.H.N\" \"B.C.W\" \"B.F.W\" ## [7] \"B.H.W\" \"B.F.P\" \"B.H.P\" \"t0\" \"mean_v.CnN\" \"mean_v.CwN\" ## [13] \"mean_v.CnW\" \"mean_v.CwW\" \"mean_v.FnN\" \"mean_v.FwN\" \"mean_v.FpN\" \"mean_v.FnW\" ## [19] \"mean_v.FwW\" \"mean_v.FpW\" \"mean_v.fa\" \"mean_v.FpP\" \"mean_v.HnN\" \"mean_v.HwN\" ## [25] \"mean_v.HpN\" \"mean_v.HnW\" \"mean_v.HwW\" \"mean_v.HpW\" \"mean_v.HpP\" ## ## Constants are (see attr(,\"constants\") ): ## N.C N.F N.H st0 B.C.P ## 2 3 3 0 Inf ## mean_v.FAKERATE sd_v ## 1 1 ## ## Model type = norm (posdrift = TRUE ) A note to the value entered for the constants argument. The N.C, N.F and N.H in the constants argument represents the number of accumulators in the control, focal and non-focal conditions are 2, 3 and 3, respectively. The B.C.P represents the LBA B parameter (b = A + B) of the PM accumulator in the control condition is fixed at infinitve. This is another trick, signifying this particular accumulator requires infinitive amount of evidence to trigger a decision. The drift rate of the FAKERATE accumulator is set at one, which is inconsequential because its threshold is infinitive. A reminder: The LBA B parameter represents the travelling distance of an accumulator. The threshold parameter is denoted as . A is the LBA starting point parameter. This is a rather nuanced model object. Let’s check its internal to see how the 2 and 3 alternating accumulators are set up for the different PM condition. npar &lt;- length(GetPNames(model0)) ## Create a true parameter vector for recovery p.vector &lt;- c(A = .3, B.C.N = 1.3, B.F.N = 1.3, B.H.N = 1.3, B.C.W = 1.3, B.F.W = 1.4, B.H.W = 1.5, B.F.P = 1.1, B.H.P = 1.3, t0=.1, mean_v.CnN = 2.8, mean_v.CwN = -0.3, mean_v.CnW=-1, mean_v.CwW = 2.9, mean_v.FnN = 2.8, mean_v.FwN=-.3, mean_v.FpN = -1.6, mean_v.FnW = -1, mean_v.FwW = 2.9, mean_v.FpW = .5 , mean_v.fa = -2.4, mean_v.FpP = 2.5, mean_v.HnN = 2.8, mean_v.HwN = -.5, mean_v.HpN = -.6, mean_v.HnW = -.7, mean_v.HwW = 3.0, mean_v.HpW = 1.6, mean_v.HpP = 2.3) dat0 &lt;- simulate(model0, nsim=1e2, ps=p.vector) dmi0 &lt;- BuildDMI(dat0, model0) ## Remember in \"Model Array\" tutorial, we introduce the model object is a 3-D ## TRUE-FALSE array. Its first dimention is the factorial combination (aka ## design cell). dim0 &lt;- cell &lt;- dimnames(model0)[[1]] print(dim0) ## [1] \"n.C.N\" \"w.C.N\" \"p.C.N\" \"n.F.N\" \"w.F.N\" \"p.F.N\" \"n.H.N\" \"w.H.N\" \"p.H.N\" \"n.C.W\" ## [11] \"w.C.W\" \"p.C.W\" \"n.F.W\" \"w.F.W\" \"p.F.W\" \"n.H.W\" \"w.H.W\" \"p.H.W\" \"n.C.P\" \"w.C.P\" ## [21] \"p.C.P\" \"n.F.P\" \"w.F.P\" \"p.F.P\" \"n.H.P\" \"w.H.P\" \"p.H.P\" TableParameters constructs an accumulator-parameter table with each row representing an accumulator and each column representing a parameter. acc_tab0 &lt;- TableParameters(p.vector, 1, model0, FALSE) acc_tab1 &lt;- TableParameters(p.vector, \"w.C.N\", model0, FALSE) acc_tab2 &lt;- TableParameters(p.vector, \"w.F.P\", model0, FALSE) print(acc_tab0) print(acc_tab1) print(acc_tab2) ## A b t0 mean_v sd_v st0 nacc ## 1 0.3 1.6 0.1 2.8 1 0 2 ## 2 0.3 1.6 0.1 -1.0 1 0 2 ## 3 0.3 Inf 0.1 1.0 1 0 2 ## A b t0 mean_v sd_v st0 nacc ## 1 0.3 1.6 0.1 -0.3 1 0 2 ## 2 0.3 1.6 0.1 2.9 1 0 2 ## 3 0.3 Inf 0.1 1.0 1 0 2 ## A b t0 mean_v sd_v st0 nacc ## 1 0.3 1.6 0.1 -0.3 1 0 3 ## 2 0.3 1.7 0.1 2.9 1 0 3 ## 3 0.3 1.4 0.1 -2.4 1 0 3 Note that TableParameters has calculated b (=A+B). The 7th column indicates the number of accumulator in this condition. It describes the condition, so the 2nd and 3rd rows in the 7th column is redundant. After setting up the model, the sampling procedure is very much a routine. We set up prior distributions for the 29 parameters. In this case of a simulation study, we can check whether the prior distributions are improbable with respect to the true parameter. pname &lt;- GetPNames(model0) p.prior0 &lt;- BuildPrior( dists = c(rep(\"tnorm\", 9), \"beta\", rep(\"tnorm\", 19)), p1 = rep(1, npar), p2 = c(rep(2, 9), 1, rep(2, 19)), lower = c(rep(0, 10), rep(NA, 19)), upper = c(rep(NA, 9), 1, rep(NA, 19))) names(p.prior0) &lt;- pname plot(p.prior0, ps = p.vector) ## Sampling. We turned off the block-sampling to update an entire parameter at once. ## The block-sampling method updates only some of the parameters in a parameter vector. fit0 &lt;- StartNewsamples(dmi, p.prior0, block = FALSE, thin=2) fit0_correct &lt;- run(fit0, thin=2, block = FALSE) hat &lt;- gelman(fit0_correct, verbose=TRUE); est &lt;- summary(fit0_correct, recovery = TRUE, ps = p.vector, verbose = TRUE) Model 1 To test the role of the PM condition on its influence on the decision threshold, we fit a second model assuming no association between the threshold and the PM condition. ## map_mean_v is identical as before model1 &lt;- ggdmc:::BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = \"MAPMV\", sd_v = \"1\", st0 = \"1\", N = \"cond\"), match.map = list(M = list(n = \"N\", w = \"W\", p = \"P\"), MAPMV = map_mean_v), factors = list(S = c(\"n\",\"w\",\"p\"), cond = c(\"C\",\"F\", \"H\")), constants = c(N.C = 2, N.F = 3, N.H = 3, st0 = 0, mean_v.FAKERATE = 1, sd_v = 1), responses = c(\"N\", \"W\", \"P\"), type = \"norm\") ggdmc:::GetPNames(model1) ## Set up a different p.vector to test whether we can also recover this set. p.vector &lt;- c(A = .5, B.N = 1.2, B.W = 1, B.P = 1.5, t0=.15, mean_v.CnN = 1.25, mean_v.CwN = .35, mean_v.CnW = .25, mean_v.CwW = 1.15, mean_v.FnN = 1.8, mean_v.FwN = .35, mean_v.FpN = .12, mean_v.FnW = .11, mean_v.FwW = 1.32, mean_v.FpW = 1.33, mean_v.fa = -1.2, mean_v.FpP = 1.45, mean_v.HnN = 1.67, mean_v.HwN = .14, mean_v.HpN = .23, mean_v.HnW = .3, mean_v.HwW = 1.21, mean_v.HpW = 1.5, mean_v.HpP = 1.11) dat1 &lt;- simulate(model1, nsim=1e2, ps=p.vector) dmi1 &lt;- BuildDMI(dat1, model) pname &lt;- ggdmc:::GetPNames(model1) npar &lt;- length(pname) p.prior1 &lt;- ggdmc:::BuildPrior( dists = c(rep(\"tnorm\", 4), \"beta\", rep(\"tnorm\", 19)), p1 = rep(1, npar), p2 = c(rep(2, 4), 1, rep(2, 19)), lower = c(rep(0, 5), rep(NA, 19)), upper = c(rep(NA, 4), 1, rep(NA, 19))) names(p.prior1) &lt;- pname plot(p.prior1, ps = p.vector) fit0 &lt;- StartNewsamples(dmi1, p.prior1, block = FALSE, thin=2) fit1_correct &lt;- run(fit0, thin=4, block = FALSE) hat &lt;- gelman(fit1_correct, verbose=TRUE); est &lt;- summary(fit1_correct, recovery = TRUE, ps = p.vector, verbose = TRUE) # A B.N B.P B.W mean_v.CnN mean_v.CnW mean_v.CwN mean_v.CwW # True 0.50 1.20 1.50 1.00 1.25 0.25 0.35 1.15 # 2.5% Estimate 0.02 0.66 0.65 0.66 0.99 -0.56 -0.56 0.65 # 50% Estimate 0.39 1.16 1.17 1.13 1.33 -0.02 -0.02 1.00 # 97.5% Estimate 1.09 1.50 1.65 1.46 1.64 0.45 0.44 1.32 # Median-True -0.11 -0.04 -0.33 0.13 0.08 -0.27 -0.37 -0.15 # mean_v.fa mean_v.FnN mean_v.FnW mean_v.FpN mean_v.FpP mean_v.FpW # True -1.20 1.80 0.11 0.12 1.45 1.33 # 2.5% Estimate -2.66 1.42 -1.01 0.02 0.88 0.74 # 50% Estimate -1.56 1.74 -0.33 0.54 1.49 1.12 # 97.5% Estimate -0.75 2.04 0.26 0.99 2.01 1.48 # Median-True -0.36 -0.06 -0.44 0.42 0.04 -0.21 # mean_v.FwN mean_v.FwW mean_v.HnN mean_v.HnW mean_v.HpN mean_v.HpP # True 0.35 1.32 1.67 0.30 0.23 1.11 # 2.5% Estimate -0.45 0.84 1.09 -0.24 -0.37 0.47 # 50% Estimate 0.11 1.18 1.41 0.27 0.20 1.10 # 97.5% Estimate 0.57 1.49 1.72 0.70 0.68 1.66 # Median-True -0.24 -0.14 -0.26 -0.03 -0.03 -0.01 # mean_v.HpW mean_v.HwN mean_v.HwW t0 # True 1.50 0.14 1.21 0.15 # 2.5% Estimate 0.84 -0.63 0.58 0.10 # 50% Estimate 1.20 -0.11 0.94 0.16 # 97.5% Estimate 1.54 0.36 1.28 0.26 # Median-True -0.30 -0.25 -0.27 0.01 Model Comparison ## Use model 0 to fit data generated by model 1 fit0 &lt;- StartNewsamples(dmi0_wrong, p.prior0, block = FALSE, thin=2) fit0_wrong &lt;- run(fit0, thin=4, block = FALSE) hat &lt;- gelman(fit0_wrong, verbose=TRUE); ## Use model 1 to fit data generated by model 0 fit0 &lt;- StartNewsamples(dmi1_wrong, p.prior1, block = FALSE, thin=2) fit1_wrong &lt;- run(fit0, thin=4, block = FALSE) hat &lt;- gelman(fit1_wrong, verbose=TRUE); ## This compares using model0 and model 1 to fit dat0. ## The cond factor does not affect the B parameter, because the DIC difference ## is small. DIC(fit0_correct); DIC(fit1_wrong) # [1] 319.4222 # [1] 314.2239 ## This compares using model0 and model 1 to fit dat1. ## The cond factor does not affect the B parameter, because the DIC difference ## is small. DIC(fit1_correct); DIC(fit0_wrong); # [1] 2078.136 # [1] 2080.449 Modelling Data from Multiple Participants ## Model 1 -------------------------------------------- ## 27 elements with 20 levels ## Population distribution, pop.mean &lt;- c(A = .3, B.C.N = 1.3, B.F.N = 1.3, B.H.N = 1.3, B.C.W = 1.3, B.F.W = 1.4, B.H.W = 1.5, B.F.P = 1.1, B.H.P = 1.3, t0=.1, mean_v.CnN = 2.8, mean_v.CwN = -0.3, mean_v.CnW=-1, mean_v.CwW = 2.9, mean_v.FnN = 2.8, mean_v.FwN=-.3, mean_v.FpN = -1.6, mean_v.FnW = -1, mean_v.FwW = 2.9, mean_v.FpW = .5 , mean_v.fa = -2.4, mean_v.FpP = 2.5, mean_v.HnN = 2.8, mean_v.HwN = -.5, mean_v.HpN = -.6, mean_v.HnW = -.7, mean_v.HwW = 3.0, mean_v.HpW = 1.6, mean_v.HpP = 2.3) pop.scale &lt;-c(A = .05, B.C.N = .05, B.F.N = .05, B.H.N = .05, B.C.W = .05, B.F.W = .05, B.H.W = .05, B.F.P = .05, B.H.P = .05, t0=.05, mean_v.CnN = .05, mean_v.CwN = .05, mean_v.CnW = .05, mean_v.CwW = .05, mean_v.FnN = .05, mean_v.FwN = .05, mean_v.FpN = .05, mean_v.FnW = .05, mean_v.FwW = .05, mean_v.FpW = .05, mean_v.fa = .05, mean_v.FpP = .05, mean_v.HnN = .05, mean_v.HwN = .05, mean_v.HpN = .05, mean_v.HnW = .05, mean_v.HwW = .05, mean_v.HpW = .05, mean_v.HpP = .05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", 29), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 9), .1, rep(NA, 19)), upper = c(rep(NA,9), 1, rep(NA, 19))) dat0 &lt;- simulate(model0, nsub = 12, nsim = 50, prior = pop.prior) dmi0 &lt;- BuildDMI(dat0, model0) ps0 &lt;- attr(dat0, \"parameters\") pname &lt;- GetPNames(model0) p.prior &lt;- BuildPrior( dists = c(rep(\"tnorm\", 9), \"beta\", rep(\"tnorm\", 19)), p1 = rep(1, npar), p2 = c(rep(2, 9), 1, rep(2, 19)), lower = c(rep(0, 10), rep(NA, 19)), upper = c(rep(NA, 9), 1, rep(NA, 19))) mu.prior &lt;- BuildPrior( dists = c(rep(\"tnorm\", 9), \"beta\", rep(\"tnorm\", 19)), p1 = rep(1, npar), p2 = c(rep(2, 9), 1, rep(2, 19)), lower = c(rep(0, 10), rep(NA, 19)), upper = c(rep(NA, 9), 1, rep(NA, 19))) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = rep(1, npar), p2 = rep(1, npar)) names(p.prior) &lt;- pname names(mu.prior) &lt;- pname names(sigma.prior) &lt;- pname priors0 &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior) save(model0, dat0, dmi0, ps0, priors0, file = \"tests Group2 PM12S.RData\") ## Sampling separately ---------- load(\"tests Group2 PM12S.RData\") fit0 &lt;- StartNewsamples(dmi0, priors0[[1]], ncore=6, thin=4) fit &lt;- run(fit0, thin=2, ncore=6) est0 &lt;- summary(fit, recovery = TRUE, ps = ps0, verbose =TRUE) rhat0 &lt;- gelman(fit, verbose=TRUE) save(fit0, fit, model0, dat0, dmi0, ps0, priors0, file = \"tests Group2 PM12S.RData\") Reference Strickland, L., Loft, S., Remington, R. W., &amp; Heathcote, A. (2018). Racing to remember: A theory of decision control in event-based prospective memory. Psychological Review, 125(6), 851-887. http: dx.doi.org 10.1037 rev0000113 Wagenmakers, E.-J., Ratcliff, R., Gomez, P., &amp; McKoon, G. (2008). A diffusion model account of criterion shifts in the lexical decision task. Journal of Memory and Language, 58, 140-159. doi:10.1016 j.jml.2007.04.006."
					}

					
				
			
		
			
				
					,
					

					"fixed-effect-model-cddm12s": {
						"id": "fixed-effect-model-cddm12s",
						"title": "CDDM",
						"category": "",
						"url": " /fixed-effect-model/cddm12S/",
						"content": "Disclaimer: We have striven to minimize the number of errors. However, we cannot guarantee the note is 100% accurate. This note records the codes for fitting fixed-effect 2-D diffusion model. In this tutorial, we conducted a parameter recovery of a CDDM associating a speed-and-accuracy (SAT) factor with the threshold parameter, a. A classic SAT experiment usually encourages participants to emphaize response speed in one condition and response accuracy in the other. The SAT factor is often found selectively affecting threshold-related parameters in 1-D diffusion model. The model description prepares for a design with two factors: a three-level stimulus factor (S) and a two-level speed-and-accuracy (SAT) factor. However, it assumes only the threshold parameter, a is affected by the SAT factor and the S factor is inconseuqntial. ## This cleans up objects in the workspace, but already-loaded pacakges will be ## stilled loaded. To get a clear start of an R session, use the hot-key Ctrl-Shift-F10 ## in RStudio for instance. rm(list = ls()) require(ggdmc) nw &lt;- 4 model &lt;- BuildModel( p.map = list(v1 = \"1\", v2 = \"1\", a = \"SAT\", t0 = \"1\", sigma1=\"1\", sigma2=\"1\", eta1=\"1\", eta2=\"1\", tmax=\"1\", h=\"1\"), match.map = NULL, constants = c(sigma1 = 1, sigma2 = 1, eta1=0, eta2=0, tmax=3, h=1e-4), factors = list(S = c(\"s1\", \"s2\", \"s3\"), SAT=c(\"speed\", \"accuracy\")), responses = paste0('theta_', letters[1:nw]), type = \"cddm\") npar &lt;- length(GetPNames(model)) In the following, we set up an assumed mechanism to generate twelve sets of true parameters, inside the simulate function. The true parameters are stored as a ps matrix, an attribute attached to the output object from the simulate function. Each row of the ps matrix represents one participant. pop.mean &lt;- c(v1=1.2, v2=2.2, a.speed = 1.5, a.accuracy = 2.0, t0=0.1) pop.scale &lt;- c(v1=2.5, v2=2.5, a.speed=2, a.accuracy=2.0, t0=0.5) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(-5, 2), 0, 0, 0), upper = c(rep( 5, 2), 5, 5, 2)) dat &lt;- simulate(model, nsub=12, nsim = 30, prior = pop.prior); ps &lt;- attr(dat, \"parameters\") dmi &lt;- BuildDMI(dat, model) We then set up prior distributions for each CDDM parameter. Before we proceed to fit the data, we check whether our prior distributions cover the all true target parameters. p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(-10, 2), 0, 0, 0), upper = c(rep( 10, 2), 8, 8, 5)) prior_d &lt;- plot(p.prior, save = TRUE) wide &lt;- data.table::data.table(ps) wide$s &lt;- factor(1:nrow(ps)) pveclines &lt;- data.table::melt.data.table( wide, id.vars = \"s\", variable.name = \"Parameter\", value.name = \"true\") ## require(ggplot2) p0 &lt;- ggplot(prior_d, aes_string(x = \"xpos\", y = \"ypos\")) + geom_line() + geom_vline(data = pveclines, aes_string(xintercept = \"true\"), linetype = \"dotted\", size = 1) + xlab(\"\")+ ylab(\"\")+ facet_wrap(~Parameter, scales=\"free\") + theme_bw() + theme(legend.position = \"none\", strip.text.x = element_text(size = 16), strip.text.y = element_blank(), axis.title.y = element_blank(), axis.text.x = element_text(size = 16), axis.text.y = element_text(size = 16), axis.title.x = element_blank()) Then we use 3 CPU cores, ncore=3 to run 3 parallel model fits. The block option indicates whether we want to update the entire parameter vector or just one parameter in the vector at a time. This is critical for the hierarchical model fit, but inconsequential for the fixed-effect model fit, so to gain more speed, we choose to disable it, block=FALSE. Note in R 3.6.1, the mc.cores option in mclapply has been altered. It now takes the ncore option via “getOpion(“mc.cores”, 2L)”. This renders the ggdmc 0.2.6.0 always run 2 cores, which is a default setting of the mclapply function. To launch the number of CPU core you want, you have to adjust this manually in the two internal R functions, run_many and rerun_many, accordingly, or update your ggdmc via devtool tools. ## The latest ggdmc 0.2.7.1 will report its processing time! ## Processing time: 214.3 secs. fit0 &lt;- StartNewsamples(dmi, p.prior, block=FALSE, ncore=3) ## Processing time: 611.43 secs. fit &lt;- run(fit0, block=FALSE, ncore=3) After finishing the model fit, we check whether the chains are converged. The potential scale reduction factors, PSRT, are all below well 1.10. rhat &lt;- gelman(fit, verbose=TRUE) # Diagnosing theta for many participants separately # Mean 5 3 10 7 1 4 2 11 8 12 6 9 # 1.02 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.02 1.02 1.02 1.03 1.10 est &lt;- summary(fit, recovery = TRUE, ps = ps, verbose = FALSE) Plus, the parameters are precisely recovered. # Summary each participant separately # v1 v2 a t0 # Mean 1.98 1.92 1.52 0.12 # True 1.98 2.01 1.49 0.12 # Diff 0.00 0.10 -0.03 0.00 # Sd 0.14 0.17 0.12 0.04 # True 0.08 0.09 0.07 0.04 # Diff -0.06 -0.08 -0.05 0.00"
					}

					
				
			
		
			
				
					,
					

					"fixed-effect-model-many-participants": {
						"id": "fixed-effect-model-many-participants",
						"title": "Multiple Participants",
						"category": "",
						"url": " /fixed-effect-model/many_participants/",
						"content": "In this tutorial, I illustrated fitting multiple participants, assuming the mechanism of data generation is fixed-effect models. That is, each participant is accounted for by independent mechanisms. I also assume the LBA model is the true RT model. I made up a two-factor factorial design. The first two-level factor is the stimulus (S). Suppose the stimuli have two types: one is low quality face photos, so people find it hard to recognize and the other is normal quality face photo. The second factor is the frequency (F), supposing one type is the celebrity photos, so people perhaps see more often, and the other is the photos of randomly selected strangers. In the model set-up, I presume a rate model, which has its drift rates affected by the two factors, S and F. The latent factor, M, is just a LBA way to model independent accumulators. Another factor, R, not explicitly in the factorial design, is an indicator factor, indicating the response type affecting the threshold parameter (i.e., accumulators traveling distance). require(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = c(\"S\", \"F\", \"M\"), sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1 = 1, s2 = 2)), factors = list(S = c(\"s1\", \"s2\"), F = c(\"f1\", \"f2\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") ## [1] \"A\" \"B.r1\" \"B.r2\" ## [4] \"t0\" \"mean_v.s1.f1.true\" \"mean_v.s2.f1.true\" ## [7] \"mean_v.s1.f2.true\" \"mean_v.s2.f2.true\" \"mean_v.s1.f1.false\" ## [10] \"mean_v.s2.f1.false\" \"mean_v.s1.f2.false\" \"mean_v.s2.f2.false\" ## [13] \"sd_v.true\" npar &lt;- length(GetPNames(model)) To simulate many participants, I set up a population distribution, which is not in line with the assumption of fixed-effects model. That is, this way to generate data is to presume that a random-effects model at work. For the purpose of illustration, I forgo this issue for now. pop.mean &lt;- c(A = .4, B.r1 = .85, B.r2 = .8, t0 = .1, mean_v.s1.f1.true = 2.5, mean_v.s2.f1.true = 3.5, mean_v.s1.f2.true = 4.5, mean_v.s2.f2.true = 5.5, mean_v.s1.f1.false = 1.00, mean_v.s2.f1.false = 1.10, mean_v.s1.f2.false = 1.05, mean_v.s2.f2.false = 1.20, sd_v.true = .25) pop.scale &lt;- c(A = .1, B.r1 = .1, B.r2 = .1, t0 = .05, mean_v.s1.f1.true = .2, mean_v.s2.f1.true = .2, mean_v.s1.f2.true = .2, mean_v.s2.f2.true = .2, mean_v.s1.f1.false = .2, mean_v.s2.f1.false = .2, mean_v.s1.f2.false = .2, mean_v.s2.f2.false = .2, sd_v.true = .1) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 4), rep(NA, 8), 0), upper = c(rep(NA, npar))) We may want to check how the prior distributions look like. ggdmc has a plot function to do just that. Note you need to load ggdmc package (i.e., require(ggdmc)) to make plot function changes its default behaviour. plot(pop.prior) ## Simulate some data dat &lt;- simulate(model, nsim = 30, nsub = 8, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) dplyr::tbl_df(dat) ## # A tibble: 960 x 5 ## s S F R RT ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 s1 f1 r1 0.438 ## 2 1 s1 f1 r1 0.517 ## 3 1 s1 f1 r1 0.407 ## 4 1 s1 f1 r1 0.454 ## 5 1 s1 f1 r1 0.449 ## 6 1 s1 f1 r1 0.463 ## 7 1 s1 f1 r1 0.552 ## 8 1 s1 f1 r1 0.411 ## 9 1 s1 f1 r1 0.387 ## 10 1 s1 f1 r1 0.486 ## # ... with 950 more rows The true averaged parameter vectors, which were randomly chosen based on pop.prior, can be retrieved by looking up the parameters attribute, attached onto the dat object. However, note the real true values are pop.mean and pop.scale, because the data are generated based on random-effects model. require(matrixStats) ps &lt;- attr(dat, \"parameters\") mu &lt;- round(colMeans2(ps), 2) sigma &lt;- round(colSds(ps), 2) truevalues &lt;- rbind(mu, sigma) colnames(truevalues) &lt;- GetPNames(model) ## Set up prior distributions p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*20, lower = c(rep(0, 4), rep(NA, 8), 0), upper = c(rep(NA, npar))) Now we am ready to fit the eight participants. Ideally, this can be done simultaneously, if a eight-core machine is available. Here I used a four-core machine so launched 2 cores only (ncore = 2). ## Sampling ------------- fit0 &lt;- StartNewsamples(dmi, p.prior, ncore = 2) fit &lt;- run(fit0, 5e2, ncore = 2) Use plot to check whether posterior log-likelihood converged. plot(fit) gelman function prints the potential scale reduction factor (psrf). A psrf value less than 1.1 suggests chains are well-mixed. res &lt;- gelman(fit, verbose = TRUE) # Diagnosing theta for many participants separately # 15 20 13 18 6 17 12 19 5 11 8 3 16 14 1 2 9 # 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.02 1.02 # 4 7 10 # 1.02 1.03 1.03 # Mean # [1] 1.01 By setting the option, pll (posterior log-likelihood), to FALSE and the option, den (density plot), to TRUE , we can check the trace plots for each model parameters. Because there are several participants, the size of the figure is considerably large. It may be a better to plot separately in a pdf file and check it later. pdf(\"figs subjects-density.pdf\") lapply(fit, ggdmc:::plot.model, pll = FALSE, den = TRUE ) dev.off() One specific feature in ggdmc is that it uses pMCMC, so occasionally, we want to check a subset of chains. The function will randomly pick three chains to plot plot(sam, subchain = TRUE) You can indicate how many subset of chains to plot, too. plot(sam, subchain = TRUE, nsubchain = 4)) You can also indicate which chains, instead of randomly selecting a subset of chains. plot(sam, subchain = TRUE, nsubchain = 4, chains = c(1:4)) These are a lot of checks! Finally and fortunately, because this is a parameter recovery study, I can look up the true parameters to see if I do estimate them well. summary function will do the trick by entering TRUE for the recovery option and entering the true parameter matrix, which I had stored it to a ps object before, to ps option. est &lt;- summary(sam, recovery = TRUE, ps = ps) # Summary each participant separately # A B.r1 B.r2 t0 mean_v.s1.f1.true mean_v.s2.f1.true mean_v.s1.f2.true # Mean 0.50 1.02 0.96 0.10 3.05 4.34 5.50 # True 0.41 0.82 0.77 0.10 2.49 3.56 4.50 # Diff -0.09 -0.20 -0.19 0.00 -0.56 -0.78 -1.00 # Sd 0.15 0.20 0.17 0.05 0.49 0.44 0.76 # True 0.11 0.10 0.09 0.05 0.21 0.19 0.22 # Diff -0.04 -0.10 -0.08 -0.01 -0.28 -0.25 -0.54 # mean_v.s2.f2.true mean_v.s1.f1.false mean_v.s2.f1.false mean_v.s1.f2.false # Mean 6.69 1.59 1.58 0.70 # True 5.49 1.03 1.15 1.08 # Diff -1.20 -0.56 -0.43 0.38 # Sd 0.82 0.34 1.21 1.58 # True 0.22 0.15 0.23 0.15 # Diff -0.59 -0.19 -0.98 -1.43 # mean_v.s2.f2.false sd_v.true # Mean -0.38 0.30 # True 1.11 0.24 # Diff 1.48 -0.06 # Sd 1.23 0.11 # True 0.16 0.10 # Diff -1.07 -0.01"
					}

					
				
			
		
			
				
					,
					

					"fixed-effect-model-one-participant": {
						"id": "fixed-effect-model-one-participant",
						"title": "One Participant",
						"category": "",
						"url": " /fixed-effect-model/one_participant/",
						"content": "Fixed-effects models assume each participant has his her own specific mechanism of parameter generation. This assumption is relative to the random-effect models, which assume one common mechanism is responsible for generating parameters for all participants. The latter is sometimes dubbed, hierarchical or multi-level models, although the three terms could carry subtle different ideas. In this tutorial, I illustrated the method of conducting the fixed-effects modelling. Given many observations of response times (RT) and choices, one modelling aim is to estimate the parameters that generate the observations. A typical scenario is we collect data by inviting participants to visit our laboratory, having them do some cognitive tasks, and recording their RTs and choices. Often, we would use a RT model, for example diffusion decision model (DDM) (Ratcliff &amp; McKoon, 2008)1 to estimate latent variables. I first set up a model object. The type = “rd”, refers to Ratcliff’s diffusion model. require(ggdmc) model &lt;- BuildModel( p.map = list(a = \"1\", v = \"1\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\")), responses = c(\"r1\", \"r2\"), constants = c(st0 = 0, d = 0), type = \"rd\") p.vector &lt;- c(a = 1, v = 1.2, z = .38, sz = .25, sv = .2, t0 = .15) ntrial &lt;- 1e2 dat &lt;- simulate(model, nsim = ntrial, ps = p.vector) dmi &lt;- BuildDMI(dat, model) ## A tibble: 200 x 3 ## use dplyr::tbl_df(dat) to print this ## S R RT ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 s1 r1 0.249 ## 2 s1 r1 0.246 ## 3 s1 r2 0.262 ## 4 s1 r1 0.519 ## 5 s1 r1 0.205 ## 6 s1 r1 0.177 ## 7 s1 r1 0.174 ## 8 s1 r1 0.378 ## 9 s1 r1 0.197 ## 10 s1 r1 0.224 ## ... with 190 more rows Because the data were simulated from one set of presumed true values, p.vector, I can use them later to verify whether the sampling process does recovery the parameters. In Bayesian inference, we also need prior distributions, so let’s build a set of prior distributions for each DDM parameters. A beta distribution with shape1 = 1 and shape2 = 1, equals to a uniform distribution (beta(1, 1)). This choice was to regularize the parameters, (1) the start point, z, (2) its variability sz and (3) t0. All three were bounded by 0 and 1. Others used truncated normal distributions bounded by lower and upper arguments. plot drew the prior distributions, providing a visual check method. This method, in the case of parameter recovery study, was to make sure the prior distribution does cover the true values. p.prior &lt;- BuildPrior( dists = c(rep(\"tnorm\", 2), \"beta\", \"beta\", \"tnorm\", \"beta\"), p1 = c(a = 1, v = 0, z = 1, sz = 1, sv = 1, t0 = 1), p2 = c(a = 1, v = 2, z = 1, sz = 1, sv = 1, t0 = 1), lower = c(0, -5, NA, NA, 0, NA), upper = c(5, 5, NA, NA, 5, NA)) plot(p.prior, ps = p.vector) By default StartNewsamples used p.prior to randomly draw start points and samples 200 MCMC samples. This step used a mixture of crossover and migration operators. The run function by default drew 500 MCMC samples, using only crossover operator. gelman function reported PSRF value of 1.06 in this case. A potential scale reduction factor (PSRF2) less than 1.1 suggested chains are converged. fit0 &lt;- StartNewsamples(dmi, p.prior) fit &lt;- run(fit0) rhat &lt;- gelman(fit, verbose = TRUE) es &lt;- effectiveSize(fit) ## Diagnosing a single participant, theta. Rhat = 1.06 plot by default drew posterior log-likelihood. With the option, start, it changed to a latter start iteration to draw. p0 &lt;- plot(fit0) ## p0 &lt;- plot(fit0, start = 101) p1 &lt;- plot(fit) png(\"pll.png\", 800, 600) gridExtra::grid.arrange(p0, p1, ncol = 1) dev.off() The upper panel showed the chains quickly converged to posterior log-likelihoods near 100th iteration and the bottom panel confirmed the rhat value (&lt; 1.1). p2 &lt;- plot(fit, pll = FALSE, den= FALSE) p3 &lt;- plot(fit, pll = FALSE, den= TRUE) png(\"den.png\", 800, 600) gridExtra::grid.arrange(p2, p3, ncol = 1) dev.off() In a simulation study, we can check whether the sampling process is OK, using summary est &lt;- summary(fit, recover = TRUE, ps = p.vector, verbose = TRUE) ## Recovery summarises only default quantiles: 2.5% 50% 97.5% ## a sv sz t0 v z ## True 1.0000 0.2000 0.2500 0.1500 1.2000 0.3800 ## 2.5% Estimate 0.9656 0.0401 0.0112 0.1338 1.1463 0.3504 ## 50% Estimate 1.0419 0.6010 0.2174 0.1444 1.4983 0.3867 ## 97.5% Estimate 1.1509 1.7128 0.4781 0.1522 2.0005 0.4273 ## Median-True 0.0419 0.4010 -0.0326 -0.0056 0.2983 0.0067 Finally, we might want to check whether the model fits the data well. There are many methods to quantify the goodness of fit. Here, I illustrated two methods. First method is to calculate DIC and BPIC. These information criteria are useful for model selection. (need &gt; ggdmc 2.5.5) DIC(fit) DIC(fit, BPIC=TRUE) Secondly, I simulated post-predictive data. xlim trims off outlier values in the simulation. Note there are two different versions of the post-predictive functions, because the ggdmc version &gt; 0.2.7.5 starts to use S4 class, which use @, instead of $ to extract elemnts in an object. predict_one &lt;- function(object, npost = 100, rand = TRUE, factors = NA, xlim = NA, seed = NULL) { require(ggdmc) if(packageVersion('ggdmc') == '0.2.6.0') { message('Using $ to extract object in v 0.2.6.0') out &lt;- predict_one0260(object, npost, rand, factors, xlim, seed) } else { message('Using @ to extract object in v 0.2.6.0') out &lt;- predict_one0280(object, npost, rand, factors, xlim, seed) } return(out) } predict_one0260 &lt;- function(object, npost, rand, factors, xlim, seed) { model &lt;- attr(object$data, 'model') facs &lt;- names(attr(model, \"factors\")); if (!is.null(factors)) { if (any(is.na(factors))) factors &lt;- facs if (!all(factors %in% facs)) stop(paste(\"Factors argument must contain one or more of:\", paste(facs, collapse=\",\"))) } resp &lt;- names(attr(model, \"responses\")) ns &lt;- table(object$data[,facs], dnn = facs) npar &lt;- object$n.pars nchain &lt;- object$n.chains nmc &lt;- object$nmc ntsample &lt;- nchain * nmc pnames &lt;- object$p.names thetas &lt;- matrix(aperm(object$theta, c(3,2,1)), ncol = npar) colnames(thetas) &lt;- pnames if (is.na(npost)) { use &lt;- 1:ntsample } else { if (rand) { use &lt;- sample(1:ntsample, npost, replace = F) } else { ## Debugging purpose use &lt;- round(seq(1, ntsample, length.out = npost)) } } npost &lt;- length(use) posts &lt;- thetas[use, ] nttrial &lt;- sum(ns) ## number of total trials v &lt;- lapply(1:npost, function(i) { ggdmc:::simulate_one(model, n = ns, ps = posts[i,], seed = seed) }) out &lt;- data.table::rbindlist(v) reps &lt;- rep(1:npost, each = nttrial) out &lt;- cbind(reps, out) if (!any(is.na(xlim))) { out &lt;- out[RT &gt; xlim[1] &amp; RT &lt; xlim[2]] } return(out) } predict_one0280 &lt;- function(object, npost, rand, factors, xlim, seed) { ## Update for using S4 class model &lt;- object@dmi@model facs &lt;- names(attr(model, \"factors\")); if (!is.null(factors)) { if (any(is.na(factors))) factors &lt;- facs if (!all(factors %in% facs)) stop(paste(\"Factors argument must contain one or more of:\", paste(facs, collapse=\",\"))) } resp &lt;- names(attr(model, \"responses\")); ns &lt;- table(object@dmi@data[,facs], dnn = facs); npar &lt;- object@npar nchain &lt;- object@nchain nmc &lt;- object@nmc; ntsample &lt;- nchain * nmc pnames &lt;- object@pnames thetas &lt;- matrix(aperm(object@theta, c(3,2,1)), ncol = npar) colnames(thetas) &lt;- pnames if (is.na(npost)) { use &lt;- 1:ntsample } else { if (rand) { use &lt;- sample(1:ntsample, npost, replace = F) } else { ## Debugging purpose use &lt;- round(seq(1, ntsample, length.out = npost)) } } npost &lt;- length(use) posts &lt;- thetas[use, ] nttrial &lt;- sum(ns) ## number of total trials v &lt;- lapply(1:npost, function(i) { ggdmc:::simulate_one(model, n = ns, ps = posts[i,], seed = seed) }) out &lt;- data.table::rbindlist(v) reps &lt;- rep(1:npost, each = nttrial) out &lt;- cbind(reps, out) if (!any(is.na(xlim))) { out &lt;- out[RT &gt; xlim[1] &amp; RT &lt; xlim[2]] } return(out) } pp &lt;- predict_one(fit, xlim = c(0, 5)) dat &lt;- fit@dmi@data ## use this line for version &gt; 0.2.7.5 ## dat &lt;- fit$data ## use this line for version 0.2.6.0 dat$C &lt;- ifelse(dat$S == \"s1\" &amp; dat$R == \"r1\", TRUE, ifelse(dat$S == \"s2\" &amp; dat$R == \"r2\", TRUE, ifelse(dat$S == \"s1\" &amp; dat$R == \"r2\", FALSE, ifelse(dat$S == \"s2\" &amp; dat$R == \"r1\", FALSE, NA)))) pp$C &lt;- ifelse(pp$S == \"s1\" &amp; pp$R == \"r1\", TRUE, ifelse(pp$S == \"s2\" &amp; pp$R == \"r2\", TRUE, ifelse(pp$S == \"s1\" &amp; pp$R == \"r2\", FALSE, ifelse(pp$S == \"s2\" &amp; pp$R == \"r1\", FALSE, NA)))) dat$reps &lt;- NA dat$type &lt;- \"Data\" pp$reps &lt;- factor(pp$reps) pp$type &lt;- \"Simulation\" DT &lt;- rbind(dat, pp) require(ggplot2) p1 &lt;- ggplot(DT, aes(RT, color = reps, size = type)) + geom_freqpoly(binwidth = .05) + scale_size_manual(values = c(1, .3)) + scale_color_grey(na.value = \"black\") + theme(legend.position = \"none\") + facet_grid(S ~ C) The grey lines are model predictions. By default, predict_one randomly draws 100 parameter estimates and simulate data based on them. Therefore, there are 100 lines, showing the prediction variability. The solid dark line shows the data. In this case, the dark line is within the range covering by the grey lines. Note that the error responses (FALSE) are not predicted as well as the correct responses. This is fairly common, when the number of trial is small. In this case, it has only 13 trials. This is often dubbed, drift-diffusion model, but in Ratcliff and McKoon’s work, they called it diffusion decision model. &#8617; Brook, S. P., &amp; Gelman, A. (1998) General Methods for Monitoring Convergence of Iterative Simulations, Journal of Computational and Graphical Statistics, 7:4 . &#8617;"
					}

					
				
			
		
			
				
					,
					

					"mcmc-hastings": {
						"id": "mcmc-hastings",
						"title": "Metropolis-Hastings",
						"category": "",
						"url": " /mcmc/hastings/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"mcmc-mcmc": {
						"id": "mcmc-mcmc",
						"title": "Markov Chain Monte Carlo",
						"category": "",
						"url": " /mcmc/mcmc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"mcmc-rwm": {
						"id": "mcmc-rwm",
						"title": "Random Walk Metropolis",
						"category": "",
						"url": " /mcmc/rwm/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hcddm": {
						"id": "random-effect-model-hcddm",
						"title": "HCDDM",
						"category": "",
						"url": " /random-effect-model/hcddm/",
						"content": "Disclaimer: We have striven to minimize the number of errors. However, we cannot guarantee the note is 100% accurate. This note records the codes for fitting hierarchical 2-D diffusion model. The explanation will be added later. First, we set up a 2-D diffusion model, simulate a data set and then define three sets of prior distributions for the CDDM parameters. model &lt;- BuildModel( p.map = list(v1 = \"1\", v2 = \"1\", a = \"1\", t0 = \"1\", sigma1=\"1\", sigma2=\"1\", eta1=\"1\", eta2=\"1\", tmax=\"1\", h=\"1\"), match.map = NULL, constants = c(sigma1 = 1, sigma2 = 1, eta1=0, eta2=0, tmax=6, h=1e-4), factors = list(S = c(\"s1\", \"s2\")), responses = paste0('theta_', letters[1:4]), type = \"cddm\") npar &lt;- length(GetPNames(model)) pop.mean &lt;- c(v1 = 2, v2 = 2, a = 1.5, t0=0.1) pop.scale &lt;- c(v1 = .1, v2 = .1, a = .05, t0=0.05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(-5, -5, 0, 0), upper = c( 5, 5, 5, 2)) ## Simulate some data dat &lt;- simulate(model, nsub = 12, nsim = 1e2, prior = pop.prior) ps &lt;- attr(dat, \"parameters\") dmi &lt;- BuildDMI(dat, model) p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1=c(v1=0, v2=0, a=1, t0=1), p2=c(v1=2, v2=2, a=2, t0=1), lower = c(-5, -5, rep(0, 2)), upper = rep(NA, npar)) mu.prior &lt;- ggdmc::BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(-5,-5, 0, 0), upper = c(5, 5, 5, 1) ) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = c(v1=1, v2=1, a = 1, t0=1), p2 = rep(1, npar), upper = rep(NA, npar)) priors &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior) A conventional practice in our parameter-recovery study is to check the prior distributions plot(pop.prior, ps=ps) plot(p.prior, ps=ps) plot(mu.prior) plot(sigma.prior) save(priors, dmi, model, ps, pop.mean, pop.scale, file = \"tests Group5 test_hcddm12S.RData\") The hierarchical model fit takes usually more time than the fixed-effect model fit. ## 23.7 mins fit0 &lt;- StartNewsamples(dmi, priors, nmc=5e2) ## 25.8 mins fit &lt;- run(fit0) ## 51.5 mins fit &lt;- run(fit, thin=2) save(fit, fit0, priors, dmi, model, ps, pop.mean, pop.scale, file = \"tests Group5 test_hcddm12S.RData\") The PSRT reports the chains are converged. res &lt;- hgelman(fit, verbose = TRUE) ## hyper 10 6 4 2 5 12 11 9 7 3 8 1 ## 1.14 1.02 1.02 1.03 1.04 1.04 1.04 1.05 1.06 1.10 1.13 1.14 1.16 The estimates averaged across particiapnts and their standard deviations are close to the true values. est0 &lt;- summary(fit, recovery = TRUE, ps = ps, verbose = TRUE) # v1 v2 a t0 # Mean 2.03 1.92 1.54 0.10 # True 1.99 2.03 1.51 0.10 # Diff -0.04 0.11 -0.03 0.00 # Sd 0.11 0.11 0.02 0.05 # True 0.11 0.11 0.03 0.04 # Diff 0.00 0.00 0.02 0.00 The estimates of the hyper parameters are fairly close to the true values, too. est1 &lt;- summary(fit, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1, verbose = TRUE) # a t0 v1 v2 # True 1.50 0.10 2.00 2.00 # 2.5% Estimate 1.50 0.01 1.90 1.79 # 50% Estimate 1.54 0.09 2.03 1.92 # 97.5% Estimate 1.58 0.13 2.16 2.04 # Median-True 0.04 -0.01 0.03 -0.08 est2 &lt;- summary(fit, hyper = TRUE, recovery = TRUE, ps = pop.scale, type = 2, verbose = TRUE) # a t0 v1 v2 # True 0.05 0.05 0.10 0.10 # 2.5% Estimate 0.01 0.04 0.09 0.08 # 50% Estimate 0.04 0.07 0.17 0.16 # 97.5% Estimate 0.11 0.14 0.34 0.32 # Median-True -0.01 0.02 0.07 0.06"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hddm": {
						"id": "random-effect-model-hddm",
						"title": "HDDM",
						"category": "",
						"url": " /random-effect-model/hddm/",
						"content": "We have striven to minimize the number of errors. However, we canot guarantee the note is 100% accurate. In this tutorial, I conducted a parameter recovery study, demonstrating the pMCMC method to fit a hierarchical DDM for a relatively simple factorial design. Set-up a model object This particular design is from Heathcote et al’s (2018) DMC tutorial, which assumes a word frequency (my interpretation) factor affecting the mean drift rate (v). Note it is not a good practice to use “F” notation in R, because it is also used as a shorthand for the reserved word, meaning FALSE. However, one of the R strengths is it permits idiosyncratic programming habits, even bad ones. In the example here, it seems not cause any errors. ## version 0.2.6.0 ## install.packages(\"ggdmc\") loadedPackages &lt;-c(\"ggdmc\", \"data.table\", \"ggplot2\", \"gridExtra\", \"ggthemes\") sapply(loadedPackages, require, character.only=TRUE) model &lt;- BuildModel( p.map = list(a = \"1\", v = \"F\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(s1 = \"r1\", s2 = \"r2\")), factors = list(S = c(\"s1\", \"s2\"), F = c(\"f1\", \"f2\")), constants = c(st0 = 0, d = 0), responses = c(\"r1\", \"r2\"), type = \"rd\") npar &lt;- length(GetPNames(model)) To conduct a parameter recovery study, I firstly assumed a hidden multi-level mechanism generating the data. That is, I presumed there is a distribution at the population participants level. This distribution is a 7 dimension distribution, which has 7 marginal distributions. Each of them is in control of one DDM parameter. pop.mean &lt;- c(a = 2, v.f1 = 4, v.f2 = 3, z = .5, sz = .3, sv = 1, t0 = .3) pop.scale &lt;- c(a = .5, v.f1 =.5, v.f2 = .5, z = .1, sz = .1, sv = .3, t0 = .05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(0, rep(-5, 2), rep(0, 4)), upper = c(5, rep( 7, 2), 1, 2, 1, 1)) As usual, I want to visually check if the assumed mechanism is reasonable. plot(pop.prior) After making sure that the data generating mechanism is proper, I then simulated a data set with 40 participants and 250 trials for each condition. dat &lt;- simulate(model, prior = pop.prior, nsim = 250, nsub = 40) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") In the hierarchical case of the parameter recovery study, my aim is to recover not only ps matrix, but also the data generating mechanism. That is, I also want to known pop.mean, pop.scale and their marginal distribution. Note ps is a matrix, storing the true values for each DDM parameter. That is, the simulate function randomly selects nsub set of true parameters based on the prior distribution, pop.prior. Each row of the ps matrix represents one parameter vector of a participant. tibble::as_tibble(ps) ## A tibble: 40 x 7 ## a v.f1 v.f2 z sz sv t0 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.93 4.30 2.95 0.486 0.151 0.775 0.304 ## 2 1.52 3.74 2.40 0.589 0.258 0.809 0.340 ## 3 1.85 4.31 2.62 0.636 0.318 0.903 0.349 ## 4 2.34 3.94 2.53 0.578 0.127 0.993 0.283 ## 5 2.44 4.50 2.68 0.566 0.246 0.589 0.261 ## 6 2.73 4.08 3.68 0.465 0.182 0.973 0.290 ## 7 2.34 2.89 3.69 0.742 0.307 0.592 0.325 ## 8 1.90 4.35 3.56 0.512 0.186 0.513 0.309 ## 9 2.60 4.18 2.81 0.541 0.367 0.758 0.270 ## 10 1.64 3.77 2.37 0.596 0.286 0.995 0.375 ## with 30 more rows The above is only preparation work for a parameter recovery study. In the following, I will start to draw samples from the posterior distribution, aiming to recover pop.mean, pop.scale, the target distribution, and the ps matrix. I already have the likelihood, which is the DDM equation (Voss, Rothermund, Voss, 2004). I will need to set up the prior distributions for the seven DDM parameters. In the case of hierarchical model, there are two sets of prior distributions: one is usually called hyper-prior distributions and the other simply prior distributions. The former is my prior belief about the distributions (pp.prior below) accounted by pop.mean and pop.scale. The latter is my prior belief about separate DDM mechanisms for each individual participant (p.prior below). p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,-5, -5, rep(0, 4)), upper = c(5, 7, 7, 1, 2, 1, 1)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,-5, -5, rep(0, 4)), upper = c(5, 7, 7, 1, 2, 1, 1)) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = rep(1, npar), p2 = rep(1, npar), upper = rep(2, npar)) names(sigma.prior) &lt;- GetPNames(model) A convention in ggdmc is to bind location and scale prior distributions as one list object. This is just for the convenience of data handling in R, which is not so convenient in C++. Note the names of each element in the list is critical. priors &lt;- list(pprior = p.prior, locaton = mu.prior, scale = sigma.prior) Then, the following run sampling. ## run the \"?\" to see the details of function options ?StartNewsamples ?run ## 7.84 mins fit0 &lt;- StartNewsamples(data=dmi, prior=priors) ## 19 mins fit &lt;- run(fit0) ## 7.54 hrs fit &lt;- run(fit, thin = 8) save(pop.mean, pop.scale, pop.prior, model, dat, dmi, npar, ps, fit0, fit, file = \"data hierarchical ggdmc_4_7_DDM.rda\") One may use the repeat function to run automatic model fit. ## 4 rounds thin &lt;- 8 repeat { hsam &lt;- run(RestartHypersamples(5e2, hsam, thin = thin), pm = 0, hpm = 0) save(pop.mean, pop.scale, pop.prior, model, dat, dmi, npar, ps, hsam0, hsam, file = \"data hierarchical ggdmc_4_7_DDM.rda\") rhats &lt;- hgelman(hsam) thin &lt;- thin * 2 if (all(rhats &lt; 1.1)) break } save(pop.mean, pop.scale, pop.prior, model, dat, dmi, npar, ps, hsam0, hsam, file = \"data hierarchical ggdmc_4_7_DDM.rda\") Similar to many standard modeling works, I must diagnose the model fit to certain I drew a reliable posterior distribution, reflecting the target distribution. I can check visually as well as calculate some statistics. First, I conducted visual checks for the trace plots and posterior distributions. Trace plots of posterior log-likelihood at hyper level Trace plots of the hyper parameters Trace plots of posterior log-likelihood at the data level Trace plots of each DDM parameters for each participants Posterior density plots (i.e., marginal posterior distributions) for the hyper parameters Posterior density plots of each DDM parameters at the data level plot(hsam, hyper = TRUE) ## 1. plot(hsam, hyper = TRUE, pll = FALSE) ## 2. plot(hsam) ## 3. plot(hsam, pll = FALSE) ## 4. plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE) ## 5. plot(hsam, pll = FALSE, den = TRUE) ## 6. These are a lot of figures to check. I have not presented the figures of posterior probability density for every participant (i.e., figure 6), because there are too many. You can print them in a pdf file to check. Then, I calculated the potential scale reduction factor, for both the hyper parameters and the parameters for each participant. rhats &lt;- hgelman(hsam, verbose = TRUE) ## Diagnosing theta for many participants separately ## Diagnosing the hyper parameters, phi ## hyper 1 2 3 4 5 6 7 8 9 10 11 12 ## 1.02 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ## 13 14 15 16 17 18 19 20 21 22 23 24 25 ## 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ## 26 27 28 29 30 31 32 33 34 35 36 37 38 ## 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ## 39 40 ## 1.01 1.01 Finally, I want to know if I do recover the data generation mechanism and true parameter values for every participant (i.e., ps). This can be achieved by using the summary function. hest1 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.mean, type = 1) hest2 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.scale, type = 2) round(hest1, 2) round(hest2, 2) ests &lt;- summary(hsam, recovery = TRUE, ps = ps) ## a sv sz t0 v.f1 v.f2 z ## True 2.00 1.00 0.30 0.30 4.00 3.00 0.50 ## 2.5% Estimate 1.83 0.69 0.22 0.28 3.69 2.76 0.49 ## 50% Estimate 2.00 0.88 0.29 0.29 3.87 2.93 0.52 ## 97.5% Estimate 2.17 0.99 0.34 0.31 4.07 3.12 0.55 ## Median-True 0.00 -0.12 -0.01 -0.01 -0.13 -0.07 0.02 ## a sv sz t0 v.f1 v.f2 z ## True 0.50 0.30 0.10 0.05 0.50 0.50 0.10 ## 2.5% Estimate 0.43 0.15 0.03 0.03 0.38 0.41 0.08 ## 50% Estimate 0.53 0.30 0.07 0.04 0.49 0.53 0.10 ## 97.5% Estimate 0.71 0.58 0.14 0.05 0.65 0.68 0.13 ## Median-True 0.03 0.00 -0.03 -0.01 -0.01 0.03 0.00 ## Summary statistics across participants ## a v.f1 v.f2 z sz sv t0 ## Mean 2.00 3.87 2.94 0.52 0.29 0.72 0.29 ## True 1.98 3.86 2.94 0.52 0.28 0.78 0.29 ## Diff -0.02 -0.01 0.00 0.00 -0.01 0.06 0.00 ## Sd 0.51 0.44 0.49 0.10 0.04 0.15 0.04 ## True 0.52 0.47 0.51 0.10 0.09 0.19 0.04 ## Diff 0.00 0.02 0.02 0.00 0.05 0.04 0.00"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hlba": {
						"id": "random-effect-model-hlba",
						"title": "HLBA Model",
						"category": "",
						"url": " /random-effect-model/hlba/",
						"content": "In this tutorial, I used the DE-MCMC sampler (Turner et al., 2013) to fit a hierarchical LBA model. This particular design has a stimulus (S) and a frequency (F) factor. Set-up Model I used the usual DMC-style syntax to set up the model. I hope, at this point, it might be clear for the reader who has read through the Model Array, that the meaning of syntax in p.map. p.map = list(A = “1”, B = “R”, t0 = “1”, mean_v = c(“F”, “M”), sd_v = “M”, st0 = “1”), library(ggdmc) model &lt;- BuildModel( p.map = list(A = \"1\", B = \"R\", t0 = \"1\", mean_v = c(\"F\", \"M\"), sd_v = \"M\", st0 = \"1\"), match.map = list(M = list(s1=1, s2=2)), factors = list(S = c(\"s1\", \"s2\"),F = c(\"f1\", \"f2\")), constants = c(sd_v.false = 1, st0 = 0), responses = c(\"r1\", \"r2\"), type = \"norm\") ## Parameter vector names (unordered) are: ( see attr(,\"p.vector\") ) ## [1] \"A\" \"B.r1\" \"B.r2\" \"t0\" ## [5] \"mean_v.f1.true\" \"mean_v.f2.true\" \"mean_v.f1.false\" \"mean_v.f2.false\" ## [9] \"sd_v.true\" ## ## Constants are (see attr(,\"constants\") ): ## sd_v.false st0 ## 1 0 ## ## Model type = norm (posdrift = TRUE ) npar &lt;- length(GetPNames(model)) mean_v = c(“F”, “M”), refers to that the mean of the drift rate is affected by the F and M factors. The former is an experimental factor and the latter is a latent LBA specific factor. B = “R” refers to that the travel distance parameter is affected by the response factor, which in a binary decision task, is two levels. Here it is either “r1” or “r2”, defined in the response option argument. responses = c(“r1”, “r2”), Therefore, the data.frame (an R way to store real or simulated data ) should have an R column, similar to the below: &gt; dplyr::tbl_df(dat) # A tibble: 40,000 x 5 s S F R RT &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; 1 1 s1 f1 r1 0.745 2 1 s1 f1 r1 0.883 3 1 s1 f1 r1 0.884 4 1 s1 f1 r1 0.678 5 1 s1 f1 r1 0.729 6 1 s1 f1 r1 0.803 7 1 s1 f1 r1 0.735 8 1 s1 f1 r1 0.756 9 1 s1 f1 r1 0.847 10 1 s1 f1 r1 0.855 # ... with 39,990 more rows s refers to subject label, S is stimulus factor, F is frequency factor, R is response factor, fct refers to factor, namely a categorical variable, dbl refers to double, namely a continuous, numerical variable. In this design, the S factor does not affect any model parameters, and the F factor affects the mean of the drift rate, mean_v = c(“F”, “M”). Because this is a parameter recovery study, I simulated a data set based on the above model. Similarly, I used a random-effects model to generate the data set, so I defined a set of population distribution for each LBA parameters. The names of the parameter were reported by the BuildModel function. Simulate Data pop.mean &lt;- c(A=.4, B.r1=.6, B.r2=.8, t0=.3, mean_v.f1.true=1.5, mean_v.f2.true=1, mean_v.f1.false=0, mean_v.f2.false=0, sd_v.true = .25) pop.scale &lt;-c( rep(.1, 3), .05, rep(.2, 4), .1) names(pop.scale) &lt;- names(pop.mean) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 3), .1, rep(NA, 4), 0), upper = c(rep(NA, 3), 1, rep(NA, 5))) dat &lt;- simulate(model, nsim = 30, nsub = 40, prior = pop.prior) dmi &lt;- BuildDMI(dat, model) ps &lt;- attr(dat, \"parameters\") Then I set up prior distributions and hyper-prior distributions. ### FIT RANDOM EFFECTS p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*5, lower = c(0,0,0,.1,NA,NA,NA,NA,0), upper = c(NA,NA,NA,NA,NA,NA,NA,NA,NA)) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = c(1,1,1,1,2,2,2,2,1), lower = c(0,0,0,.1,NA,NA,NA,NA,0), upper = c(NA,NA,NA,NA,NA,NA,NA,NA,NA)) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = c(A=1, B.r1=1, B.r2=1, t0=1, mean_v.f1.true=1, mean_v.f2.true=1, mean_v.f1.false=1, mean_v.f2.false=1, sd_v.true = 1), p2 = rep(1, npar)) ###### Must names \"pprior\", \"location\", and \"scale\" to the prirors list ####### priors &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior) ###### Must names \"pprior\", \"location\", and \"scale\" to the prirors list ####### Sampling Next, I started the sampling. When the debug argument is set TRUE, the run function uses the conventional DE-MCMC sampler, with the its original migration operator. fit0 &lt;- StartNewsamples(dmi, priors) fit &lt;- run(fit0, thin = 8) Model Diagnosis Next, I checked if the model has converged and analyze the model estimation. In this tutorial, I did the numerical checks firstly by calculating the potential scale reduction factors (Brook &amp; Gelman,1998). All are less than 1.05, suggesting all chains are well mixed. rhat &lt;- hgelman(fit, verbose = TRUE) ## hyper 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 1.02 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 28 29 30 31 32 33 34 35 36 37 38 39 40 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 Then, I calculated effective samples at the hyper parameters, for one participant at the parameter of the data level, and similarly for all participants. This is to check if enough posterior samples are drawn. hes &lt;- effectiveSize(hsam, hyper = TRUE) ## ## A.h1 B.r1.h1 B.r2.h1 t0.h1 ## 4655 1416 1152 2061 ## mean_v.f1.true.h1 mean_v.f2.true.h1 mean_v.f1.false.h1 mean_v.f2.false.h1 ## 1748 2828 1504 1505 ## sd_v.true.h1 A.h2 B.r1.h2 B.r2.h2 ## 3997 3406 3611 2630 ## t0.h2 mean_v.f1.true.h2 mean_v.f2.true.h2 mean_v.f1.false.h2 ## 4906 5329 4615 4342 ## mean_v.f2.false.h2 sd_v.true.h2 ## 4481 3887 es1 &lt;- effectiveSize(hsam[[1]]) ## A B.r1 B.r2 t0 mean_v.f1.true ## 13531 2819 2629 6100 3755 ## mean_v.f2.true mean_v.f1.false mean_v.f2.false sd_v.true ## 3793 4983 4239 6572 es &lt;- effectiveSize(hsam) round(apply(data.frame(es), 1, mean)) round(apply(data.frame(es), 1, sd)) round(apply(data.frame(es), 1, max)) round(apply(data.frame(es), 1, min)) ## A B.r1 B.r2 t0 mean_v.f1.true ## Mean 11159 2972 2726 6577 4055 ## SD 2728 295 172 903 562 ## MAX 13967 3778 3196 8183 5273 ## MIN 4525 2382 2331 4581 3092 ## mean_v.f2.true mean_v.f1.false mean_v.f2.false sd_v.true ## 3654 4793 4683 8110 ## 598 435 455 1748 ## 5056 5582 5699 13163 ## 2647 3960 3865 5830 Then I did the visual check by plotting the six types of trace and density plots. Firstly, I plot the hyper parameters. By entering TRUE to the option, save, you save the data in a data.table format1, for further processing. For instance, you can change the figure to fit the publication requirement. Trace plots of posterior log-likelihood at hyper level Trace plots of the hyper parameters Posterior density plots the hyper parameters plot(hsam, hyper = TRUE) DT1 &lt;- plot(hsam, hyper = TRUE, pll = FALSE, save = TRUE) plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE) Next I checked the trace plots of the posterior log-likelihood and each of the LBA parameters at the data level. Trace plots of posterior log-likelihood at the data level Trace plots of the LBA parameters for each participants plot(hsam) plot(hsam, pll = FALSE) Last, I checked the (posterior) density plots at the data level. There are too many density plots for each participants (nsubject x nparameter = 360), so I did not present them here. Posterior density plots the LBA parameters for each parameters Because this is a parameter recovery study, in the following, I used summary function to check whether Bayesian estimates do recover the true parameters of all participant as well as the mechanism of data generation, namely, pop.mean and pop.scale. There are five arguments in the summary function you need to know to trigger the smart parameter recovery computation. First is hyper = TRUE, which calculate the phi array matrix, which stores hyper parameters. Second is the recovery = TRUE, which informs the function to look for a true parameter vector, which you should enter it at ps argument (ps = pop.mean). Otherwise, the function will throw an error message, complaining that it cannot find the true parameter vector. est1 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1) ## Error in summary_recoverone(samples, start, end, ps, digits, verbose) : ## Names of p.vector do not match parameter names in samples The fourth argument is type = 1. This is a hyper parameter recovery specific. This is to recover the location (mostly mean) parameters. When type = 2, the function will attempt to recover the scale parameters, which mostly refer to standard deviations. The last useful argument is verbose = TRUE, printing message. est1 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.mean, type = 1) ## No print, the estimates are stored in est1. est2 &lt;- summary(hsam, hyper = TRUE, recovery = TRUE, ps = pop.scale, type = 2, verbose = TRUE) ## Storing estmates in est2 and print results rounding to the second digits ## A B.r1 B.r2 mean_v.f1.false mean_v.f1.true mean_v.f2.false ## True 0.10 0.10 0.10 0.20 0.20 0.20 ## 2.5% Estimate 0.10 0.09 0.02 0.15 0.17 0.19 ## 50% Estimate 0.14 0.12 0.06 0.22 0.21 0.26 ## 97.5% Estimate 0.19 0.16 0.10 0.30 0.27 0.34 ## Median-True 0.04 0.02 -0.04 0.02 0.01 0.06 ## mean_v.f2.true sd_v.true t0 ## True 0.20 0.10 0.05 ## 2.5% Estimate 0.21 0.11 0.03 ## 50% Estimate 0.26 0.14 0.04 ## 97.5% Estimate 0.34 0.21 0.06 ## Median-True 0.06 0.04 -0.01 By checking the Median-True, namely 50% quantile minus true values, I can confirm that I did recover the true hyper scale parameters. The lines, 2.5% Estimate and 97.5% Estimate inform that the 95% credible intervals, cover the true hyper parameters well. As expected the A and B parameters are sometimes at the boundaries of the 95% intervals, as they are linearly correlated. More often, researchers concern the location estimates, which I printed out the results stored in est1. A B.r1 B.r2 mean_v.f1.false mean_v.f1.true mean_v.f2.false True 0.40 0.60 0.80 0.00 1.50 0.00 2.5% Estimate 0.35 0.61 0.83 0.00 1.47 0.00 50% Estimate 0.40 0.68 0.91 0.16 1.58 0.17 97.5% Estimate 0.45 0.76 0.99 0.31 1.68 0.32 Median-True 0.00 0.08 0.11 0.16 0.08 0.17 mean_v.f2.true sd_v.true t0 True 1.00 0.25 0.30 2.5% Estimate 1.04 0.17 0.26 50% Estimate 1.14 0.25 0.28 97.5% Estimate 1.25 0.30 0.30 Median-True 0.14 0.00 -0.02 The estimates look pretty good. My estimation correctly reflects the difference of the two threshold-related parameters. The true values are B.r1 = .6 and _B.r2 = .8. The estimates are B.r1 = .68 and _B.r2 = .91 (B.r2 &gt; B.r1). Even the difference is very close (0.20 vs. 0.23). As expected, the mean drift rates for the error accumulators (e.g., mean_v.f1.false) are very difficult to estimate, because I set my prior distributions belief truncated at the 0 boundary, reflect how prior belief may affect the estimate. More important is the estimates of the mean drift rate for the correct accumulators (e.g., mean_v.f1.true). Again the condition f1 (e.g., high word frequency) has faster drift than the condition f2 (e.g., low word frequency). The estimates, f1 = 1.58 &gt; f2 = 1.14, closely match the true values f1 = 1.50 &gt; f2 = 1.00. There are more useful options in the summary function, which I will return to in a later tutorial. hest &lt;- summary(hsam, hyper = TRUE, hmeans = TRUE) hest &lt;- summary(hsam, hyper = TRUE, hci = TRUE, prob = c(\"25%\", \"75%\")) hest &lt;- summary(hsam, hyper = TRUE, hci = TRUE, prob = c(\"25%\", \"75%\"), digits = 3) Posterior predictive check You can also pipe the result to DMC to use its h.post.predict.dmc to conduct posterior predictive check at the hyper parameter. Note this will take a while, because piping back to DMC means using R language to process large Bayesian MCMC data. Further tutorials for DMC, please refer to Heathcote and colleagues (2018). setwd(\" media yslin MERLIN Documents DMCpaper \") source (\"dmc dmc.R\") load_model (\"LBA\",\"lba_B.R\") setwd(\" media yslin MERLIN Documents ggdmc_paper \") hpp &lt;- h.post.predict.dmc(hsam) plot.pp.dmc(hpp) tmp &lt;- lapply(hpp, function(x){plot.pp.dmc(x, style = \"cdf\") }) Reference Heathcote, A., Lin, Y.-S., Reynolds, A., Strickland, L., Gretton, M. &amp; Matzke, D., (2018). Dynamic model of choice. Behavior Research Methods. https: doi.org 10.3758 s13428-018-1067-y. a different way to store and manipulation data in R. &#8617;"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-hpm": {
						"id": "random-effect-model-hpm",
						"title": "HPM Model",
						"category": "",
						"url": " /random-effect-model/hpm/",
						"content": "Disclaimer: This tutorial is to fit Strickland et al’s (2018) PM model. For any questions regarding the model, please contact luke.strickland@uwa.edu.au Here we continue the PM tutorial to show how to conduct a hierarchical PM model. FR &lt;- list(S = c(\"n\",\"w\",\"p\"), cond=c(\"C\",\"F\", \"H\"), R=c(\"N\", \"W\", \"P\")) lev &lt;- c(\"CnN\",\"CwN\", \"CnW\",\"CwW\", \"FnN\",\"FwN\",\"FpN\", \"FnW\",\"FwW\",\"FpW\", \"fa\",\"FpP\", \"HnN\",\"HwN\",\"HpN\", \"HnW\",\"HwW\",\"HpW\", \"HpP\", \"FAKERATE\") map_mean_v &lt;- ggdmc:::MakeEmptyMap(FR, lev) map_mean_v[1:27] &lt;- c( \"CnN\",\"CwN\",\"FAKERATE\", \"FnN\",\"FwN\",\"FpN\", \"HnN\",\"HwN\",\"HpN\", \"CnW\",\"CwW\",\"FAKERATE\", \"FnW\",\"FwW\",\"FpW\", \"HnW\",\"HwW\",\"HpW\", \"FAKERATE\",\"FAKERATE\",\"FAKERATE\", \"fa\",\"fa\",\"FpP\", \"fa\",\"fa\",\"HpP\") model0 &lt;- BuildModel( p.map = list(A = \"1\", B = c(\"cond\", \"R\"), t0 = \"1\", mean_v = c(\"MAPMV\"), sd_v = \"1\", st0 = \"1\", N = \"cond\"), match.map = list(M = list(n = \"N\", w = \"W\", p = \"P\"), MAPMV = map_mean_v), factors = list(S = c(\"n\",\"w\",\"p\"), cond = c(\"C\",\"F\", \"H\")), constants = c(N.C = 2, N.F = 3, N.H = 3, st0 = 0, B.C.P = Inf, mean_v.FAKERATE = 1, sd_v = 1), responses = c(\"N\", \"W\", \"P\"), type = \"norm\") npar &lt;- length(GetPNames(model0)) pop.mean &lt;- c(A = .3, B.C.N = 1.3, B.F.N = 1.3, B.H.N = 1.3, B.C.W = 1.3, B.F.W = 1.4, B.H.W = 1.5, B.F.P = 1.1, B.H.P = 1.3, t0=.1, mean_v.CnN = 2.8, mean_v.CwN = -0.3, mean_v.CnW=-1, mean_v.CwW = 2.9, mean_v.FnN = 2.8, mean_v.FwN=-.3, mean_v.FpN = -1.6, mean_v.FnW = -1, mean_v.FwW = 2.9, mean_v.FpW = .5 , mean_v.fa = -2.4, mean_v.FpP = 2.5, mean_v.HnN = 2.8, mean_v.HwN = -.5, mean_v.HpN = -.6, mean_v.HnW = -.7, mean_v.HwW = 3.0, mean_v.HpW = 1.6, mean_v.HpP = 2.3) pop.scale &lt;-c(A = .05, B.C.N = .05, B.F.N = .05, B.H.N = .05, B.C.W = .05, B.F.W = .05, B.H.W = .05, B.F.P = .05, B.H.P = .05, t0=.05, mean_v.CnN = .05, mean_v.CwN = .05, mean_v.CnW = .05, mean_v.CwW = .05, mean_v.FnN = .05, mean_v.FwN = .05, mean_v.FpN = .05, mean_v.FnW = .05, mean_v.FwW = .05, mean_v.FpW = .05, mean_v.fa = .05, mean_v.FpP = .05, mean_v.HnN = .05, mean_v.HwN = .05, mean_v.HpN = .05, mean_v.HnW = .05, mean_v.HwW = .05, mean_v.HpW = .05, mean_v.HpP = .05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", 29), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 9), .1, rep(NA, 19)), upper = c(rep(NA,9), 1, rep(NA, 19))) dat0 &lt;- simulate(model0, nsub = 20, nsim = 30, prior = pop.prior) dmi0 &lt;- BuildDMI(dat0, model0) ps0 &lt;- attr(dat0, \"parameters\") pname &lt;- GetPNames(model0) p.prior &lt;- BuildPrior( dists = c(rep(\"tnorm\", 9), \"beta\", rep(\"tnorm\", 19)), p1 = rep(1, npar), p2 = c(rep(2, 9), 1, rep(2, 19)), lower = c(rep(0, 10), rep(NA, 19)), upper = c(rep(NA, 9), 1, rep(NA, 19))) mu.prior &lt;- BuildPrior( dists = c(rep(\"tnorm\", 9), \"beta\", rep(\"tnorm\", 19)), p1 = rep(1, npar), p2 = c(rep(2, 9), 1, rep(2, 19)), lower = c(rep(0, 10), rep(NA, 19)), upper = c(rep(NA, 9), 1, rep(NA, 19))) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = rep(1, npar), p2 = rep(1, npar)) names(p.prior) &lt;- pname names(mu.prior) &lt;- pname names(sigma.prior) &lt;- pname priors0 &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior) fit0 &lt;- StartNewsamples(dmi0, priors0, thin = 2) ## 2 * 56 mins fit0_correct &lt;- run(fit0, thin = 2) save(fit0, fit0_correct, model0, dat0, dmi0, ps0, file = \"tests Group2 hPM1.RData\") rhat0 &lt;- hgelman(fit0) dev &lt;- DIC(fit0_correct, BPIC = TRUE) Reference Strickland, L., Loft, S., Remington, R. W., &amp; Heathcote, A. (2018). Racing to remember: A theory of decision control in event-based prospective memory. Psychological Review, 125(6), 851-887. http: dx.doi.org 10.1037 rev0000113"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-shooting-decision1": {
						"id": "random-effect-model-shooting-decision1",
						"title": "Shooting Decision Model - Recovery Study",
						"category": "",
						"url": " /random-effect-model/shooting-decision1/",
						"content": "Pleskac, Cesario and Johnson (2017) examined a thorny issue in U.S.A (Wong, 2016; Chinese Community Reels After Brooklyn NYPD Shooting, 2014; American’s police on trial, 2014), whether a police officer’s decision, to shoot or not to shoot, is affected by the race of a shooting target and many other related factors. This is an important question that cognitive experiments might be able to provide some insights. They analyzed four data sets with the hierarchical Wiener diffusion model. They kindly provide their data and JAGS codes at their project OSF. You may also want to read their article, which describes the findings. A previous study, fitting data from also a first-person-shooter task with fixed-effect DDM, is reported in Correll, Wittenbrink, Crawford and Sadler (2015). In this tutorial, I used the data in Pleskac, Cesario and Johnson (2017) to demonstrate how to use pMCMC to fit hierarchical Wiener diffusion to empirical data, although one might account for the decision scenario more relatistically by using the urgency-gating model (Cisek, Puskas, &amp; El-Murr, 2009). That says, I did not argue the urgency- gating model is better than the HDDM, as the story about the LCA model suggested (Miletic, Turner, Forstmann, &amp; van Maanen, 2017). Only when we put them into tests can we know better whether the urgency-gating model is better than the HDDM to account for the shooting decisions. In the case of fitting empirical data I need to rely on other techniques, for example posterior predective check (Gelman, Carlin, Stern, Dunson, Vehtari, &amp; Rubin, 2014), to check whether posterior distributions appropriately reflect target distributions, because I do not know the true data-generation mechanism. Set-up a model object First, I defined a race-threshold model,based on the stereotype findings in classic social psychology literature (Duncan, 1976; Sagar &amp; Schofield, 1990). One question related to this classic observation is that whether this stereotype affects people’s decision threshold, their decision rate, or both. In the first model,I set up a race-threshold model as model1. Below is a list of the abbreviations for each experimental factor. RACE: the stimulus shows an African American (A) or a European American (E) S: the stimulus shows one holding a gun (G) or other object (N, not a gun) R: response to “shoot” or “not” to shoot Below is a list of the abbrevations of the DDM parameters. a: the boundary separation v: the mean of the drift rate z: the mean of the starting point of the diffusion relative to threshold separation d: differences in the non-decisional component between upper and lower threshold sz: the width of the support of the distribution of zr sv: the standard deviation of the drift rate t0: the mean of the non-decisional component of the response time st0: the width of the support of the distribution of t0 a = RACE refers to the model assumes that the race factor affects the a parameter. In R style formula, this may look like, a ~ RACE. match.map = list(M = list(G = “shoot”, N = “not”)), stands for that when a trial records a stimuls type as “G”, and a response of “shoot”, it will be coded (by BuildModel) as correct response (TRUE), otherwise error response (FALSE). factors = list(S = c(“G”, “N”), RACE = c(“E”, “A”)), stands for that experiment has two factors, S and RACE, each of them has two levels. The former has a G level for holding a gun object, and a N level for holding a nongun object and the latter has a E level for European American and and A level for African American, levels. library(ggdmc) model &lt;- BuildModel( p.map = list(a = \"RACE\", v = \"S\", z = \"1\", d = \"1\", sz = \"1\", sv = \"1\", t0 = \"1\", st0 = \"1\"), match.map = list(M = list(G = \"shoot\", N = \"not\")), factors = list(S = c(\"G\", \"N\"), RACE = c(\"E\", \"A\")), constants = c(st0 = 0, d = 0), responses = c(\"shoot\", \"not\"), type = \"rd\") npar &lt;- length(GetPNames(model)) I conducted a parameter recovery study to certain that the model can fit the data properly beforce I fit to the real data. ## Population distribution pop.mean &lt;- c(a.E = 1.5, a.A = 2.5, v.G = 3, v.N = 2, z = .5, sz = .3, sv = 1, t0 = .2) pop.scale &lt;- c(a.E =.5, a.A = .8, v.G = .5, v.N = .5, z = .1, sz = .1, sv = .3, t0=.05) pop.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale, lower = c(rep(0, 2), rep(-5, 2), rep(0, 4)), upper = c(rep(5, 2), rep(7, 2), rep(2, 4))) As usual, I want to visually check if the assumed mechanism is reasonable. plot(pop.prior) I loaded the empirical data to see the trial number and participant number in the empirical data. load(\"data race study3.rda\") dplyr::tbl_df(study3) ## A tibble: 12,033 x 7 ## RT S B CT RACE R s ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0.753 gun blur safe black not 11 ## 2 0.851 non blur safe white shoot 11 ## 3 0.742 gun clear safe black not 11 ## 4 0.636 non clear safe white shoot 11 ## 5 0.644 gun blur safe black shoot 11 ## 6 0.625 non clear safe black not 11 ## 7 0.889 non clear safe white shoot 11 ## 8 0.597 gun blur safe black not 11 ## 9 0.724 gun clear safe white not 11 ## 10 0.656 non blur safe white shoot 11 ## ... with 12,023 more rows By using the internal function .N in data.table, I knew the actual trial number in a design cell is very small (6 to 33 trials) in study 3. One more reason that I tested only RACE and S factor, which give trial number between 50 to 93. Later, I will tested another model, which also has B and CT factors, to see whether the HDDM still returns good estimates when the trial numbers are very small. s: subject id S: stimulus factor B: object factor: blurrd or clear view CT: context factor: safe or dangerous neighbor study3[, .N, .(s, S, B, CT, RACE)] ## ## s S B CT RACE N ## 1: 11 gun blur safe black 22 ## 2: 11 non blur safe white 23 ## 3: 11 gun clear safe black 19 ## 4: 11 non clear safe white 18 ## 5: 11 non clear safe black 21 ## --- ## 604: 348 gun clear danger black 17 ## 605: 348 non clear danger white 28 ## 606: 348 gun blur danger white 20 ## 607: 348 non clear danger black 16 ## 608: 348 gun blur danger black 23 range(study3[, .N, .(s, S, B, CT, RACE)]$N) ## [1] 6 33 study3[, .N, .(s, S, RACE)] # s S RACE N # 1: 11 gun black 82 # 2: 11 non white 82 # 3: 11 non black 78 # 4: 11 gun white 78 # 5: 19 gun white 83 # --- # 148: 344 non black 84 # 149: 348 non white 93 # 150: 348 gun black 81 # 151: 348 gun white 79 # 152: 348 non black 67 range(study3[, .N, .(s, S, RACE)]$N) ## [1] 50 93 nrow(study3[, .N, .(s)]) ## 38 subjects Then I set up the same number of subjects participants and an optimistic trial numbers. nsubject &lt;- 38 ntrial &lt;- 100 dat &lt;- simulate(model, nsim = ntrial, nsub = nsubject, p.prior = pop.prior) dmi &lt;- BindDataModel(dat, model) ps &lt;- attr(dat, \"parameters\") Next I started the sampling. This will take a few hours. ## Set-up Priors -------- p.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*10, lower = c(rep(0, 2), rep(-5, 2), rep(0, 4)), upper = c(rep(10, 2), rep(7, 2), rep(5, 4))) mu.prior &lt;- BuildPrior( dists = rep(\"tnorm\", npar), p1 = pop.mean, p2 = pop.scale*10, lower = c(rep(0, 2), rep(-5, 2), rep(0, 4)), upper = c(rep(10, 2), rep(7, 2), rep(5, 4))) sigma.prior &lt;- BuildPrior( dists = rep(\"beta\", npar), p1 = rep(1, npar), p2 = rep(1, npar), upper = rep(2, npar)) names(sigma.prior) &lt;- GetPNames(model) pp.prior &lt;- list(mu.prior, sigma.prior) ## Sampling ---------- hsam &lt;- run(StartNewHypersamples(5e2, dmi, p.prior, pp.prior, 16), pm = .3, hpm = .3) hsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 32), pm = .3, hpm = .3) ## 3 hrs hsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 16), pm = .3, hpm = .3) ## 90 mins save(model, pop.prior, nsubject, ntrial, dat, dmi, hsam, file = \"data hierarchical shoot-decision-recovery.rda\") As usual, I checked the model so as to know it is reliable. Trace plots of posterior log-likelihood at hyper level Trace plots of the hyper parameters Trace plots of posterior log-likelihood at the data level Trace plots of each DDM parameters for each participants Posterior density plots (i.e., marginal posterior distributions) for the hyper parameters Posterior density plots the DDM parameters for each parameters Brook and Gelman potential scale reduction factors plot(hsam, hyper = TRUE) ## 1. plot(hsam, hyper = TRUE, pll = FALSE) ## 2. plot(hsam) ## 3. plot(hsam, pll = FALSE) ## 4. plot(hsam, hyper = TRUE, pll = FALSE, den = TRUE) ## 5. plot(hsam, pll = FALSE, den = TRUE) ## 6. rhat &lt;- hgelman(hsam) ## 7. ## Diagnosing theta for many participants separately ## Diagnosing the hyper parameters, phi ## hyper 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 1.06 1.00 1.00 1.00 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 ## 30 31 32 33 34 35 36 37 38 ## 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.01 1.03 Then, I checked if my race-threshold model can recover the parameters. hest1 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.mean, type = 1) hest2 &lt;- summary(hsam, recovery = TRUE, hyper = TRUE, ps = pop.scale, type = 2) ests &lt;- summary(hsam, recovery = TRUE, ps = ps) ## Summary each participant separately ## a.E a.A v.G v.N z sz sv t0 ## Mean 1.58 2.61 4.12 3.00 0.48 0.32 1.02 0.22 ## True 1.69 2.50 2.94 2.08 0.51 0.33 1.02 0.19 ## Diff 0.11 -0.11 -1.18 -0.92 0.03 0.00 0.00 -0.02 ## Sd 0.49 0.80 0.39 0.59 0.10 0.12 0.39 0.05 ## True 0.46 0.82 0.44 0.51 0.10 0.09 0.35 0.05 ## Diff -0.04 0.02 0.05 -0.08 -0.01 -0.03 -0.04 0.00 lapply(list(hest1, hest2), round, 2) ## Mean ## a.A a.E sv sz t0 v.G v.N z ## True 2.50 1.50 1.00 0.30 0.20 3.00 2.00 0.50 ## 2.5% Estimate 2.32 1.39 0.58 0.09 0.20 3.88 2.75 0.44 ## 50% Estimate 2.60 1.57 0.98 0.31 0.22 4.11 3.00 0.48 ## 97.5% Estimate 2.89 1.75 1.18 0.39 0.23 4.36 3.25 0.51 ## Median-True 0.10 0.07 -0.02 0.01 0.02 1.11 1.00 -0.02 The recovery study supports the hypothesis that the race-threshold model can recover the parameters reliably. Therefore, if this model is a true model, we can use it to confirm or reject the hypothesis that a police office has a higher shoot threshold towards a black than a white target (i.e., a.A &gt; a.E). Both at the level of individual participants and at the level of hyper parameters. Of course, this presumes that the hierarchical race-threshold model is an appropriate model to better reflect the true phenomenon. That is, if the data do reflect this hypothesis, my hierarchical DDM is able to reveal it. I will return to the technique of model selection in a later tutorial. Note without highly efficient software, like ggdmc, the model selection work is very difficult to conduct. Another strength in ggdmc (as well as DMC), relative to the Python-HDDM (Wiecki, Sofer &amp; Frank, 2013) is that ggdmc estimates the DDM variabilities at the hyper level 1. I will return this particular strength of ggdmc in another tutorial. ## SD ## a.A a.E sv sz t0 v.G v.N z ## True 0.80 0.50 0.30 0.10 0.05 0.50 0.50 0.10 ## 2.5% Estimate 0.67 0.41 0.33 0.10 0.04 0.34 0.50 0.09 ## 50% Estimate 0.85 0.52 0.50 0.17 0.05 0.52 0.67 0.11 ## 97.5% Estimate 1.14 0.71 0.85 0.32 0.07 0.74 0.90 0.14 ## Median-True 0.05 0.02 0.20 0.07 0.00 0.02 0.17 0.01 Now I am ready to fit the empirical data with the race-threshold model. Reference Pleskac, T.J., Cesario, J. &amp; Johnson, D.J. (2017). How race affects evidence accumulation during the decision to shoot. Psychonomic Bulletin &amp; Review, 1-30. https: doi.org 10.3758 s13423-017-1369-6 Wong, J. C. (2016, Aprial, 18). ‘Scapegoated?’ The police killing that left Asian Americans angry – and divided. The Guardian. Chinese Community Reels After Brooklyn NYPD Shooting. (2014, December 24). NBC News. American’s police on trial. (2014, December, 11). The Economist. If one wishes to estimate the DDM variabilities at the hyper level in Python-HDDM, s he would need to modify the Python-HDDM source codes, which is possible but less convenient. &#8617;"
					}

					
				
			
		
			
				
					,
					

					"random-effect-model-shooting-decision2": {
						"id": "random-effect-model-shooting-decision2",
						"title": "Shooting Decision Model - Empirical Data",
						"category": "",
						"url": " /random-effect-model/shooting-decision2/",
						"content": "I continued the shooting decision model by fitting the empirical data in study 1 in Pleskac, Cesario and Johnson (2017). First I started from the pre-processing of the data. The aim of the pre-process is to replicate their behaviour analysis, so I can be sure that my data pre-processing is in line with theirs. First, I used a combination of sapply and table functions to check all coding numbers in the categorical variables columns. require(ggdmc) dat &lt;- fread(\"data race Study1TrialData.csv\") dplyr::tbl_df(dat) ## A tibble: 5,600 x 11 ## subject race0W1B object0NG1G conditionRaceObj conditionRace rt resp0DS1S ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 4 2 464 1 ## 2 1 1 0 2 2 658 0 ## 3 1 1 1 4 2 776 1 ## 4 1 0 1 3 1 646 1 ## 5 1 0 0 1 1 624 0 ## 6 1 0 1 3 1 518 1 ## 7 1 1 0 2 2 678 0 ## 8 1 0 1 3 1 511 1 ## 9 1 0 0 1 1 602 1 ## 10 1 1 1 4 2 808 1 ## ... with 5,590 more rows, and 4 more variables: diffusionRT &lt;dbl&gt;, ybin &lt;int&gt;, ## lowerLim &lt;dbl&gt;, upperLim &lt;dbl&gt; sapply(dat[, c(\"subject\", \"race0W1B\", \"object0NG1G\", \"conditionRaceObj\", \"conditionRace\", \"resp0DS1S\")], table) ## $subject ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 ## 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 ## 45 46 47 48 49 50 51 52 53 54 55 56 ## 100 100 100 100 100 100 100 100 100 100 100 100 ## ## $race0W1B ## ## 0 1 ## 2800 2800 ## ## $object0NG1G ## ## 0 1 ## 2800 2800 ## ## $conditionRaceObj ## ## 1 2 3 4 ## 1400 1400 1400 1400 ## ## $conditionRace ## ## 1 2 ## 2800 2800 ## ## $resp0DS1S ## ## 0 1 ## 2636 2796 Second, I relabeled their numerical coding to character strings, using ifelse and factor functions. In the “object0NG1G” for example, ifelse function finds “0” and converts it to “non”, meaning non-gun condition. Otherwise, it converts any numbers it found to “gun”, meaning gun condition. Because I have used table to check all available numbers, leaving all other number to else is OK. factor function converts the integer column (which will be interpreted as continuous variable) to categorical (i.e., nominal) variables. Next, I used the data.table way to remove the redundant columns, because I have reformatted them to follow our convention standard (e.g., using single uppercase letters referring to experimental factors). dat$S &lt;- factor(ifelse(dat$object0NG1G == 0, \"non\", \"gun\")) dat$RACE &lt;- factor(ifelse(dat$race0W1B == 0, \"white\", \"black\")) dat$R &lt;- factor(ifelse(dat$resp0DS1S == 0, \"not\", \"shoot\")) dat$RT &lt;- dat$rt 1e3 dat$s &lt;- factor(dat$subject) dat[, c(\"subject\", \"race0W1B\", \"object0NG1G\", \"conditionRaceObj\", \"conditionRace\", \"rt\", \"resp0DS1S\", \"diffusionRT\", \"ybin\", \"lowerLim\", \"upperLim\") := NULL] Real data sets often contain some abnormal responses, such as outliers, very slow, very quick, and wrong key responses. I used is.nan function to check whether the RT columns have this type of responses. is.nan returns a logical vector, indicating that if an element it found is “Not a number”, it will return FALSE, otherwise TRUE. I then added all elements in the vector to see how many TRUEs (1) are there. Logical TRUE in R is interpreted as 1, relative to logical FALSE, which is interpreted as 0. I found there are 168 such responses, which I removed them by a simple data.table syntax and stored the result as d. is.nan(dat$RT) ## FALSE FALSE FALSE FALSE FALSE FALSE ... sum(is.nan(dat$RT)) ## [1] 168 d &lt;- dat[!is.nan(dat$RT)] To help the calculation of the proportions of correct and error responses, I created two logical columns, C and error, to store whether a trial records a correct or error response. d$C &lt;- ifelse(d$S == \"gun\" &amp; d$R == \"shoot\", TRUE, ifelse(d$S == \"non\" &amp; d$R == \"not\", TRUE, ifelse(d$S == \"gun\" &amp; d$R == \"not\", FALSE, ifelse(d$S == \"non\" &amp; d$R == \"shoot\", FALSE, NA)))) d$error &lt;- ifelse(d$S == \"gun\" &amp; d$R == \"shoot\", FALSE, ifelse(d$S == \"non\" &amp; d$R == \"not\", FALSE, ifelse(d$S == \"gun\" &amp; d$R == \"not\", TRUE, ifelse(d$S == \"non\" &amp; d$R == \"shoot\", TRUE, NA)))) Next I examine how many trials in each experimental condition. This can be achieved by a simple data.table syntax. d[, .N, .(s, S, RACE)] # s S RACE N # 1: 1 gun black 25 # 2: 1 non black 25 # 3: 1 gun white 25 # 4: 1 non white 25 # 5: 2 non black 25 # --- # 220: 55 non white 25 # 221: 56 gun black 25 # 222: 56 gun white 25 # 223: 56 non black 25 # 224: 56 non white 25 Applying table function on the N column in the above resulting data table, I can check exactly the per-condition trial numbers. table(d[, .N, .(s, S, RACE)]$N) ## 19 21 22 23 24 25 ## 1 3 11 33 51 125 There are six trial numbers: 19, 21, 22, 23, 24, 25, with mostly subject-conditions combination (125) have 25 trials. nrow(d[, .N, .(s)]) unique(d$s) ## Fig. 3 source(\"~ rc data.analysis.R\") source(\"~ rc utils.R\") source(\"~ functions summarise.R\") d acc0 &lt;- summarySE(d, mv = \"error\", gvs = c(\"s\", \"RACE\", \"S\")) mrt0 &lt;- summarySE(d[C == TRUE], mv = \"RT\", gvs = c(\"s\", \"RACE\", \"S\")) ## Within se average across subjects for pc and nt figA &lt;- summarySEwithin(acc0, wvs = c(\"RACE\", \"S\"), mv = \"error\") figB &lt;- summarySEwithin(mrt0, wvs = c(\"RACE\", \"S\"), mv = \"RT\") names(figA) &lt;- c(\"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") names(figB) &lt;- c(\"RACE\", \"S\", \"N\", \"y\", \"sd\", \"se\", \"ci\") head(figA) dplyr::tbl_df(figA) levels(figA$RACE) figA$RACE &lt;- factor(figA$RACE, levels = c(\"white\", \"black\"), labels = c(\"White\", \"Black\")) figA$S &lt;- factor(figA$S, levels = c(\"non\", \"gun\"), labels = c(\"Non-Gun\", \"Gun\")) # figA$gp &lt;- factor(paste(figA$CT, figA$S), # levels = c(\"safe non\", \"safe gun\", \"danger non\", \"danger gun\"), # labels = c(\"Neutral Non-Gun\", \"Neutral Gun\", \"Dangerous Non-Gun\", \"Dangerous Gun\")) figB$RACE &lt;- factor(figB$RACE, levels = c(\"white\", \"black\"), labels = c(\"White\", \"Black\")) figB$S &lt;- factor(figB$S, levels = c(\"non\", \"gun\"), labels = c(\"Non-Gun\", \"Gun\")) # figB$gp &lt;- factor(paste(figB$CT, figB$S), # levels = c(\"safe non\", \"safe gun\", \"danger non\", \"danger gun\"), # labels = c(\"Neutral Non-Gun\", \"Neutral Gun\", \"Dangerous Non-Gun\", \"Dangerous Gun\")) # figA$parameter &lt;- \"ER\" # figB$parameter &lt;- \"RT\" # fig7 &lt;- rbind(figA, figB) # head(fig7) # Error bars represent standard error of the mean p1 &lt;- ggplot(figA, aes(x = S, y = y, fill = RACE)) + geom_bar(position = position_dodge(), color = \"black\", stat=\"identity\") + geom_errorbar(aes(ymin = y - se, ymax = y + se), width=.1, position=position_dodge(.9)) + coord_cartesian(ylim = c(0, .20)) + scale_fill_manual(values = c(\"#FFFFFF\", \"#CCCCCC\")) + ylab(\"Error Rate\") + coord_cartesian(ylim = c(0, .08)) + # facet_grid(.~BC) + theme_bw() + theme(legend.position = c(.85, .75), strip.background = element_blank(), axis.title.y = element_text(size = 20), strip.text.x = element_text(size = 18), axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_text(size = 18), axis.text.x = element_blank()) p2 &lt;- ggplot(figB, aes(x = S, y = y, fill = RACE)) + geom_bar(position = position_dodge(), color = \"black\", stat=\"identity\") + geom_errorbar(aes(ymin = y - se, ymax = y + se), width=.1, position=position_dodge(.9)) + scale_fill_manual(values = c(\"#FFFFFF\", \"#CCCCCC\")) + ylab(\"Correct Response Time (s)\") + coord_cartesian(ylim = c(.54, .65)) + # facet_grid(.~BC) + theme_bw() + theme(legend.position = \"none\", strip.text.x = element_blank(), axis.title.y = element_text(size = 20), axis.text.x = element_text(size = 18), axis.text.y = element_text(size = 18), axis.title.x = element_blank()) png(\"figs race fig3.png\", 800, 600) grid.arrange(p1, p2, ncol = 1) dev.off() save(dat, d, file = \"data race study1.rda\") load(\"data race study1.rda\") load(\"data race shoot-decision-recovery.rda\") study3_subset &lt;- study3[, c(\"s\", \"S\", \"RACE\", \"R\", \"RT\")] dplyr::tbl_df(study3_subset) ## A tibble: 12,033 x 5 ## s S RACE R RT ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 11 gun black not 0.753 ## 2 11 non white shoot 0.851 ## 3 11 gun black not 0.742 ## 4 11 non white shoot 0.636 ## 5 11 gun black shoot 0.644 ## 6 11 non black not 0.625 ## 7 11 non white shoot 0.889 ## 8 11 gun black not 0.597 ## 9 11 gun white not 0.724 ## 10 11 non white shoot 0.656 ## ... with 12,023 more rows To match the abbreviations used in the model object, I changed the “non”, and “gun” to “N” and “G” as well as “black” and “white” to “A” and “E”. study3_subset$S &lt;- factor(ifelse(study3_subset$S == “non”, “N”, “G”)) study3_subset$RACE &lt;- factor(ifelse(study3_subset$RACE == “black”, “A”, “E”)) Then I converted the data.table to data.frame, which was then bound to the model object. edat &lt;- data.frame(study3_subset); edmi &lt;- BindDataModel(edat, model) Next is just to repeat what I had done in the recovery study. path &lt;- \"data race Study3 DDM stimulus-threshold.rda\" ehsam &lt;- run(StartNewHypersamples(5e2, edmi, p.prior, pp.prior, 32), pm = .3, hpm = .3) ## 18 mins ehsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 32), pm = .3, hpm = .3) save(model, p.prior, pp.prior, pop.prior, nsubject, ntrial, dat, dmi, hsam, npar, pop.mean, pop.scale, ps, study3, edat, edmi, ehsam, counter, file = path) I then set up an automatic fitting routine to fit the model until it converges. counter &lt;- 1 repeat { ehsam &lt;- run(RestartHypersamples(5e2, hsam, thin = 32), pm = .3, hpm = .3) save(model, p.prior, pp.prior, pop.prior, nsubject, ntrial, dat, dmi, hsam, npar, pop.mean, pop.scale, ps, study3, edat, edmi, ehsam, counter, file = path) rhats &lt;- hgelman(ehsam) counter &lt;- counter + 1 thin &lt;- thin * 2 if (all(rhats &lt; 1.1) || counter &gt; 1e2) break } Model Diagnosis Potential scale reduction factor (psrf) Effective sample sizes rhats &lt;- hgelman(ehsam) # Diagnosing theta for many participants separately # Diagnosing the hyper parameters, phi # hyper 1 2 3 4 5 6 7 8 9 10 11 12 13 # 1.03 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 # 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 # 28 29 30 31 32 33 34 35 36 37 38 # 1.00 1.00 1.00 1.00 1.00 1.00 1.01 1.01 1.01 1.01 1.01 effectiveSize(ehsam, hyper = TRUE) # a.E.h1 a.A.h1 v.G.h1 v.N.h1 z.h1 sz.h1 sv.h1 t0.h1 a.E.h2 a.A.h2 v.G.h2 v.N.h2 # 2047 1985 1337 1819 1995 981 1444 2084 1724 1874 1314 1666 # z.h2 sz.h2 sv.h2 t0.h2 # 1936 1072 1267 1957 effectiveSize(ehsam, verbose = TRUE) # a.E a.A v.G v.N z sz sv t0 # MEAN 6295 5888 5464 6021 7029 5339 5895 6457 # SD 940 937 964 922 987 767 948 612 # MAX 7366 7202 6483 7403 7979 6783 7350 7372 # MIN 2042 1873 1922 2037 1922 1930 2147 3914 Trace plots for the log-posterior likelihood at the hyper level Trace plots for hyper parameters Posterior density plots for the hyper parameters Trace plots for the log-posterior likelihood at the data level Posterior density plots for the DDM parameters in the 38 participants The last figures were not presented here. They were printed separately in a pdf file for later checking. p1 &lt;- plot(ehsam, hyper = TRUE) p2 &lt;- plot(ehsam, hyper = TRUE, pll = FALSE) p3 &lt;- plot(ehsam, hyper = TRUE, pll = FALSE, den = TRUE) p4 &lt;- plot(ehsam) p5 &lt;- plot(ehsam, pll = FALSE, den = TRUE) Because this is not a parameter recovery study, the next step is to estimate the DDM parameters. In the race-threshold model, we, following Pleskac, Cesario and Johson’s (2017) hypothesis, expected to see a higher threshold (at the boundary separation parameter) for a black target than for a white target. One particular strength in the hierarchical modeling is that we can ask the question that whether this specific hypothesis happens at the population level, because the hierarchical modeling assumes the 38 participants in this study are just a small subset of people the researchers (pseudo-)randomly drew from a large population, presumably the entire population in U.S.A. This is in contrast to the fixed-effect model, which assumes each participant has her his own DDM mechanism of data generation. The following is how you may do these in ggdmc syntax. When entering hmean = TRUE, the summary function will calculate the average values for the hyper parameters. Similarly, the option, hci = TRUE triggers the calculation of credible interval at the hyper parameters. hest1 &lt;- summary(hsam, hyper = TRUE, hmean = TRUE) hest2 &lt;- summary(hsam, hyper = TRUE, hci = TRUE) # a.E.h1 a.A.h1 v.G.h1 v.N.h1 z.h1 sz.h1 sv.h1 t0.h1 # h1 1.57 2.60 4.11 3.00 0.48 0.29 0.96 0.22 # h2 0.53 0.86 0.52 0.68 0.11 0.18 0.52 0.05 # Random-effect model with multiple participants # L 2.5% 50% 97.5% S 2.5% 50% 97.5% # a.E 1.39 1.57 1.75 0.41 0.52 0.71 # a.A 2.32 2.60 2.89 0.67 0.85 1.14 # v.G 3.88 4.11 4.36 0.34 0.52 0.74 # v.N 2.75 3.00 3.25 0.50 0.67 0.90 # z 0.44 0.48 0.51 0.09 0.11 0.14 # sz 0.09 0.31 0.39 0.10 0.17 0.32 # sv 0.58 0.98 1.18 0.33 0.50 0.85 # t0 0.20 0.22 0.23 0.04 0.05 0.07 The results support the hypothesis that black targets result in higher decision threshold than white targets (a.E.h1 = 1.57 [1.39 - 1.75] &lt; a.A.h1 = 2.60 [2.32 - 2.89]). Note I can make this claim because the credible intervals for these two conditions are not overlapped. Also, as expected, the drift rate for gun objects is faster than that for the non-gun objects (v.G.h1 = 4.11 [3.88 - 4.36] &gt; v.N.h1 = 3.00 [2.75 - 3.25]). The finding of boundary separation here differs from the result in Pleskac, Cesario, &amp; Johnson (2017, Fig. 9; also Threshold separation section on page 18). They did not find threshold difference, perhaps because they analyzed four factors, resulting in small trial numbers in each condition. I showed R codes below to print this information. s: subject S: stimulus, gun vs. nongun B: blur or clear object CT: context, danger or neutral context RACE: black vs. white targets study3[, .N, .(s, S, B, CT, RACE)] ## s S B CT RACE N ## 1: 11 gun blur safe black 22 ## 2: 11 non blur safe white 23 ## 3: 11 gun clear safe black 19 ## 4: 11 non clear safe white 18 ## 5: 11 non clear safe black 21 ## --- ## 604: 348 gun clear danger black 17 ## 605: 348 non clear danger white 28 ## 606: 348 gun blur danger white 20 ## 607: 348 non clear danger black 16 ## 608: 348 gun blur danger black 23 &gt; range(study3[, .N, .(s, S, B, CT, RACE)]$N) ## [1] 6 33 In case you may be interested, I listed the estimates for each participants below. Not every participant has a higher threshold for black targets than white targets. ests &lt;- summary(hsam) round(ests, 2) ## ## a.E a.A v.G v.N z sz sv t0 ## 1 1.30 3.38 4.60 2.84 0.40 0.18 1.02 0.17 ## 2 1.25 2.66 3.16 2.99 0.45 0.52 1.97 0.17 ## 3 2.44 3.82 3.61 2.88 0.50 0.42 0.83 0.16 ## 4 2.70 2.10 3.87 2.84 0.49 0.37 0.71 0.16 ## 5 1.92 2.05 4.13 3.16 0.46 0.23 1.34 0.20 ## 6 1.59 2.56 4.10 3.40 0.41 0.27 1.35 0.26 ## 7 1.13 2.91 4.04 3.11 0.42 0.45 0.87 0.21 ## 8 1.83 2.97 4.16 2.69 0.48 0.36 0.62 0.21 ## 9 1.49 1.32 4.37 2.19 0.38 0.31 1.21 0.22 ## 10 1.28 2.88 4.80 1.94 0.48 0.14 1.31 0.27 ## 11 0.99 1.26 4.54 3.27 0.59 0.25 1.05 0.19 ## 12 1.23 4.24 4.42 3.53 0.41 0.22 1.11 0.19 ## 13 1.50 2.17 3.69 3.13 0.51 0.41 1.03 0.17 ## 14 1.90 2.07 4.20 2.59 0.42 0.50 0.63 0.26 ## 15 1.57 3.35 4.33 2.47 0.61 0.21 1.04 0.32 ## 16 1.55 2.89 3.88 3.43 0.33 0.28 1.06 0.21 ## 17 1.22 2.58 4.81 2.95 0.45 0.30 0.41 0.28 ## 18 2.23 0.31 4.37 2.74 0.33 0.49 1.21 0.18 ## 19 1.55 2.77 4.13 3.00 0.40 0.51 0.44 0.22 ## 20 1.20 2.61 4.50 4.04 0.42 0.50 0.45 0.25 ## 21 0.74 2.68 4.41 2.39 0.47 0.25 0.81 0.19 ## 22 1.78 2.89 3.42 3.97 0.64 0.17 2.02 0.14 ## 23 2.25 2.60 3.99 4.02 0.55 0.24 0.96 0.18 ## 24 1.76 2.92 4.69 2.70 0.70 0.44 0.44 0.17 ## 25 0.91 3.45 3.98 2.52 0.72 0.15 0.88 0.32 ## 26 1.33 3.12 3.72 3.56 0.29 0.55 1.17 0.25 ## 27 1.42 2.96 3.72 3.18 0.55 0.20 1.34 0.28 ## 28 1.25 1.95 4.06 1.99 0.45 0.28 1.51 0.30 ## 29 1.21 2.39 4.22 3.69 0.55 0.42 0.73 0.19 ## 30 2.45 2.01 4.71 2.54 0.39 0.19 1.31 0.21 ## 31 1.27 1.71 4.34 3.81 0.45 0.24 0.85 0.17 ## 32 0.82 2.61 3.98 3.24 0.56 0.33 0.87 0.25 ## 33 1.80 2.42 3.51 3.92 0.44 0.21 1.44 0.17 ## 34 1.93 2.49 3.97 2.70 0.62 0.37 1.42 0.32 ## 35 1.42 1.39 3.93 1.60 0.42 0.34 1.44 0.21 ## 36 2.53 4.26 3.60 2.60 0.32 0.25 0.58 0.17 ## 37 1.08 2.86 4.18 3.35 0.52 0.44 0.70 0.22 ## 38 2.06 3.61 4.24 3.11 0.63 0.34 0.61 0.22 ## Mean 1.58 2.61 4.12 3.00 0.48 0.32 1.02 0.22 Reference Pleskac, T.J., Cesario, J. &amp; Johnson, D.J. (2017). How race affects evidence accumulation during the decision to shoot. Psychonomic Bulletin &amp; Review, 1-30. https: doi.org 10.3758 s13423-017-1369-6"
					}

					
				
			
		
			
				
					,
					

					"sampling-crossover": {
						"id": "sampling-crossover",
						"title": "Crossover",
						"category": "",
						"url": " /sampling/crossover/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-demc": {
						"id": "sampling-demc",
						"title": "Differential Evolution Monte Carlo",
						"category": "",
						"url": " /sampling/demc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-demcmc": {
						"id": "sampling-demcmc",
						"title": "Differential Evolution Markov Chain Monte Carlo",
						"category": "",
						"url": " /sampling/demcmc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-dgmc": {
						"id": "sampling-dgmc",
						"title": "Distributed Genetic Monte Carlo",
						"category": "",
						"url": " /sampling/dgmc/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-genetic": {
						"id": "sampling-genetic",
						"title": "Population-based Monte Carlo",
						"category": "",
						"url": " /sampling/genetic/",
						"content": ""
					}

					
				
			
		
			
				
					,
					

					"sampling-migration": {
						"id": "sampling-migration",
						"title": "Migration",
						"category": "",
						"url": " /sampling/migration/",
						"content": "Migration operator is one crucial operator in the distributed genetic algorithm (Tanese, 1989; Hu &amp; Tsai, 2005), originated from the genetic algorithm (Holland, 1975; Goldberg, 1989). The algorithm uses a similar scheme, like chromosomes exchange gene information. Migration in MCMC computation uses the same method as in random-walk Metropolis to propose a new proposal, which is then subjected to the Metropolis decision step to accept or reject as a valid sample from a target distribution. The method of migration operator to propose a proposal is that it adds random noise vector onto a “current” parameter vector as a new proposal."
					}

					
				
			
		
			
				
					,
					

					"sampling-mutation": {
						"id": "sampling-mutation",
						"title": "Mutation",
						"category": "",
						"url": " /sampling/mutation/",
						"content": ""
					}

					
				
			
		
	};
</script>
<script src="/scripts/lunr.min.js"></script>
<script src="/scripts/search.js"></script>

			</article>
		</section>

		<script>
			document.getElementById("open-nav").addEventListener("click", function () {
				document.body.classList.toggle("nav-open");
			});

			
		</script>

		  
		</script>
	</body>
</html>
