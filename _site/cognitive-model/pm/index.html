<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>PM Model | Cognitive Models</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="PM Model" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Disclaimer: This tutorial is to fit Strickland et al’s (2018) PM model. For any questions regarding the model, please contact luke.strickland@uwa.edu.au" />
<meta property="og:description" content="Disclaimer: This tutorial is to fit Strickland et al’s (2018) PM model. For any questions regarding the model, please contact luke.strickland@uwa.edu.au" />
<link rel="canonical" href="http://localhost:4000/cognitive-model/pm/" />
<meta property="og:url" content="http://localhost:4000/cognitive-model/pm/" />
<meta property="og:site_name" content="Cognitive Models" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-03T06:11:35+00:00" />
<script type="application/ld+json">
{"datePublished":"2020-10-03T06:11:35+00:00","description":"Disclaimer: This tutorial is to fit Strickland et al’s (2018) PM model. For any questions regarding the model, please contact luke.strickland@uwa.edu.au","@type":"Article","url":"http://localhost:4000/cognitive-model/pm/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/siteicon.png"}},"headline":"PM Model","dateModified":"2020-10-03T06:11:35+00:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Cognitive Models" />

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,400italic,700,700italic|Open+Sans:400,400italic,600,600italic,700,700italic|Inconsolata:400,700">
		<link rel="stylesheet" href="/css/main.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">

		
	</head>

	<body>
	  <header>
	    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
	    
			<h1>
				<a href="/"><img src="/images/emblem.svg" width="40" height="40" alt="Cognitive Models logo"></a>
				Cognitive Models
				<button type="button" class="open-nav" id="open-nav"></button>
			</h1>

			<form action="/search/" method="get">
				<input type="text" name="q" id="search-input" placeholder="Search" autofocus>
				<input type="submit" value="Search" style="display: none;">
			</form>

			<nav >
				<ul>
					<li class="nav-item top-level ">
						
						<a href="/"></a>
					</li>
				</ul>

				<ul>
					
					
						<li class="nav-item top-level ">
							
							<a href="/BUGS/hnormal/">BUGS Examples Volumn 1</a>
							<ul>
								
									<li class="nav-item "><a href="/BUGS/hnormal/">Hierarchical Normal Model</a></li>
								
									<li class="nav-item "><a href="/BUGS/seeds/">Random effect logistic regression</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/approximation/kde/">Likelihood Free Method</a>
							<ul>
								
									<li class="nav-item "><a href="/approximation/kde/">Kernel Density Estimation</a></li>
								
									<li class="nav-item "><a href="/approximation/pda/">Probability Density Approximation</a></li>
								
									<li class="nav-item "><a href="/approximation/ppda/">Parallel Probability Density Approximation</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/basics/model_array/">Modelling Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/basics/model_array/">Model Array</a></li>
								
									<li class="nav-item "><a href="/basics/simulation/">Simulation</a></li>
								
									<li class="nav-item "><a href="/basics/descriptive/">Descriptive Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/summary/">Summary Statistics</a></li>
								
									<li class="nav-item "><a href="/basics/leastsq/">Least Square Method</a></li>
								
									<li class="nav-item "><a href="/basics/mle/">Maximising Likelihoods</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/bayes-basics/theorem/">Bayesian Basics</a>
							<ul>
								
									<li class="nav-item "><a href="/bayes-basics/theorem/">Bayes' Theorem</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/prior/">Prior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/likelihood/">Model Likelihood</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/posterior/">Posterior Distribution</a></li>
								
									<li class="nav-item "><a href="/bayes-basics/diagnosis/">Checking Fitted Models</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level current">
							
							<a href="/cognitive-model/lba/">Cognitive Model</a>
							<ul>
								
									<li class="nav-item "><a href="/cognitive-model/lba/">LBA Model</a></li>
								
									<li class="nav-item current"><a href="/cognitive-model/pm/">PM Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/plba/">PLBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm/">Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/ddm1c/">One-choice Diffusion Decision Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/cddm/">Circular Drift-diffusion Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/pddm/">PDDM</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lba3/">Three-accumulator LBA Model</a></li>
								
									<li class="nav-item "><a href="/cognitive-model/lca/">LCA Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/fixed-effect-model/one_participant/">Fixed-effects Model</a>
							<ul>
								
									<li class="nav-item "><a href="/fixed-effect-model/one_participant/">One Participant</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/many_participants/">Multiple Participants</a></li>
								
									<li class="nav-item "><a href="/fixed-effect-model/cddm12S/">CDDM</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/mcmc/mcmc/">Modern Bayesian Statistics</a>
							<ul>
								
									<li class="nav-item "><a href="/mcmc/mcmc/">Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/mcmc/rwm/">Random Walk Metropolis</a></li>
								
									<li class="nav-item "><a href="/mcmc/hastings/">Metropolis-Hastings</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/random-effect-model/hlba/">Hierarchical Model</a>
							<ul>
								
									<li class="nav-item "><a href="/random-effect-model/hlba/">HLBA Model</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hddm/">HDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision1/">Shooting Decision Model - Recovery Study</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/shooting-decision2/">Shooting Decision Model - Empirical Data</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hcddm/">HCDDM</a></li>
								
									<li class="nav-item "><a href="/random-effect-model/hpm/">HPM Model</a></li>
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="/sampling/genetic/">Sampling Techniques</a>
							<ul>
								
									<li class="nav-item "><a href="/sampling/genetic/">Population-based Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/crossover/">Crossover</a></li>
								
									<li class="nav-item "><a href="/sampling/migration/">Migration</a></li>
								
									<li class="nav-item "><a href="/sampling/mutation/">Mutation</a></li>
								
									<li class="nav-item "><a href="/sampling/demc/">Differential Evolution Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/demcmc/">Differential Evolution Markov Chain Monte Carlo</a></li>
								
									<li class="nav-item "><a href="/sampling/dgmc/">Distributed Genetic Monte Carlo</a></li>
								
							</ul>
						</li>
					
				</ul>

				<ul>
					<li class="nav-item top-level ">
						
						<a href="/changelog/">Change Log</a>
					</li>
				</ul>
			</nav>
		</header>

		<section class="main">
			<div class="page-header">
				<h2>Cognitive Model</h2>
				<h3>PM Model</h3>
			</div>
			<article class="content">
				<blockquote>
  <p>Disclaimer: This tutorial is to fit Strickland et al’s (2018) PM model. For any questions regarding the model, please contact luke.strickland@uwa.edu.au</p>
</blockquote>

<p>When we mention memory, say a childhood event happening in the past, we are often talking about the <em>retrospective memory</em>, our memory for the past. <em>Prospective memory</em>, on the other hand, refers to we memorize something in order to do it in the future. For example, we set a reminder in the calender in our mobile phone to remind ourselves to do weekly shopping, say every Friday evening. When the reminder chimes, we then associate the reminder chime with the memory of “time to do the shopping.</p>

<p>The prospective memory paradigm is a cognitive task testing such memory. It engages participants in some ongoing tasks with two basic choices. Take the lexical decision-making task (Wagenmakers, Ratcliff, Gomez, &amp; McKoon, 2008) as an example, the choices are word vs. non-word.</p>

<p>In this tutorial, we fit the prospective memory model to a simulated data set. In particular, we use a semi-factorial design from the Stricland et al (2018).  In addition to the two basic choices, word vs. non-word, the PM paradigms require participants to remember a third type of stimuli associated with a third response. This third type of stimuli is PM targets. For example, in addition to the typical word and non-word stimuli, participants occassionally were presented with a word describing an animal, like badger, otter, dolphin, wallaby, and so on, and the 
participants was instructed to choose the third response for the animal words.</p>

<h1 id="experimental-design">Experimental Design</h1>
<p>The semi-factorial design we illustrate here tested two factors. The first  factor is the stimulus factor, which has three levels, word, non-word and PM target. The second factor is the PM factor, which has three conditions. Condition 1 is the ‘control’ condition, in which participants responded to a typical 2AFC lexical decision-making task.</p>

<p>Condition 2 is a ‘focal’ PM condition, with three stimulus types - word, non-word, and PM target, each associated with three separate response types. Comparing to the third condition described later, focal PM targets are easier to detect in the context of the ongoing task. For examine, the PM targets in a focal PM condition could be dog, cat, lion, tiger, panda, kangaroos and so on, those animals that are often mentioned.</p>

<p>Condition 3 is a ‘non-focal’ PM condition, again with three types of stimuli - word, non-word, and PM target, each corresponding to three responses. Comparing to the focal condition, non-focal PM targets are more difficult to detect. Again, continuing with the animal example, non-focal PM target could be wombat, cheetah, echidna, devil, solenodon, and so on, those animals that are less frequently mentioned in everyday dialect.</p>

<p>In summary, factor 1, denoted as <em>S</em>, is a within-block manipulation of three-level stimulus type. Factor 2 is a between-block manipulation of three-level prospective memory. The three factor 2 levels are (1) no requirment to engage prospective memory, (2) easy prospective memory and (3) hard prospective memory. We denoted this factor as <em>cond</em>. In addition to the two factor, to determine whether a response is hit, correct rejection, false alarm or miss, we use a resposne factor, denoted as <em>R</em>, which has three levels, word, nonword and PM responses.</p>

<ol>
  <li><strong>S</strong>, non-word (n), word (w), PM (p)</li>
  <li><strong>cond</strong>, focal (F), non-focal (H), control (C).</li>
  <li><strong>R</strong>, nonword response (N), word response (W), PM response (P).</li>
</ol>

<blockquote>
  <p>Note we differentiate the upper- and the lower-case letters.</p>
</blockquote>

<p>We assume an accumulator model where participants swap between two- and three-accumulator architectures. The model specification results in an incomplete crossing of the two factors, as there was no PM stimuli in the control blocks of trials.</p>

<p>Table 1. Semi-factorial design.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>w</th>
      <th>n</th>
      <th>p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>C</td>
      <td>X</td>
      <td>X</td>
      <td> </td>
    </tr>
    <tr>
      <td>F</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>H</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
    </tr>
  </tbody>
</table>

<p>To set up a model object, we first create a list, named <em>FR</em>, pooling all three factors together.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FR &lt;- list(S = c("n","w","p"), cond = c("C","F","H"), R = c("N","W","P"))
</code></pre></div></div>

<p>In prospective memory paradigms, we define the false alarms with regard to the PM responses as participants commit a PM response on a non-PM trial. We observe this type of false alarm are rare, and thereby the drift rate parameter associated with the PM-false-alarm accumulator are not constrained by much data. Thus, it is a good idea to pool those rates into one PM false alarm parameter (fa).</p>

<p>Table 2. Focal and non-focal conditions. The signal-detection categorization with regard to PM targets. O and X represents correct and incorrect responses. These two categories are correct rejections with regard to PM targets.</p>

<table>
  <thead>
    <tr>
      <th>R/S</th>
      <th>w</th>
      <th>n</th>
      <th>p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>W</td>
      <td>O</td>
      <td>X</td>
      <td>miss</td>
    </tr>
    <tr>
      <td>N</td>
      <td>X</td>
      <td>O</td>
      <td>miss</td>
    </tr>
    <tr>
      <td>P</td>
      <td>fa</td>
      <td>fa</td>
      <td>hit</td>
    </tr>
  </tbody>
</table>

<p>To establish the relationship in Table 2 for the three PM conditions, we create a string vector, storing the factor levels. The <em>fa</em> label represents false alarms. We use a trick to model the situation that the control condition has no PM targets and thereby participants would not even contemplate a PM response. Therefore, it makes sense to assume in the control blocks, participants engage a two-accumulator decision-making process, rather than 3-accumulator process, which possibly happens in the two PM conditions. We create a <em>FAKERATE</em> label to signify this nuanced modelling approach.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lev &lt;- c("CnN","CwN", "CnW","CwW",
         "FnN","FwN","FpN", "FnW","FwW","FpW", "fa","FpP",
         "HnN","HwN","HpN", "HnW","HwW","HpW", "HpP", 
         "FAKERATE")
</code></pre></div></div>

<p>Table 3-1. Focal condition, using <em>lev</em> labels.</p>

<table>
  <thead>
    <tr>
      <th>R/S</th>
      <th>w</th>
      <th>n</th>
      <th>p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>W</td>
      <td>FwW</td>
      <td>FnW</td>
      <td>FpW</td>
    </tr>
    <tr>
      <td>N</td>
      <td>FwN</td>
      <td>FnN</td>
      <td>FpN</td>
    </tr>
    <tr>
      <td>P</td>
      <td>fa</td>
      <td>fa</td>
      <td>FpP</td>
    </tr>
  </tbody>
</table>

<p>Table 3-2. Non-focal condition, using <em>lev</em> labels.</p>

<table>
  <thead>
    <tr>
      <th>R/S</th>
      <th>w</th>
      <th>n</th>
      <th>p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>W</td>
      <td>HwW</td>
      <td>HnW</td>
      <td>HpW</td>
    </tr>
    <tr>
      <td>N</td>
      <td>HwN</td>
      <td>HnN</td>
      <td>HpN</td>
    </tr>
    <tr>
      <td>P</td>
      <td>fa</td>
      <td>fa</td>
      <td>HpP</td>
    </tr>
  </tbody>
</table>

<p>Table 3-3. Control condition, using <em>lev</em> labels.</p>

<table>
  <thead>
    <tr>
      <th>R/S</th>
      <th>w</th>
      <th>n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>W</td>
      <td>CwW</td>
      <td>CnW</td>
    </tr>
    <tr>
      <td>N</td>
      <td>CwN</td>
      <td>CnN</td>
    </tr>
  </tbody>
</table>

<p>Secondly, we use the <em>MakeEmptyMap</em> function to create a NA vector. Each
of the elements in the vector is lablled by the 27 full-crossed factorial combinations.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>require(ggdmc)

map_mean_v &lt;- ggdmc:::MakeEmptyMap(FR, lev)  
print(map_mean_v)
## n.C.N w.C.N p.C.N n.F.N w.F.N p.F.N n.H.N w.H.N p.H.N n.C.W w.C.W p.C.W 
## &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  

## n.F.W w.F.W p.F.W n.H.W w.H.W p.H.W n.C.P w.C.P p.C.P n.F.P w.F.P p.F.P 
## &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  

## n.H.P w.H.P p.H.P 
## &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; 

length(map_mean_v)
## [1] 27

levels(map_mean_v)
##  [1] "CnN"      "CwN"      "CnW"      "CwW"      "FnN"      "FwN"      "FpN"     
##  [8] "FnW"      "FwW"      "FpW"      "fa"       "FpP"      "HnN"      "HwN"     
## [15] "HpN"      "HnW"      "HwW"      "HpW"      "HpP"      "FAKERATE"
</code></pre></div></div>

<p>Then we manually relabel the 27 elements, rendering the control condition to model two-accumulator process and the PM conditions to model three-accumulator process. That is, we label, for example p.C.N (i.e., a non-word response to a PM target in the control block), as <em>FAKERATE</em>.  Except the five <em>FAKERATE</em> and four <em>fa</em> conditions, the other conditions just remove the dot symbol and rearrange the labelling sequence of three factors as cond-S-R.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>map_mean_v[1:27] &lt;- c(
  "CnN","CwN","FAKERATE",
  "FnN","FwN","FpN",
  "HnN","HwN","HpN",
  
  "CnW","CwW","FAKERATE",
  "FnW","FwW","FpW",
  "HnW","HwW","HpW",
  
  "FAKERATE","FAKERATE","FAKERATE",
  "fa","fa","FpP",
  "fa","fa","HpP"
)

</code></pre></div></div>

<p>The following table compares the changes of labelling.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>map_mean_v before</td>
      <td>n.C.N</td>
      <td>w.C.N</td>
      <td>p.C.N</td>
      <td>n.F.N</td>
      <td>w.F.N</td>
      <td>p.F.N</td>
      <td>n.H.N</td>
    </tr>
    <tr>
      <td>map_mean_v after</td>
      <td>CnN</td>
      <td>CwN</td>
      <td>FAKERATE</td>
      <td>FnN</td>
      <td>FwN</td>
      <td>FpN</td>
      <td>HnN</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>map_mean_v before</td>
      <td>w.H.N</td>
      <td>p.H.N</td>
      <td>n.C.W</td>
      <td>w.C.W</td>
      <td>p.C.W</td>
      <td>n.F.W</td>
      <td>w.F.W</td>
    </tr>
    <tr>
      <td>map_mean_v after</td>
      <td>HwN</td>
      <td>HpN</td>
      <td>CnW</td>
      <td>CwW</td>
      <td>FAKERATE</td>
      <td>FnW</td>
      <td>FwW</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>map_mean_v before</td>
      <td>p.F.W</td>
      <td>n.H.W</td>
      <td>w.H.W</td>
      <td>p.H.W</td>
      <td>n.C.P</td>
      <td>w.C.P</td>
      <td>p.C.P</td>
    </tr>
    <tr>
      <td>map_mean_v after</td>
      <td>FpW</td>
      <td>HnW</td>
      <td>HWW</td>
      <td>HpW</td>
      <td>FAKERATE</td>
      <td>FAKERATE</td>
      <td>FAKERATE</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>map_mean_v before</td>
      <td>n.F.P</td>
      <td>w.F.P</td>
      <td>p.F.P</td>
      <td>n.H.P</td>
      <td>w.H.P</td>
      <td>p.H.P</td>
    </tr>
    <tr>
      <td>map_mean_v after</td>
      <td>fa</td>
      <td>fa</td>
      <td>FpP</td>
      <td>fa</td>
      <td>fa</td>
      <td>HpP</td>
    </tr>
  </tbody>
</table>

<p>Instead of assigning the regular “M” factor to the mean_v parameter, which controls the LBA accumulator, we associate the mean_v parameter with the newly created <strong>map_mean_v</strong> vector. In the syntax of BuildModel, we assign the <strong>map_mean_v</strong> to a <em>MAPMV</em> object by enter a list to the <strong>match.map</strong> option.</p>

<h2 id="model-0">Model 0</h2>
<p>The following model assumes the PM condition associates with the decision threshold and the drift rate is associated with the PM, stimulus and response conditions, following the above <strong>map_mean_v</strong> set up.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model0 &lt;- BuildModel(
  p.map     = list(A = "1", B = c("cond", "R"), t0 = "1", mean_v = c("MAPMV"), 
                   sd_v = "1", st0 = "1", N = "cond"), 
  match.map = list(M = list(n = "N", w = "W", p = "P"), MAPMV = map_mean_v),
  factors   = list(S = c("n","w","p"), cond = c("C","F", "H")),
  constants = c(N.C = 2, N.F = 3, N.H = 3, st0 = 0, B.C.P = Inf, 
                mean_v.FAKERATE = 1, sd_v = 1), 
  responses = c("N", "W", "P"), 
  type      = "norm")

## Parameter vector names are: ( see attr(,"p.vector") )
##  [1] "A"          "B.C.N"      "B.F.N"      "B.H.N"      "B.C.W"      "B.F.W"     
##  [7] "B.H.W"      "B.F.P"      "B.H.P"      "t0"         "mean_v.CnN" "mean_v.CwN"
## [13] "mean_v.CnW" "mean_v.CwW" "mean_v.FnN" "mean_v.FwN" "mean_v.FpN" "mean_v.FnW"
## [19] "mean_v.FwW" "mean_v.FpW" "mean_v.fa"  "mean_v.FpP" "mean_v.HnN" "mean_v.HwN"
## [25] "mean_v.HpN" "mean_v.HnW" "mean_v.HwW" "mean_v.HpW" "mean_v.HpP"
## 
## Constants are (see attr(,"constants") ):
##             N.C             N.F             N.H             st0           B.C.P 
##               2               3               3               0             Inf 
## mean_v.FAKERATE            sd_v 
##               1               1 
## 
## Model type = norm (posdrift = TRUE )
</code></pre></div></div>

<p>A note to the value entered for the constants argument. The N.C, N.F and N.H in the constants argument represents the number of accumulators in the control, focal and non-focal conditions are 2, 3 and 3, respectively. The <strong>B.C.P</strong> represents the LBA <em>B</em> parameter (b = A + B) of the PM accumulator in the control condition is fixed at infinitve. This is another trick, signifying this particular accumulator requires infinitive amount of evidence to trigger a decision. The drift rate of the FAKERATE accumulator is set at one, which is inconsequential because its threshold is infinitive.</p>

<blockquote>
  <p>A reminder: The LBA <em>B</em> parameter represents the travelling distance of an accumulator. The threshold parameter is denoted as <script type="math/tex">b = A + B</script>. <em>A</em> is the LBA starting point parameter.</p>
</blockquote>

<p>This is a rather nuanced model object. Let’s check its internal to see how the 2 and 3 alternating accumulators are set up for the different PM condition.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npar &lt;- length(GetPNames(model0))

## Create a true parameter vector for recovery
p.vector &lt;- c(A = .3, B.C.N = 1.3,  B.F.N = 1.3,  B.H.N = 1.3,
              B.C.W = 1.3,  B.F.W = 1.4,  B.H.W = 1.5,
              B.F.P = 1.1,  B.H.P = 1.3,

              t0=.1,

              mean_v.CnN = 2.8,  mean_v.CwN = -0.3, mean_v.CnW=-1,
              mean_v.CwW = 2.9,  mean_v.FnN = 2.8,  mean_v.FwN=-.3,

              mean_v.FpN = -1.6, mean_v.FnW = -1,   mean_v.FwW = 2.9,
              mean_v.FpW = .5 ,  mean_v.fa = -2.4,  mean_v.FpP = 2.5,

              mean_v.HnN = 2.8, mean_v.HwN = -.5,   mean_v.HpN = -.6,
              mean_v.HnW = -.7, mean_v.HwW = 3.0,   mean_v.HpW = 1.6,
              mean_v.HpP = 2.3)

dat0 &lt;- simulate(model0, nsim=1e2, ps=p.vector)
dmi0 &lt;- BuildDMI(dat0, model0)

## Remember in "Model Array" tutorial, we introduce the model object is a 3-D 
## TRUE-FALSE array. Its first dimention is the factorial combination (aka 
## design cell).
dim0 &lt;- cell &lt;- dimnames(model0)[[1]]
print(dim0)
##  [1] "n.C.N" "w.C.N" "p.C.N" "n.F.N" "w.F.N" "p.F.N" "n.H.N" "w.H.N" "p.H.N" "n.C.W"
## [11] "w.C.W" "p.C.W" "n.F.W" "w.F.W" "p.F.W" "n.H.W" "w.H.W" "p.H.W" "n.C.P" "w.C.P"
## [21] "p.C.P" "n.F.P" "w.F.P" "p.F.P" "n.H.P" "w.H.P" "p.H.P"
</code></pre></div></div>

<p>TableParameters constructs an accumulator-parameter table with each row representing an accumulator and each column representing a parameter.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>acc_tab0 &lt;- TableParameters(p.vector, 1, model0, FALSE)
acc_tab1 &lt;- TableParameters(p.vector, "w.C.N", model0, FALSE)
acc_tab2 &lt;- TableParameters(p.vector, "w.F.P", model0, FALSE)

print(acc_tab0)
print(acc_tab1)
print(acc_tab2)

##     A   b  t0 mean_v sd_v st0 nacc
## 1 0.3 1.6 0.1    2.8    1   0    2
## 2 0.3 1.6 0.1   -1.0    1   0    2
## 3 0.3 Inf 0.1    1.0    1   0    2
##     A   b  t0 mean_v sd_v st0 nacc
## 1 0.3 1.6 0.1   -0.3    1   0    2
## 2 0.3 1.6 0.1    2.9    1   0    2
## 3 0.3 Inf 0.1    1.0    1   0    2
##     A   b  t0 mean_v sd_v st0 nacc
## 1 0.3 1.6 0.1   -0.3    1   0    3
## 2 0.3 1.7 0.1    2.9    1   0    3
## 3 0.3 1.4 0.1   -2.4    1   0    3
</code></pre></div></div>

<p>Note that <em>TableParameters</em> has calculated b (=A+B). The 7th column indicates the number of accumulator in this condition. It describes the condition, so the 2nd and 3rd rows in the 7th column is redundant.</p>

<p>After setting up the model, the sampling procedure is very much a routine. We set up prior distributions for the 29 parameters. In this case of a simulation study, we can check whether the prior distributions are improbable with respect to the true parameter.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pname &lt;- GetPNames(model0)
p.prior0 &lt;- BuildPrior(
  dists = c(rep("tnorm", 9), "beta", rep("tnorm", 19)),
  p1    = rep(1, npar),
  p2    = c(rep(2, 9), 1, rep(2, 19)),
  lower = c(rep(0, 10),  rep(NA, 19)),
  upper = c(rep(NA, 9), 1, rep(NA, 19)))
names(p.prior0) &lt;- pname

plot(p.prior0, ps = p.vector)

## Sampling. We turned off the block-sampling to update an entire parameter at once.
## The block-sampling method updates only some of the parameters in a parameter vector.  
fit0 &lt;- StartNewsamples(dmi, p.prior0, block = FALSE, thin=2)
fit0_correct  &lt;- run(fit0, thin=2, block = FALSE)
hat  &lt;- gelman(fit0_correct, verbose=TRUE);
est  &lt;- summary(fit0_correct, recovery = TRUE, ps = p.vector, verbose = TRUE)
</code></pre></div></div>

<h2 id="model-1">Model 1</h2>
<p>To test the role of the PM condition on its influence on the decision threshold, we fit a second model assuming no association between the threshold and the PM condition.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## map_mean_v is identical as before
model1 &lt;- ggdmc:::BuildModel(
  p.map     = list(A = "1", B = "R", t0 = "1", mean_v = "MAPMV",
                   sd_v = "1", st0 = "1", N = "cond"),
  match.map = list(M = list(n = "N", w = "W", p = "P"),
                   MAPMV = map_mean_v),
  factors   = list(S = c("n","w","p"), cond = c("C","F", "H")),
  constants = c(N.C = 2, N.F = 3, N.H = 3, st0 = 0,
                mean_v.FAKERATE = 1, sd_v = 1),
  responses = c("N", "W", "P"),
  type      = "norm")

ggdmc:::GetPNames(model1)

## Set up a different p.vector to test whether we can also recover this set.
p.vector &lt;- c(A = .5, B.N = 1.2,  B.W = 1,  B.P = 1.5,
              t0=.15,

              mean_v.CnN = 1.25, mean_v.CwN = .35, mean_v.CnW = .25,
              mean_v.CwW = 1.15, mean_v.FnN = 1.8, mean_v.FwN = .35,

              mean_v.FpN = .12, mean_v.FnW = .11, mean_v.FwW = 1.32,
              mean_v.FpW = 1.33, mean_v.fa = -1.2, mean_v.FpP = 1.45,

              mean_v.HnN = 1.67, mean_v.HwN = .14, mean_v.HpN = .23,
              mean_v.HnW = .3, mean_v.HwW = 1.21, mean_v.HpW = 1.5,
              mean_v.HpP = 1.11)


dat1 &lt;- simulate(model1, nsim=1e2, ps=p.vector)
dmi1 &lt;- BuildDMI(dat1, model)

pname &lt;- ggdmc:::GetPNames(model1)
npar &lt;- length(pname)
p.prior1 &lt;- ggdmc:::BuildPrior(
  dists = c(rep("tnorm", 4), "beta", rep("tnorm", 19)),
  p1    = rep(1, npar),
  p2    = c(rep(2, 4), 1, rep(2, 19)),
  lower = c(rep(0, 5),  rep(NA, 19)),
  upper = c(rep(NA, 4), 1, rep(NA, 19)))
names(p.prior1) &lt;- pname
plot(p.prior1, ps = p.vector)

fit0 &lt;- StartNewsamples(dmi1, p.prior1, block = FALSE, thin=2)
fit1_correct  &lt;- run(fit0, thin=4, block = FALSE)
hat  &lt;- gelman(fit1_correct, verbose=TRUE);
est  &lt;- summary(fit1_correct, recovery = TRUE, ps = p.vector, verbose = TRUE)
#                    A   B.N   B.P  B.W mean_v.CnN mean_v.CnW mean_v.CwN mean_v.CwW
# True            0.50  1.20  1.50 1.00       1.25       0.25       0.35       1.15
# 2.5% Estimate   0.02  0.66  0.65 0.66       0.99      -0.56      -0.56       0.65
# 50% Estimate    0.39  1.16  1.17 1.13       1.33      -0.02      -0.02       1.00
# 97.5% Estimate  1.09  1.50  1.65 1.46       1.64       0.45       0.44       1.32
# Median-True    -0.11 -0.04 -0.33 0.13       0.08      -0.27      -0.37      -0.15
#                mean_v.fa mean_v.FnN mean_v.FnW mean_v.FpN mean_v.FpP mean_v.FpW
# True               -1.20       1.80       0.11       0.12       1.45       1.33
# 2.5% Estimate      -2.66       1.42      -1.01       0.02       0.88       0.74
# 50% Estimate       -1.56       1.74      -0.33       0.54       1.49       1.12
# 97.5% Estimate     -0.75       2.04       0.26       0.99       2.01       1.48
# Median-True        -0.36      -0.06      -0.44       0.42       0.04      -0.21
#                mean_v.FwN mean_v.FwW mean_v.HnN mean_v.HnW mean_v.HpN mean_v.HpP
# True                 0.35       1.32       1.67       0.30       0.23       1.11
# 2.5% Estimate       -0.45       0.84       1.09      -0.24      -0.37       0.47
# 50% Estimate         0.11       1.18       1.41       0.27       0.20       1.10
# 97.5% Estimate       0.57       1.49       1.72       0.70       0.68       1.66
# Median-True         -0.24      -0.14      -0.26      -0.03      -0.03      -0.01
#                mean_v.HpW mean_v.HwN mean_v.HwW   t0
# True                 1.50       0.14       1.21 0.15
# 2.5% Estimate        0.84      -0.63       0.58 0.10
# 50% Estimate         1.20      -0.11       0.94 0.16
# 97.5% Estimate       1.54       0.36       1.28 0.26
# Median-True         -0.30      -0.25      -0.27 0.01

</code></pre></div></div>

<h1 id="model-comparison">Model Comparison</h1>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Use model 0 to fit data generated by model 1
fit0 &lt;- StartNewsamples(dmi0_wrong, p.prior0, block = FALSE, thin=2)
fit0_wrong  &lt;- run(fit0, thin=4, block = FALSE)
hat  &lt;- gelman(fit0_wrong, verbose=TRUE);

## Use model 1 to fit data generated by model 0
fit0 &lt;- StartNewsamples(dmi1_wrong, p.prior1, block = FALSE, thin=2)
fit1_wrong  &lt;- run(fit0, thin=4, block = FALSE)
hat  &lt;- gelman(fit1_wrong, verbose=TRUE);

## This compares using model0 and model 1 to fit dat0.
## The cond factor does not affect the B parameter, because the DIC difference 
## is small.
DIC(fit0_correct); DIC(fit1_wrong)
# [1] 319.4222
# [1] 314.2239

## This compares using model0 and model 1 to fit dat1.
## The cond factor does not affect the B parameter,  because the DIC difference 
## is small.
DIC(fit1_correct); DIC(fit0_wrong); 
# [1] 2078.136
# [1] 2080.449
</code></pre></div></div>

<h1 id="modelling-data-from-multiple-participants">Modelling Data from Multiple Participants</h1>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Model 1 --------------------------------------------
## 27 elements with 20 levels
## Population distribution, 
pop.mean &lt;- c(A = .3, B.C.N = 1.3,  B.F.N = 1.3,  B.H.N = 1.3,
               B.C.W = 1.3,  B.F.W = 1.4,  B.H.W = 1.5,
               B.F.P = 1.1,  B.H.P = 1.3,

               t0=.1,

               mean_v.CnN = 2.8,  mean_v.CwN = -0.3, mean_v.CnW=-1,
               mean_v.CwW = 2.9,  mean_v.FnN = 2.8,  mean_v.FwN=-.3,

               mean_v.FpN = -1.6, mean_v.FnW = -1,   mean_v.FwW = 2.9,
               mean_v.FpW = .5 ,  mean_v.fa = -2.4,  mean_v.FpP = 2.5,

               mean_v.HnN = 2.8, mean_v.HwN = -.5,   mean_v.HpN = -.6,
               mean_v.HnW = -.7, mean_v.HwW = 3.0,   mean_v.HpW = 1.6,
               mean_v.HpP = 2.3)

pop.scale &lt;-c(A = .05, B.C.N = .05,  B.F.N = .05,  B.H.N = .05,
              B.C.W = .05,  B.F.W = .05,  B.H.W = .05,
              B.F.P = .05,  B.H.P = .05,
              t0=.05,

               mean_v.CnN = .05,  mean_v.CwN = .05, mean_v.CnW = .05,
               mean_v.CwW = .05,  mean_v.FnN = .05,  mean_v.FwN = .05,

               mean_v.FpN = .05, mean_v.FnW = .05,   mean_v.FwW = .05,
               mean_v.FpW = .05,  mean_v.fa = .05,  mean_v.FpP = .05,

               mean_v.HnN = .05, mean_v.HwN = .05,   mean_v.HpN = .05,
               mean_v.HnW = .05, mean_v.HwW = .05,   mean_v.HpW = .05,
               mean_v.HpP = .05)

pop.prior &lt;- BuildPrior(
   dists = rep("tnorm", 29),
   p1 = pop.mean,
   p2 = pop.scale,
   lower = c(rep(0, 9), .1, rep(NA, 19)),
   upper = c(rep(NA,9),  1, rep(NA, 19)))

dat0 &lt;- simulate(model0, nsub = 12, nsim = 50, prior = pop.prior)
dmi0 &lt;- BuildDMI(dat0, model0)
ps0 &lt;- attr(dat0, "parameters")

pname &lt;- GetPNames(model0)
p.prior &lt;- BuildPrior(
   dists = c(rep("tnorm", 9), "beta", rep("tnorm", 19)),
   p1    = rep(1, npar),
   p2    = c(rep(2, 9), 1, rep(2, 19)),
   lower = c(rep(0, 10),  rep(NA, 19)),
   upper = c(rep(NA, 9), 1, rep(NA, 19)))
mu.prior &lt;- BuildPrior(
   dists = c(rep("tnorm", 9), "beta", rep("tnorm", 19)),
   p1    = rep(1, npar),
   p2    = c(rep(2, 9), 1, rep(2, 19)),
   lower = c(rep(0, 10),  rep(NA, 19)),
   upper = c(rep(NA, 9), 1, rep(NA, 19)))
sigma.prior &lt;- BuildPrior(
   dists = rep("beta", npar),
   p1    = rep(1, npar),
   p2    = rep(1, npar))
names(p.prior) &lt;- pname
names(mu.prior) &lt;- pname
names(sigma.prior) &lt;- pname
priors0 &lt;- list(pprior=p.prior, location=mu.prior, scale=sigma.prior)
save(model0, dat0, dmi0, ps0, priors0, file = "tests/Group2/PM12S.RData")

## Sampling separately ----------
load("tests/Group2/PM12S.RData")
fit0 &lt;- StartNewsamples(dmi0, priors0[[1]], ncore=6, thin=4)
fit  &lt;- run(fit0, thin=2, ncore=6)
est0 &lt;- summary(fit, recovery = TRUE, ps = ps0, verbose =TRUE)
rhat0 &lt;- gelman(fit, verbose=TRUE)
save(fit0, fit, model0, dat0, dmi0, ps0, priors0, file = "tests/Group2/PM12S.RData")
</code></pre></div></div>

<h1 id="reference">Reference</h1>
<ul>
  <li>Strickland, L., Loft, S., Remington, R. W., &amp; Heathcote, A. (2018). Racing to remember: A theory of decision control in event-based prospective memory. Psychological Review, 125(6), 851-887. http://dx.doi.org/10.1037/rev0000113</li>
  <li>Wagenmakers, E.-J., Ratcliff, R., Gomez, P., &amp; McKoon, G. (2008). A diffusion model account of criterion shifts in the lexical decision task. <em>Journal of Memory and Language</em>, 58, 140-159. doi:10.1016/j.jml.2007.04.006.</li>
</ul>


			</article>
		</section>

		<script>
			document.getElementById("open-nav").addEventListener("click", function () {
				document.body.classList.toggle("nav-open");
			});

			
		</script>

		  
		</script>
	</body>
</html>
